[
  {
    "title": "DRAMA: Unifying Data Retrieval and Analysis for Open-Domain Analytic Queries",
    "authors": "Chuxuan Hu, Maxwell Yang, James Weiland, Yeji Lim, Suhas Palawala, Daniel Kang",
    "published": "2025-10-31",
    "arxiv_id": "2510.27238v1",
    "url": "http://arxiv.org/abs/2510.27238v1",
    "pdf_url": "http://arxiv.org/pdf/2510.27238v1",
    "category": "information_retrieval",
    "primary_category": "cs.DB",
    "abstract": "Manually conducting real-world data analyses is labor-intensive and\ninefficient. Despite numerous attempts to automate data science workflows, none\nof the existing paradigms or systems fully demonstrate all three key\ncapabilities required to support them effectively: (1) open-domain data\ncollection, (2) structured data transformation, and (3) analytic reasoning.\n  To overcome these limitations, we propose DRAMA, an end-to-end paradigm that\nanswers users' analytic queries in natural language on large-scale open-domain\ndata. DRAMA unifies data collection, transformation, and analysis as a single\npipeline. To quantitatively evaluate system performance on tasks representative\nof DRAMA, we construct a benchmark, DRAMA-Bench, consisting of two categories\nof tasks: claim verification and question answering, each comprising 100\ninstances. These tasks are derived from real-world applications that have\ngained significant public attention and require the retrieval and analysis of\nopen-domain data. We develop DRAMA-Bot, a multi-agent system designed following\nDRAMA. It comprises a data retriever that collects and transforms data by\ncoordinating the execution of sub-agents, and a data analyzer that performs\nstructured reasoning over the retrieved data. We evaluate DRAMA-Bot on\nDRAMA-Bench together with five state-of-the-art baseline agents. DRAMA-Bot\nachieves 86.5% task accuracy at a cost of $0.05, outperforming all baselines\nwith up to 6.9 times the accuracy and less than 1/6 of the cost. DRAMA is\npublicly available at https://github.com/uiuc-kang-lab/drama.",
    "code_links": [
      "https://github.com/uiuc-kang-lab/drama"
    ],
    "comment": "Accepted to SIGMOD 2026"
  },
  {
    "title": "A Survey on Deep Text Hashing: Efficient Semantic Text Retrieval with Binary Representation",
    "authors": "Liyang He, Zhenya Huang, Cheng Yang, Rui Li, Zheng Zhang, Kai Zhang, Zhi Li, Qi Liu, Enhong Chen",
    "published": "2025-10-31",
    "arxiv_id": "2510.27232v1",
    "url": "http://arxiv.org/abs/2510.27232v1",
    "pdf_url": "http://arxiv.org/pdf/2510.27232v1",
    "category": "information_retrieval",
    "primary_category": "cs.IR",
    "abstract": "With the rapid growth of textual content on the Internet, efficient\nlarge-scale semantic text retrieval has garnered increasing attention from both\nacademia and industry. Text hashing, which projects original texts into compact\nbinary hash codes, is a crucial method for this task. By using binary codes,\nthe semantic similarity computation for text pairs is significantly accelerated\nvia fast Hamming distance calculations, and storage costs are greatly reduced.\nWith the advancement of deep learning, deep text hashing has demonstrated\nsignificant advantages over traditional, data-independent hashing techniques.\nBy leveraging deep neural networks, these methods can learn compact and\nsemantically rich binary representations directly from data, overcoming the\nperformance limitations of earlier approaches. This survey investigates current\ndeep text hashing methods by categorizing them based on their core components:\nsemantic extraction, hash code quality preservation, and other key\ntechnologies. We then present a detailed evaluation schema with results on\nseveral popular datasets, followed by a discussion of practical applications\nand open-source tools for implementation. Finally, we conclude by discussing\nkey challenges and future research directions, including the integration of\ndeep text hashing with large language models to further advance the field. The\nproject for this survey can be accessed at\nhttps://github.com/hly1998/DeepTextHashing.",
    "code_links": [
      "https://github.com/hly1998/DeepTextHashing"
    ],
    "comment": null
  },
  {
    "title": "ReaKase-8B: Legal Case Retrieval via Knowledge and Reasoning Representations with LLMs",
    "authors": "Yanran Tang, Ruihong Qiu, Xue Li, Zi Huang",
    "published": "2025-10-30",
    "arxiv_id": "2510.26178v1",
    "url": "http://arxiv.org/abs/2510.26178v1",
    "pdf_url": "http://arxiv.org/pdf/2510.26178v1",
    "category": "information_retrieval",
    "primary_category": "cs.IR",
    "abstract": "Legal case retrieval (LCR) is a cornerstone of real-world legal decision\nmaking, as it enables practitioners to identify precedents for a given query\ncase. Existing approaches mainly rely on traditional lexical models and\npretrained language models to encode the texts of legal cases. Yet there are\nrich information in the relations among different legal entities as well as the\ncrucial reasoning process that uncovers how legal facts and legal issues can\nlead to judicial decisions. Such relational reasoning process reflects the\ndistinctive characteristics of each case that can distinguish one from another,\nmirroring the real-world judicial process. Naturally, incorporating such\ninformation into the precise case embedding could further enhance the accuracy\nof case retrieval. In this paper, a novel ReaKase-8B framework is proposed to\nleverage extracted legal facts, legal issues, legal relation triplets and legal\nreasoning for effective legal case retrieval. ReaKase-8B designs an in-context\nlegal case representation learning paradigm with a fine-tuned large language\nmodel. Extensive experiments on two benchmark datasets from COLIEE 2022 and\nCOLIEE 2023 demonstrate that our knowledge and reasoning augmented embeddings\nsubstantially improve retrieval performance over baseline models, highlighting\nthe potential of integrating legal reasoning into legal case retrieval systems.\nThe code has been released on https://github.com/yanran-tang/ReaKase-8B.",
    "code_links": [
      "https://github.com/yanran-tang/ReaKase-8B"
    ],
    "comment": null
  },
  {
    "title": "Alibaba International E-commerce Product Search Competition DcuRAGONs Team Technical Report",
    "authors": "Thang-Long Nguyen-Ho, Minh-Khoi Pham, Hoang-Bao Le",
    "published": "2025-10-29",
    "arxiv_id": "2510.25428v1",
    "url": "http://arxiv.org/abs/2510.25428v1",
    "pdf_url": "http://arxiv.org/pdf/2510.25428v1",
    "category": "information_retrieval",
    "primary_category": "cs.IR",
    "abstract": "This report details our methodology and results developed for the\nMultilingual E-commerce Search Competition. The problem aims to recognize\nrelevance between user queries versus product items in a multilingual context\nand improve recommendation performance on e-commerce platforms. Utilizing Large\nLanguage Models (LLMs) and their capabilities in other tasks, our data-centric\nmethod achieved the highest score compared to other solutions during the\ncompetition. Final leaderboard is publised at\nhttps://alibaba-international-cikm2025.github.io. The source code for our\nproject is published at https://github.com/nhtlongcs/e-commerce-product-search.",
    "code_links": [
      "https://github.com/nhtlongcs/e-commerce-product-search"
    ],
    "comment": "Alibaba International E-commerce Product Search Competition @ CIKM\n  2025"
  },
  {
    "title": "Seeing Through the MiRAGE: Evaluating Multimodal Retrieval Augmented Generation",
    "authors": "Alexander Martin, William Walden, Reno Kriz, Dengjia Zhang, Kate Sanders, Eugene Yang, Chihsheng Jin, Benjamin Van Durme",
    "published": "2025-10-28",
    "arxiv_id": "2510.24870v1",
    "url": "http://arxiv.org/abs/2510.24870v1",
    "pdf_url": "http://arxiv.org/pdf/2510.24870v1",
    "category": "information_retrieval",
    "primary_category": "cs.CL",
    "abstract": "We introduce MiRAGE, an evaluation framework for retrieval-augmented\ngeneration (RAG) from multimodal sources. As audiovisual media becomes a\nprevalent source of information online, it is essential for RAG systems to\nintegrate information from these sources into generation. However, existing\nevaluations for RAG are text-centric, limiting their applicability to\nmultimodal, reasoning intensive settings because they don't verify information\nagainst sources. MiRAGE is a claim-centric approach to multimodal RAG\nevaluation, consisting of InfoF1, evaluating factuality and information\ncoverage, and CiteF1, measuring citation support and completeness. We show that\nMiRAGE, when applied by humans, strongly aligns with extrinsic quality\njudgments. We additionally introduce automatic variants of MiRAGE and three\nprominent TextRAG metrics -- ACLE, ARGUE, and RAGAS -- demonstrating the\nlimitations of text-centric work and laying the groundwork for automatic\nevaluation. We release open-source implementations and outline how to assess\nmultimodal RAG.",
    "code_links": [
      "https://github.com/alexmartin1722/mirage"
    ],
    "comment": "https://github.com/alexmartin1722/mirage"
  },
  {
    "title": "PatenTEB: A Comprehensive Benchmark and Model Family for Patent Text Embedding",
    "authors": "Iliass Ayaou, Denis Cavallucci",
    "published": "2025-10-25",
    "arxiv_id": "2510.22264v1",
    "url": "http://arxiv.org/abs/2510.22264v1",
    "pdf_url": "http://arxiv.org/pdf/2510.22264v1",
    "category": "information_retrieval",
    "primary_category": "cs.CL",
    "abstract": "Patent text embeddings enable prior art search, technology landscaping, and\npatent analysis, yet existing benchmarks inadequately capture patent-specific\nchallenges. We introduce PatenTEB, a comprehensive benchmark comprising 15\ntasks across retrieval, classification, paraphrase, and clustering, with 2.06\nmillion examples. PatenTEB employs domain-stratified splits, domain specific\nhard negative mining, and systematic coverage of asymmetric\nfragment-to-document matching scenarios absent from general embedding\nbenchmarks. We develop the patembed model family through multi-task training,\nspanning 67M to 344M parameters with context lengths up to 4096 tokens.\nExternal validation shows strong generalization: patembed-base achieves\nstate-of-the-art on MTEB BigPatentClustering.v2 (0.494 V-measure vs. 0.445\nprevious best), while patembed-large achieves 0.377 NDCG@100 on DAPFAM.\nSystematic ablations reveal that multi-task training improves external\ngeneralization despite minor benchmark costs, and that domain-pretrained\ninitialization provides consistent advantages across task families. All\nresources will be made available at https://github.com/iliass-y/patenteb.\nKeywords: patent retrieval, sentence embeddings, multi-task learning,\nasymmetric retrieval, benchmark evaluation, contrastive learning.",
    "code_links": [
      "https://github.com/iliass-y/patenteb"
    ],
    "comment": null
  },
  {
    "title": "Hybrid-Vector Retrieval for Visually Rich Documents: Combining Single-Vector Efficiency and Multi-Vector Accuracy",
    "authors": "Juyeon Kim, Geon Lee, Dongwon Choi, Taeuk Kim, Kijung Shin",
    "published": "2025-10-25",
    "arxiv_id": "2510.22215v1",
    "url": "http://arxiv.org/abs/2510.22215v1",
    "pdf_url": "http://arxiv.org/pdf/2510.22215v1",
    "category": "information_retrieval",
    "primary_category": "cs.IR",
    "abstract": "Retrieval over visually rich documents is essential for tasks such as legal\ndiscovery, scientific search, and enterprise knowledge management. Existing\napproaches fall into two paradigms: single-vector retrieval, which is efficient\nbut coarse, and multi-vector retrieval, which is accurate but computationally\nexpensive. To address this trade-off, we propose HEAVEN, a two-stage\nhybrid-vector framework. In the first stage, HEAVEN efficiently retrieves\ncandidate pages using a single-vector method over Visually-Summarized Pages\n(VS-Pages), which assemble representative visual layouts from multiple pages.\nIn the second stage, it reranks candidates with a multi-vector method while\nfiltering query tokens by linguistic importance to reduce redundant\ncomputations. To evaluate retrieval systems under realistic conditions, we also\nintroduce ViMDOC, the first benchmark for visually rich, multi-document, and\nlong-document retrieval. Across four benchmarks, HEAVEN attains 99.87% of the\nRecall@1 performance of multi-vector models on average while reducing per-query\ncomputation by 99.82%, achieving efficiency and accuracy. Our code and datasets\nare available at: https://github.com/juyeonnn/HEAVEN",
    "code_links": [
      "https://github.com/juyeonnn/HEAVEN"
    ],
    "comment": null
  },
  {
    "title": "A Benchmark for Open-Domain Numerical Fact-Checking Enhanced by Claim Decomposition",
    "authors": "V Venktesh, Deepali Prabhu, Avishek Anand",
    "published": "2025-10-24",
    "arxiv_id": "2510.22055v1",
    "url": "http://arxiv.org/abs/2510.22055v1",
    "pdf_url": "http://arxiv.org/pdf/2510.22055v1",
    "category": "information_retrieval",
    "primary_category": "cs.IR",
    "abstract": "Fact-checking numerical claims is critical as the presence of numbers provide\nmirage of veracity despite being fake potentially causing catastrophic impacts\non society. The prior works in automatic fact verification do not primarily\nfocus on natural numerical claims. A typical human fact-checker first retrieves\nrelevant evidence addressing the different numerical aspects of the claim and\nthen reasons about them to predict the veracity of the claim. Hence, the search\nprocess of a human fact-checker is a crucial skill that forms the foundation of\nthe verification process. Emulating a real-world setting is essential to aid in\nthe development of automated methods that encompass such skills. However,\nexisting benchmarks employ heuristic claim decomposition approaches augmented\nwith weakly supervised web search to collect evidences for verifying claims.\nThis sometimes results in less relevant evidences and noisy sources with\ntemporal leakage rendering a less realistic retrieval setting for claim\nverification. Hence, we introduce QuanTemp++: a dataset consisting of natural\nnumerical claims, an open domain corpus, with the corresponding relevant\nevidence for each claim. The evidences are collected through a claim\ndecomposition process approximately emulating the approach of human\nfact-checker and veracity labels ensuring there is no temporal leakage. Given\nthis dataset, we also characterize the retrieval performance of key claim\ndecomposition paradigms. Finally, we observe their effect on the outcome of the\nverification pipeline and draw insights. The code for data pipeline along with\nlink to data can be found at https://github.com/VenkteshV/QuanTemp_Plus",
    "code_links": [
      "https://github.com/VenkteshV/QuanTemp_Plus"
    ],
    "comment": "16 pages"
  },
  {
    "title": "DeepAgent: A General Reasoning Agent with Scalable Toolsets",
    "authors": "Xiaoxi Li, Wenxiang Jiao, Jiarui Jin, Guanting Dong, Jiajie Jin, Yinuo Wang, Hao Wang, Yutao Zhu, Ji-Rong Wen, Yuan Lu, Zhicheng Dou",
    "published": "2025-10-24",
    "arxiv_id": "2510.21618v1",
    "url": "http://arxiv.org/abs/2510.21618v1",
    "pdf_url": "http://arxiv.org/pdf/2510.21618v1",
    "category": "information_retrieval",
    "primary_category": "cs.AI",
    "abstract": "Large reasoning models have demonstrated strong problem-solving abilities,\nyet real-world tasks often require external tools and long-horizon\ninteractions. Existing agent frameworks typically follow predefined workflows,\nwhich limit autonomous and global task completion. In this paper, we introduce\nDeepAgent, an end-to-end deep reasoning agent that performs autonomous\nthinking, tool discovery, and action execution within a single, coherent\nreasoning process. To address the challenges of long-horizon interactions,\nparticularly the context length explosion from multiple tool calls and the\naccumulation of interaction history, we introduce an autonomous memory folding\nmechanism that compresses past interactions into structured episodic, working,\nand tool memories, reducing error accumulation while preserving critical\ninformation. To teach general-purpose tool use efficiently and stably, we\ndevelop an end-to-end reinforcement learning strategy, namely ToolPO, that\nleverages LLM-simulated APIs and applies tool-call advantage attribution to\nassign fine-grained credit to the tool invocation tokens. Extensive experiments\non eight benchmarks, including general tool-use tasks (ToolBench, API-Bank,\nTMDB, Spotify, ToolHop) and downstream applications (ALFWorld, WebShop, GAIA,\nHLE), demonstrate that DeepAgent consistently outperforms baselines across both\nlabeled-tool and open-set tool retrieval scenarios. This work takes a step\ntoward more general and capable agents for real-world applications. The code\nand demo are available at https://github.com/RUC-NLPIR/DeepAgent.",
    "code_links": [
      "https://github.com/RUC-NLPIR/DeepAgent"
    ],
    "comment": null
  },
  {
    "title": "Pctx: Tokenizing Personalized Context for Generative Recommendation",
    "authors": "Qiyong Zhong, Jiajie Su, Yunshan Ma, Julian McAuley, Yupeng Hou",
    "published": "2025-10-24",
    "arxiv_id": "2510.21276v1",
    "url": "http://arxiv.org/abs/2510.21276v1",
    "pdf_url": "http://arxiv.org/pdf/2510.21276v1",
    "category": "information_retrieval",
    "primary_category": "cs.IR",
    "abstract": "Generative recommendation (GR) models tokenize each action into a few\ndiscrete tokens (called semantic IDs) and autoregressively generate the next\ntokens as predictions, showing advantages such as memory efficiency,\nscalability, and the potential to unify retrieval and ranking. Despite these\nbenefits, existing tokenization methods are static and non-personalized. They\ntypically derive semantic IDs solely from item features, assuming a universal\nitem similarity that overlooks user-specific perspectives. However, under the\nautoregressive paradigm, semantic IDs with the same prefixes always receive\nsimilar probabilities, so a single fixed mapping implicitly enforces a\nuniversal item similarity standard across all users. In practice, the same item\nmay be interpreted differently depending on user intentions and preferences. To\naddress this issue, we propose a personalized context-aware tokenizer that\nincorporates a user's historical interactions when generating semantic IDs.\nThis design allows the same item to be tokenized into different semantic IDs\nunder different user contexts, enabling GR models to capture multiple\ninterpretive standards and produce more personalized predictions. Experiments\non three public datasets demonstrate up to 11.44% improvement in NDCG@10 over\nnon-personalized action tokenization baselines. Our code is available at\nhttps://github.com/YoungZ365/Pctx.",
    "code_links": [
      "https://github.com/YoungZ365/Pctx"
    ],
    "comment": null
  },
  {
    "title": "Rank-GRPO: Training LLM-based Conversational Recommender Systems with Reinforcement Learning",
    "authors": "Yaochen Zhu, Harald Steck, Dawen Liang, Yinhan He, Vito Ostuni, Jundong Li, Nathan Kallus",
    "published": "2025-10-23",
    "arxiv_id": "2510.20150v2",
    "url": "http://arxiv.org/abs/2510.20150v2",
    "pdf_url": "http://arxiv.org/pdf/2510.20150v2",
    "category": "information_retrieval",
    "primary_category": "cs.IR",
    "abstract": "Large language models (LLMs) are reshaping the recommender system paradigm by\nenabling users to express preferences and receive recommendations through\nconversations. Yet, aligning LLMs to the recommendation task remains\nchallenging: pretrained LLMs often generate out-of-catalog items, violate\nrequired output formats, and their ranking quality degrades sharply toward the\nend of the generated list. To this end, we propose ConvRec-R1, a two-stage\nframework for end-to-end training of LLM-based conversational recommender\nsystems. In Stage 1, we construct a behavioral-cloning dataset with a\nRemap-Reflect-Adjust pipeline, which produces high-quality, catalog-grounded\ndemonstrations from powerful blackbox LLMs to warm-start the RL training. In\nStage 2, we propose Rank-GRPO, a principled extension of group relative policy\noptimization (GRPO) tailored to tasks with rank-style outputs. Rank-GRPO treats\neach rank in the recommendation list as the unit instead of token (too\nfine-grained) or sequence (too coarse), redefining rewards to remove non-causal\ncredit assignment and introducing a rank-level importance ratio based on the\ngeometric mean of rank-wise token probabilities to stabilize policy updates.\nExperiments on the public Reddit-v2 dataset show that ConvRec-R1 converges\nfaster and achieves higher Recall and NDCG than GRPO-style baselines. Code and\ndatasets are released at https://github.com/yaochenzhu/Rank-GRPO.",
    "code_links": [
      "https://github.com/yaochenzhu/Rank-GRPO"
    ],
    "comment": null
  },
  {
    "title": "DRAMA: Unifying Data Retrieval and Analysis for Open-Domain Analytic Queries",
    "authors": "Chuxuan Hu, Maxwell Yang, James Weiland, Yeji Lim, Suhas Palawala, Daniel Kang",
    "published": "2025-10-31",
    "arxiv_id": "2510.27238v1",
    "url": "http://arxiv.org/abs/2510.27238v1",
    "pdf_url": "http://arxiv.org/pdf/2510.27238v1",
    "category": "databases",
    "primary_category": "cs.DB",
    "abstract": "Manually conducting real-world data analyses is labor-intensive and\ninefficient. Despite numerous attempts to automate data science workflows, none\nof the existing paradigms or systems fully demonstrate all three key\ncapabilities required to support them effectively: (1) open-domain data\ncollection, (2) structured data transformation, and (3) analytic reasoning.\n  To overcome these limitations, we propose DRAMA, an end-to-end paradigm that\nanswers users' analytic queries in natural language on large-scale open-domain\ndata. DRAMA unifies data collection, transformation, and analysis as a single\npipeline. To quantitatively evaluate system performance on tasks representative\nof DRAMA, we construct a benchmark, DRAMA-Bench, consisting of two categories\nof tasks: claim verification and question answering, each comprising 100\ninstances. These tasks are derived from real-world applications that have\ngained significant public attention and require the retrieval and analysis of\nopen-domain data. We develop DRAMA-Bot, a multi-agent system designed following\nDRAMA. It comprises a data retriever that collects and transforms data by\ncoordinating the execution of sub-agents, and a data analyzer that performs\nstructured reasoning over the retrieved data. We evaluate DRAMA-Bot on\nDRAMA-Bench together with five state-of-the-art baseline agents. DRAMA-Bot\nachieves 86.5% task accuracy at a cost of $0.05, outperforming all baselines\nwith up to 6.9 times the accuracy and less than 1/6 of the cost. DRAMA is\npublicly available at https://github.com/uiuc-kang-lab/drama.",
    "code_links": [
      "https://github.com/uiuc-kang-lab/drama"
    ],
    "comment": "Accepted to SIGMOD 2026"
  },
  {
    "title": "Rethinking Text-to-SQL: Dynamic Multi-turn SQL Interaction for Real-world Database Exploration",
    "authors": "Linzhuang Sun, Tianyu Guo, Hao Liang, Yuying Li, Qifeng Cai, Jingxuan Wei, Bihui Yu, Wentao Zhang, Bin Cui",
    "published": "2025-10-30",
    "arxiv_id": "2510.26495v1",
    "url": "http://arxiv.org/abs/2510.26495v1",
    "pdf_url": "http://arxiv.org/pdf/2510.26495v1",
    "category": "databases",
    "primary_category": "cs.DB",
    "abstract": "Recent advances in Text-to-SQL have achieved strong results in static,\nsingle-turn tasks, where models generate SQL queries from natural language\nquestions. However, these systems fall short in real-world interactive\nscenarios, where user intents evolve and queries must be refined over multiple\nturns. In applications such as finance and business analytics, users\niteratively adjust query constraints or dimensions based on intermediate\nresults. To evaluate such dynamic capabilities, we introduce DySQL-Bench, a\nbenchmark assessing model performance under evolving user interactions. Unlike\nprevious manually curated datasets, DySQL-Bench is built through an automated\ntwo-stage pipeline of task synthesis and verification. Structured tree\nrepresentations derived from raw database tables guide LLM-based task\ngeneration, followed by interaction-oriented filtering and expert validation.\nHuman evaluation confirms 100% correctness of the synthesized data. We further\npropose a multi-turn evaluation framework simulating realistic interactions\namong an LLM-simulated user, the model under test, and an executable database.\nThe model must adapt its reasoning and SQL generation as user intents change.\nDySQL-Bench covers 13 domains across BIRD and Spider 2 databases, totaling\n1,072 tasks. Even GPT-4o attains only 58.34% overall accuracy and 23.81% on the\nPass@5 metric, underscoring the benchmark's difficulty. All code and data are\nreleased at https://github.com/Aurora-slz/Real-World-SQL-Bench .",
    "code_links": [
      "https://github.com/Aurora-slz/Real-World-SQL-Bench"
    ],
    "comment": null
  },
  {
    "title": "Evaluating Joinable Column Discovery Approaches for Context-Aware Search",
    "authors": "Harsha Kokel, Aamod Khatiwada, Tejaswini Pedapati, Haritha Ananthakrishnan, Oktie Hassanzadeh, Horst Samulowitz, Kavitha Srinivas",
    "published": "2025-10-28",
    "arxiv_id": "2510.24599v1",
    "url": "http://arxiv.org/abs/2510.24599v1",
    "pdf_url": "http://arxiv.org/pdf/2510.24599v1",
    "category": "databases",
    "primary_category": "cs.DB",
    "abstract": "Joinable Column Discovery is a critical challenge in automating enterprise\ndata analysis. While existing approaches focus on syntactic overlap and\nsemantic similarity, there remains limited understanding of which methods\nperform best for different data characteristics and how multiple criteria\ninfluence discovery effectiveness. We present a comprehensive experimental\nevaluation of joinable column discovery methods across diverse scenarios. Our\nstudy compares syntactic and semantic techniques on seven benchmarks covering\nrelational databases and data lakes. We analyze six key criteria -- unique\nvalues, intersection size, join size, reverse join size, value semantics, and\nmetadata semantics -- and examine how combining them through ensemble ranking\naffects performance. Our analysis reveals differences in method behavior across\ndata contexts and highlights the benefits of integrating multiple criteria for\nrobust join discovery. We provide empirical evidence on when each criterion\nmatters, compare pre-trained embedding models for semantic joins, and offer\npractical guidelines for selecting suitable methods based on dataset\ncharacteristics. Our findings show that metadata and value semantics are\ncrucial for data lakes, size-based criteria play a stronger role in relational\ndatabases, and ensemble approaches consistently outperform single-criterion\nmethods.",
    "code_links": [
      "https://github.com/IBM/ContextAwareJoin"
    ],
    "comment": "This is an Experiments and Analysis paper. The source code, data,\n  and/or other artifacts have been made available at\n  https://github.com/IBM/ContextAwareJoin"
  },
  {
    "title": "A Survey of Data Agents: Emerging Paradigm or Overstated Hype?",
    "authors": "Yizhang Zhu, Liangwei Wang, Chenyu Yang, Xiaotian Lin, Boyan Li, Wei Zhou, Xinyu Liu, Zhangyang Peng, Tianqi Luo, Yu Li, Chengliang Chai, Chong Chen, Shimin Di, Ju Fan, Ji Sun, Nan Tang, Fugee Tsung, Jiannan Wang, Chenglin Wu, Yanwei Xu, Shaolei Zhang, Yong Zhang, Xuanhe Zhou, Guoliang Li, Yuyu Luo",
    "published": "2025-10-27",
    "arxiv_id": "2510.23587v1",
    "url": "http://arxiv.org/abs/2510.23587v1",
    "pdf_url": "http://arxiv.org/pdf/2510.23587v1",
    "category": "databases",
    "primary_category": "cs.DB",
    "abstract": "The rapid advancement of large language models (LLMs) has spurred the\nemergence of data agents--autonomous systems designed to orchestrate Data + AI\necosystems for tackling complex data-related tasks. However, the term \"data\nagent\" currently suffers from terminological ambiguity and inconsistent\nadoption, conflating simple query responders with sophisticated autonomous\narchitectures. This terminological ambiguity fosters mismatched user\nexpectations, accountability challenges, and barriers to industry growth.\nInspired by the SAE J3016 standard for driving automation, this survey\nintroduces the first systematic hierarchical taxonomy for data agents,\ncomprising six levels that delineate and trace progressive shifts in autonomy,\nfrom manual operations (L0) to a vision of generative, fully autonomous data\nagents (L5), thereby clarifying capability boundaries and responsibility\nallocation. Through this lens, we offer a structured review of existing\nresearch arranged by increasing autonomy, encompassing specialized data agents\nfor data management, preparation, and analysis, alongside emerging efforts\ntoward versatile, comprehensive systems with enhanced autonomy. We further\nanalyze critical evolutionary leaps and technical gaps for advancing data\nagents, especially the ongoing L2-to-L3 transition, where data agents evolve\nfrom procedural execution to autonomous orchestration. Finally, we conclude\nwith a forward-looking roadmap, envisioning the advent of proactive, generative\ndata agents.",
    "code_links": [
      "https://github.com/HKUSTDial/awesome-data-agents"
    ],
    "comment": "Please refer to our paper list and companion materials at:\n  https://github.com/HKUSTDial/awesome-data-agents"
  },
  {
    "title": "DynaQuery: A Self-Adapting Framework for Querying Structured and Multimodal Data",
    "authors": "Aymane Hassini",
    "published": "2025-10-20",
    "arxiv_id": "2510.18029v1",
    "url": "http://arxiv.org/abs/2510.18029v1",
    "pdf_url": "http://arxiv.org/pdf/2510.18029v1",
    "category": "databases",
    "primary_category": "cs.DB",
    "abstract": "The rise of Large Language Models (LLMs) has accelerated the long-standing\ngoal of enabling natural language querying over complex, hybrid databases. Yet,\nthis ambition exposes a dual challenge: reasoning jointly over structured,\nmulti-relational schemas and the semantic content of linked unstructured\nassets. To overcome this, we present DynaQuery - a unified, self-adapting\nframework that serves as a practical blueprint for next-generation \"Unbound\nDatabases.\" At the heart of DynaQuery lies the Schema Introspection and Linking\nEngine (SILE), a novel systems primitive that elevates schema linking to a\nfirst-class query planning phase. We conduct a rigorous, multi-benchmark\nempirical evaluation of this structure-aware architecture against the prevalent\nunstructured Retrieval-Augmented Generation (RAG) paradigm. Our results\ndemonstrate that the unstructured retrieval paradigm is architecturally\nsusceptible to catastrophic contextual failures, such as SCHEMA_HALLUCINATION,\nleading to unreliable query generation. In contrast, our SILE-based design\nestablishes a substantially more robust foundation, nearly eliminating this\nfailure mode. Moreover, end-to-end validation on a complex, newly curated\nbenchmark uncovers a key generalization principle: the transition from pure\nschema-awareness to holistic semantics-awareness. Taken together, our findings\nprovide a validated architectural basis for developing natural language\ndatabase interfaces that are robust, adaptable, and predictably consistent.",
    "code_links": [
      "https://github.com/aymanehassini/DynaQuery"
    ],
    "comment": "15 pages, 2 figures, 10 tables. Source code and experimental\n  artifacts are available at: https://github.com/aymanehassini/DynaQuery . The\n  'DynaQuery-Eval-5K' benchmark, introduced in this work, is also publicly\n  available at:\n  https://www.kaggle.com/datasets/aymanehassini/dynaquery-eval-5k-benchmark"
  },
  {
    "title": "DeepAnalyze: Agentic Large Language Models for Autonomous Data Science",
    "authors": "Shaolei Zhang, Ju Fan, Meihao Fan, Guoliang Li, Xiaoyong Du",
    "published": "2025-10-19",
    "arxiv_id": "2510.16872v1",
    "url": "http://arxiv.org/abs/2510.16872v1",
    "pdf_url": "http://arxiv.org/pdf/2510.16872v1",
    "category": "databases",
    "primary_category": "cs.AI",
    "abstract": "Autonomous data science, from raw data sources to analyst-grade deep research\nreports, has been a long-standing challenge, and is now becoming feasible with\nthe emergence of powerful large language models (LLMs). Recent workflow-based\ndata agents have shown promising results on specific data tasks but remain\nfundamentally limited in achieving fully autonomous data science due to their\nreliance on predefined workflows. In this paper, we introduce DeepAnalyze-8B,\nthe first agentic LLM designed for autonomous data science, capable of\nautomatically completing the end-toend pipeline from data sources to\nanalyst-grade deep research reports. To tackle high-complexity data science\ntasks, we propose a curriculum-based agentic training paradigm that emulates\nthe learning trajectory of human data scientists, enabling LLMs to\nprogressively acquire and integrate multiple capabilities in real-world\nenvironments. We also introduce a data-grounded trajectory synthesis framework\nthat constructs high-quality training data. Through agentic training,\nDeepAnalyze learns to perform a broad spectrum of data tasks, ranging from data\nquestion answering and specialized analytical tasks to open-ended data\nresearch. Experiments demonstrate that, with only 8B parameters, DeepAnalyze\noutperforms previous workflow-based agents built on most advanced proprietary\nLLMs. The model, code, and training data of DeepAnalyze are open-sourced,\npaving the way toward autonomous data science.",
    "code_links": [
      "https://github.com/ruc-datalab/DeepAnalyze"
    ],
    "comment": "Code: https://github.com/ruc-datalab/DeepAnalyze Model:\n  https://huggingface.co/RUC-DataLab/DeepAnalyze-8B"
  },
  {
    "title": "BenchPress: A Human-in-the-Loop Annotation System for Rapid Text-to-SQL Benchmark Curation",
    "authors": "Fabian Wenz, Omar Bouattour, Devin Yang, Justin Choi, Cecil Gregg, Nesime Tatbul, Çağatay Demiralp",
    "published": "2025-10-11",
    "arxiv_id": "2510.13853v1",
    "url": "http://arxiv.org/abs/2510.13853v1",
    "pdf_url": "http://arxiv.org/pdf/2510.13853v1",
    "category": "databases",
    "primary_category": "cs.CL",
    "abstract": "Large language models (LLMs) have been successfully applied to many tasks,\nincluding text-to-SQL generation. However, much of this work has focused on\npublicly available datasets, such as Fiben, Spider, and Bird. Our earlier work\nshowed that LLMs are much less effective in querying large private enterprise\ndata warehouses and released Beaver, the first private enterprise text-to-SQL\nbenchmark. To create Beaver, we leveraged SQL logs, which are often readily\navailable. However, manually annotating these logs to identify which natural\nlanguage questions they answer is a daunting task. Asking database\nadministrators, who are highly trained experts, to take on additional work to\nconstruct and validate corresponding natural language utterances is not only\nchallenging but also quite costly. To address this challenge, we introduce\nBenchPress, a human-in-the-loop system designed to accelerate the creation of\ndomain-specific text-to-SQL benchmarks. Given a SQL query, BenchPress uses\nretrieval-augmented generation (RAG) and LLMs to propose multiple natural\nlanguage descriptions. Human experts then select, rank, or edit these drafts to\nensure accuracy and domain alignment. We evaluated BenchPress on annotated\nenterprise SQL logs, demonstrating that LLM-assisted annotation drastically\nreduces the time and effort required to create high-quality benchmarks. Our\nresults show that combining human verification with LLM-generated suggestions\nenhances annotation accuracy, benchmark reliability, and model evaluation\nrobustness. By streamlining the creation of custom benchmarks, BenchPress\noffers researchers and practitioners a mechanism for assessing text-to-SQL\nmodels on a given domain-specific workload. BenchPress is freely available via\nour public GitHub repository at\nhttps://github.com/fabian-wenz/enterprise-txt2sql and is also accessible on our\nwebsite at http://dsg-mcgraw.csail.mit.edu:5000.",
    "code_links": [
      "https://github.com/fabian-wenz/enterprise-txt2sql"
    ],
    "comment": "CIDR'26"
  },
  {
    "title": "Regular Expression Indexing for Log Analysis. Extended Version",
    "authors": "Ling Zhang, Shaleen Deep, Jignesh M. Patel, Karthikeyan Sankaralingam",
    "published": "2025-10-11",
    "arxiv_id": "2510.10348v1",
    "url": "http://arxiv.org/abs/2510.10348v1",
    "pdf_url": "http://arxiv.org/pdf/2510.10348v1",
    "category": "databases",
    "primary_category": "cs.DB",
    "abstract": "In this paper, we present the design and architecture of REI, a novel system\nfor indexing log data for regular expression queries. Our main contribution is\nan $n$-gram-based indexing strategy and an efficient storage mechanism that\nresults in a speedup of up to 14x compared to state-of-the-art regex processing\nengines that do not use indexing, using only 2.1% of extra space. We perform a\ndetailed study that analyzes the space usage of the index and the improvement\nin workload execution time, uncovering interesting insights. Specifically, we\nshow that even an optimized implementation of strategies such as inverted\nindexing, which are widely used in text processing libraries, may lead to\nsuboptimal performance for regex indexing on log analysis tasks. Overall, the\nREI approach presented in this paper provides a significant boost when\nevaluating regular expression queries on log data. REI is also modular and can\nwork with existing regular expression packages, making it easy to deploy in a\nvariety of settings. The code of REI is available at\nhttps://github.com/mush-zhang/REI-Regular-Expression-Indexing.",
    "code_links": [
      "https://github.com/mush-zhang/REI-Regular-Expression-Indexing"
    ],
    "comment": null
  },
  {
    "title": "Efficient Mining of Low-Utility Sequential Patterns",
    "authors": "Jian Zhu, Zhidong Lin, Wensheng Gan, Ruichu Cai, Zhifeng Hao, Philip S. Yu",
    "published": "2025-10-11",
    "arxiv_id": "2510.10243v1",
    "url": "http://arxiv.org/abs/2510.10243v1",
    "pdf_url": "http://arxiv.org/pdf/2510.10243v1",
    "category": "databases",
    "primary_category": "cs.DB",
    "abstract": "Discovering valuable insights from rich data is a crucial task for\nexploratory data analysis. Sequential pattern mining (SPM) has found widespread\napplications across various domains. In recent years, low-utility sequential\npattern mining (LUSPM) has shown strong potential in applications such as\nintrusion detection and genomic sequence analysis. However, existing research\nin utility-based SPM focuses on high-utility sequential patterns, and the\ndefinitions and strategies used in high-utility SPM cannot be directly applied\nto LUSPM. Moreover, no algorithms have yet been developed specifically for\nmining low-utility sequential patterns. To address these problems, we formalize\nthe LUSPM problem, redefine sequence utility, and introduce a compact data\nstructure called the sequence-utility chain to efficiently record utility\ninformation. Furthermore, we propose three novel algorithm--LUSPM_b, LUSPM_s,\nand LUSPM_e--to discover the complete set of low-utility sequential patterns.\nLUSPM_b serves as an exhaustive baseline, while LUSPM_s and LUSPM_e build upon\nit, generating subsequences through shrinkage and extension operations,\nrespectively. In addition, we introduce the maximal non-mutually contained\nsequence set and incorporate multiple pruning strategies, which significantly\nreduce redundant operations in both LUSPM_s and LUSPM_e. Finally, extensive\nexperimental results demonstrate that both LUSPM_s and LUSPM_e substantially\noutperform LUSPM_b and exhibit excellent scalability. Notably, LUSPM_e achieves\nsuperior efficiency, requiring less runtime and memory consumption than\nLUSPM_s. Our code is available at https://github.com/Zhidong-Lin/LUSPM.",
    "code_links": [
      "https://github.com/Zhidong-Lin/LUSPM"
    ],
    "comment": "Preprint, 4 tables, 9 figures"
  }
]