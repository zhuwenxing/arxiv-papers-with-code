[
  {
    "title": "JointRank: Rank Large Set with Single Pass",
    "authors": "Evgeny Dedov",
    "published": "2025-06-27",
    "arxiv_id": "2506.22262v1",
    "url": "http://arxiv.org/abs/2506.22262v1",
    "pdf_url": "http://arxiv.org/pdf/2506.22262v1",
    "category": "information_retrieval",
    "primary_category": "cs.IR",
    "abstract": "Efficiently ranking relevant items from large candidate pools is a\ncornerstone of modern information retrieval systems -- such as web search,\nrecommendation, and retrieval-augmented generation. Listwise rerankers, which\nimprove relevance by jointly considering multiple candidates, are often limited\nin practice: either by model input size constraints, or by degraded quality\nwhen processing large sets. We propose a model-agnostic method for fast\nreranking large sets that exceed a model input limits. The method first\npartitions candidate items into overlapping blocks, each of which is ranked\nindependently in parallel. Implicit pairwise comparisons are then derived from\nthese local rankings. Finally, these comparisons are aggregated to construct a\nglobal ranking using algorithms such as Winrate or PageRank. Experiments on\nTREC DL-2019 show that our method achieves an nDCG@10 of 70.88 compared to the\n57.68 for full-context listwise approach using gpt-4.1-mini as long-context\nmodel, while reducing latency from 21 to 8 seconds.\n  The implementation of the algorithm and the experiments is available in the\nrepository: https://github.com/V3RGANz/jointrank",
    "code_links": [
      "https://github.com/V3RGANz/jointrank"
    ],
    "comment": "ICTIR'25 Accepted"
  },
  {
    "title": "skLEP: A Slovak General Language Understanding Benchmark",
    "authors": "Marek Šuppa, Andrej Ridzik, Daniel Hládek, Tomáš Javůrek, Viktória Ondrejová, Kristína Sásiková, Martin Tamajka, Marián Šimko",
    "published": "2025-06-26",
    "arxiv_id": "2506.21508v1",
    "url": "http://arxiv.org/abs/2506.21508v1",
    "pdf_url": "http://arxiv.org/pdf/2506.21508v1",
    "category": "information_retrieval",
    "primary_category": "cs.CL",
    "abstract": "In this work, we introduce skLEP, the first comprehensive benchmark\nspecifically designed for evaluating Slovak natural language understanding\n(NLU) models. We have compiled skLEP to encompass nine diverse tasks that span\ntoken-level, sentence-pair, and document-level challenges, thereby offering a\nthorough assessment of model capabilities. To create this benchmark, we curated\nnew, original datasets tailored for Slovak and meticulously translated\nestablished English NLU resources. Within this paper, we also present the first\nsystematic and extensive evaluation of a wide array of Slovak-specific,\nmultilingual, and English pre-trained language models using the skLEP tasks.\nFinally, we also release the complete benchmark data, an open-source toolkit\nfacilitating both fine-tuning and evaluation of models, and a public\nleaderboard at https://github.com/slovak-nlp/sklep in the hopes of fostering\nreproducibility and drive future research in Slovak NLU.",
    "code_links": [
      "https://github.com/slovak-nlp/sklep"
    ],
    "comment": "ACL 2025 Findings"
  },
  {
    "title": "Small Encoders Can Rival Large Decoders in Detecting Groundedness",
    "authors": "Istabrak Abbes, Gabriele Prato, Quentin Fournier, Fernando Rodriguez, Alaa Boukhary, Adam Elwood, Sarath Chandar",
    "published": "2025-06-26",
    "arxiv_id": "2506.21288v1",
    "url": "http://arxiv.org/abs/2506.21288v1",
    "pdf_url": "http://arxiv.org/pdf/2506.21288v1",
    "category": "information_retrieval",
    "primary_category": "cs.CL",
    "abstract": "Augmenting large language models (LLMs) with external context significantly\nimproves their performance in natural language processing (NLP) tasks. However,\nLLMs struggle to answer queries reliably when the provided context lacks\ninformation, often resorting to ungrounded speculation or internal knowledge.\nGroundedness - generating responses strictly supported by the context - is\nessential for ensuring factual consistency and trustworthiness. This study\nfocuses on detecting whether a given query is grounded in a document provided\nin context before the costly answer generation by LLMs. Such a detection\nmechanism can significantly reduce both inference time and resource\nconsumption. We show that lightweight, task specific encoder models such as\nRoBERTa and NomicBERT, fine-tuned on curated datasets, can achieve accuracy\ncomparable to state-of-the-art LLMs, such as Llama3 8B and GPT4o, in\ngroundedness detection while reducing inference latency by orders of magnitude.\nThe code is available at : https://github.com/chandarlab/Hallucinate-less",
    "code_links": [
      "https://github.com/chandarlab/Hallucinate-less"
    ],
    "comment": null
  },
  {
    "title": "Response Quality Assessment for Retrieval-Augmented Generation via Conditional Conformal Factuality",
    "authors": "Naihe Feng, Yi Sui, Shiyi Hou, Jesse C. Cresswell, Ga Wu",
    "published": "2025-06-26",
    "arxiv_id": "2506.20978v1",
    "url": "http://arxiv.org/abs/2506.20978v1",
    "pdf_url": "http://arxiv.org/pdf/2506.20978v1",
    "category": "information_retrieval",
    "primary_category": "cs.IR",
    "abstract": "Existing research on Retrieval-Augmented Generation (RAG) primarily focuses\non improving overall question-answering accuracy, often overlooking the quality\nof sub-claims within generated responses. Recent methods that attempt to\nimprove RAG trustworthiness, such as through auto-evaluation metrics, lack\nprobabilistic guarantees or require ground truth answers. To address these\nlimitations, we propose Conformal-RAG, a novel framework inspired by recent\napplications of conformal prediction (CP) on large language models (LLMs).\nConformal-RAG leverages CP and internal information from the RAG mechanism to\noffer statistical guarantees on response quality. It ensures group-conditional\ncoverage spanning multiple sub-domains without requiring manual labelling of\nconformal sets, making it suitable for complex RAG applications. Compared to\nexisting RAG auto-evaluation methods, Conformal-RAG offers statistical\nguarantees on the quality of refined sub-claims, ensuring response reliability\nwithout the need for ground truth answers. Additionally, our experiments\ndemonstrate that by leveraging information from the RAG system, Conformal-RAG\nretains up to 60\\% more high-quality sub-claims from the response compared to\ndirect applications of CP to LLMs, while maintaining the same reliability\nguarantee.",
    "code_links": [
      "https://github.com/n4feng/ResponseQualityAssessment"
    ],
    "comment": "Accepted by SIGIR 2025 short paper, 5 pages, Code is available at\n  https://github.com/n4feng/ResponseQualityAssessment"
  },
  {
    "title": "EraRAG: Efficient and Incremental Retrieval Augmented Generation for Growing Corpora",
    "authors": "Fangyuan Zhang, Zhengjun Huang, Yingli Zhou, Qintian Guo, Zhixun Li, Wensheng Luo, Di Jiang, Yixiang Fang, Xiaofang Zhou",
    "published": "2025-06-26",
    "arxiv_id": "2506.20963v1",
    "url": "http://arxiv.org/abs/2506.20963v1",
    "pdf_url": "http://arxiv.org/pdf/2506.20963v1",
    "category": "information_retrieval",
    "primary_category": "cs.IR",
    "abstract": "Graph-based Retrieval-Augmented Generation (Graph-RAG) enhances large\nlanguage models (LLMs) by structuring retrieval over an external corpus.\nHowever, existing approaches typically assume a static corpus, requiring\nexpensive full-graph reconstruction whenever new documents arrive, limiting\ntheir scalability in dynamic, evolving environments. To address these\nlimitations, we introduce EraRAG, a novel multi-layered Graph-RAG framework\nthat supports efficient and scalable dynamic updates. Our method leverages\nhyperplane-based Locality-Sensitive Hashing (LSH) to partition and organize the\noriginal corpus into hierarchical graph structures, enabling efficient and\nlocalized insertions of new data without disrupting the existing topology. The\ndesign eliminates the need for retraining or costly recomputation while\npreserving high retrieval accuracy and low latency. Experiments on large-scale\nbenchmarks demonstrate that EraRag achieves up to an order of magnitude\nreduction in update time and token consumption compared to existing Graph-RAG\nsystems, while providing superior accuracy performance. This work offers a\npractical path forward for RAG systems that must operate over continually\ngrowing corpora, bridging the gap between retrieval efficiency and\nadaptability. Our code and data are available at\nhttps://github.com/EverM0re/EraRAG-Official.",
    "code_links": [
      "https://github.com/EverM0re/EraRAG-Official"
    ],
    "comment": "Under review"
  },
  {
    "title": "RAG-VisualRec: An Open Resource for Vision- and Text-Enhanced Retrieval-Augmented Generation in Recommendation",
    "authors": "Ali Tourani, Fatemeh Nazary, Yashar Deldjoo",
    "published": "2025-06-25",
    "arxiv_id": "2506.20817v1",
    "url": "http://arxiv.org/abs/2506.20817v1",
    "pdf_url": "http://arxiv.org/pdf/2506.20817v1",
    "category": "information_retrieval",
    "primary_category": "cs.IR",
    "abstract": "This paper addresses the challenge of developing multimodal recommender\nsystems for the movie domain, where limited metadata (e.g., title, genre) often\nhinders the generation of robust recommendations. We introduce a resource that\ncombines LLM-generated plot descriptions with trailer-derived visual embeddings\nin a unified pipeline supporting both Retrieval-Augmented Generation (RAG) and\ncollaborative filtering. Central to our approach is a data augmentation step\nthat transforms sparse metadata into richer textual signals, alongside fusion\nstrategies (e.g., PCA, CCA) that integrate visual cues. Experimental\nevaluations demonstrate that CCA-based fusion significantly boosts recall\ncompared to unimodal baselines, while an LLM-driven re-ranking step further\nimproves NDCG, particularly in scenarios with limited textual data. By\nreleasing this framework, we invite further exploration of multi-modal\nrecommendation techniques tailored to cold-start, novelty-focused, and\ndomain-specific settings. All code, data, and detailed documentation are\npublicly available at: https://github.com/RecSys-lab/RAG-VisualRec",
    "code_links": [
      "https://github.com/RecSys-lab/RAG-VisualRec"
    ],
    "comment": "20 pages, 6 figures, 5 tables"
  },
  {
    "title": "ReCode: Updating Code API Knowledge with Reinforcement Learning",
    "authors": "Haoze Wu, Yunzhi Yao, Wenhao Yu, Huajun Chen, Ningyu Zhang",
    "published": "2025-06-25",
    "arxiv_id": "2506.20495v1",
    "url": "http://arxiv.org/abs/2506.20495v1",
    "pdf_url": "http://arxiv.org/pdf/2506.20495v1",
    "category": "information_retrieval",
    "primary_category": "cs.CL",
    "abstract": "Large Language Models (LLMs) exhibit remarkable code generation capabilities\nbut falter when adapting to frequent updates in external library APIs. This\ncritical limitation, stemming from reliance on outdated API knowledge from\ntheir training data, even with access to current documentation, impedes\nreliable code generation in dynamic environments. To tackle this issue, we\npropose ReCode (rule-based Reinforcement learning for Code Update), a novel\nframework that mimics human programmer adaptation to API changes. Specifically,\nwe construct a dataset of approximately 2,000 data entries to train the LLMs to\nperform version migration based on updated information. Then, we introduce a\nmodified string similarity metric for code evaluation as the reward for\nreinforcement learning. Our experiments demonstrate that ReCode substantially\nboosts LLMs' code generation performance in dynamic API scenarios, especially\non the unseen CodeUpdateArena task. Crucially, compared to supervised\nfine-tuning, ReCode has less impact on LLMs' general code generation abilities.\nWe apply ReCode on various LLMs and reinforcement learning algorithms (GRPO and\nDAPO), all achieving consistent improvements. Notably, after training,\nQwen2.5-Coder-7B outperforms that of the 32B parameter code instruction-tuned\nmodel and the reasoning model with the same architecture. Code is available at\nhttps://github.com/zjunlp/ReCode.",
    "code_links": [
      "https://github.com/zjunlp/ReCode"
    ],
    "comment": "Work in progress"
  },
  {
    "title": "CoVE: Compressed Vocabulary Expansion Makes Better LLM-based Recommender Systems",
    "authors": "Haochen Zhang, Tianyi Zhang, Junze Yin, Oren Gal, Anshumali Shrivastava, Vladimir Braverman",
    "published": "2025-06-24",
    "arxiv_id": "2506.19993v1",
    "url": "http://arxiv.org/abs/2506.19993v1",
    "pdf_url": "http://arxiv.org/pdf/2506.19993v1",
    "category": "information_retrieval",
    "primary_category": "cs.IR",
    "abstract": "Recommender systems play a pivotal role in providing relevant content to\nusers. With the rapid development of large language models (LLMs), researchers\nhave begun utilizing LLMs to build more powerful recommender systems. However,\nexisting approaches that focus on aligning LLMs with recommendation tasks do\nnot fully leverage their sequential information processing capabilities,\nleading to suboptimal performance.\n  In this paper, we propose a novel system called compressed vocabulary\nexpansion (CoVE). In CoVE, each item is assigned a unique ID within the\nexpanded vocabulary. Our framework effectively capitalizes on sequence\nunderstanding abilities of LLMs, significantly enhancing their performance on\nrecommendation tasks. Additionally, we compress the embedding layer, making\nCoVE practical for large-scale industrial applications. The effectiveness and\nperformance of CoVE are demonstrated through comprehensive experiments on\nmultiple recommendation datasets and comparisons with prior works. Our code can\nbe found at https://github.com/HaochenZhang717/CoVE-official-Repo.",
    "code_links": [
      "https://github.com/HaochenZhang717/CoVE-official-Repo"
    ],
    "comment": "Accepted by ACL 2025 Findings"
  },
  {
    "title": "From Web Search towards Agentic Deep Research: Incentivizing Search with Reasoning Agents",
    "authors": "Weizhi Zhang, Yangning Li, Yuanchen Bei, Junyu Luo, Guancheng Wan, Liangwei Yang, Chenxuan Xie, Yuyao Yang, Wei-Chieh Huang, Chunyu Miao, Henry Peng Zou, Xiao Luo, Yusheng Zhao, Yankai Chen, Chunkit Chan, Peilin Zhou, Xinyang Zhang, Chenwei Zhang, Jingbo Shang, Ming Zhang, Yangqiu Song, Irwin King, Philip S. Yu",
    "published": "2025-06-23",
    "arxiv_id": "2506.18959v2",
    "url": "http://arxiv.org/abs/2506.18959v2",
    "pdf_url": "http://arxiv.org/pdf/2506.18959v2",
    "category": "information_retrieval",
    "primary_category": "cs.IR",
    "abstract": "Information retrieval is a cornerstone of modern knowledge acquisition,\nenabling billions of queries each day across diverse domains. However,\ntraditional keyword-based search engines are increasingly inadequate for\nhandling complex, multi-step information needs. Our position is that Large\nLanguage Models (LLMs), endowed with reasoning and agentic capabilities, are\nushering in a new paradigm termed Agentic Deep Research. These systems\ntranscend conventional information search techniques by tightly integrating\nautonomous reasoning, iterative retrieval, and information synthesis into a\ndynamic feedback loop. We trace the evolution from static web search to\ninteractive, agent-based systems that plan, explore, and learn. We also\nintroduce a test-time scaling law to formalize the impact of computational\ndepth on reasoning and search. Supported by benchmark results and the rise of\nopen-source implementations, we demonstrate that Agentic Deep Research not only\nsignificantly outperforms existing approaches, but is also poised to become the\ndominant paradigm for future information seeking. All the related resources,\nincluding industry products, research papers, benchmark datasets, and\nopen-source implementations, are collected for the community in\nhttps://github.com/DavidZWZ/Awesome-Deep-Research.",
    "code_links": [
      "https://github.com/DavidZWZ/Awesome-Deep-Research"
    ],
    "comment": null
  },
  {
    "title": "Harnessing the Power of Reinforcement Learning for Language-Model-Based Information Retriever via Query-Document Co-Augmentation",
    "authors": "Jingming Liu, Yumeng Li, Wei Shi, Yao-Xiang Ding, Hui Su, Kun Zhou",
    "published": "2025-06-23",
    "arxiv_id": "2506.18670v1",
    "url": "http://arxiv.org/abs/2506.18670v1",
    "pdf_url": "http://arxiv.org/pdf/2506.18670v1",
    "category": "information_retrieval",
    "primary_category": "cs.IR",
    "abstract": "Recent studies have proposed leveraging Large Language Models (LLMs) as\ninformation retrievers through query rewriting. However, for challenging\ncorpora, we argue that enhancing queries alone is insufficient for robust\nsemantic matching; the LLM should also have sufficient understanding of the\ncorpus by directly handling and augmenting the documents themselves. To this\nend, we present an LLM-based retriever empowered to augment both user queries\nand corpus documents, with its policy fully explored via reinforcement learning\n(RL) and minimal human inductive bias. Notably, we find that simply allowing\nthe LLM to modify documents yields little benefit unless paired with our\ncarefully designed bidirectional RL framework, which enables the LLM to\nsimultaneously learn and collaborate on both query and document augmentation\npolicies. A key technical challenge in realizing such a framework lies in\njointly updating both policies during training, where the rewards for the two\ndirections depend on each other, making their entangled reward intractable. Our\napproach addresses this by introducing a reward sampling strategy and a\nspecifically designed RL algorithm that enables effective training with these\nsampled rewards. Experimental results demonstrate that our approach\nsignificantly enhances LLM-based retrieval performance in both sparse and dense\nsettings, particularly in difficult retrieval domains, and achieves strong\ncross-benchmark generalization. Our code is released at\nhttps://github.com/liujm2001/CoAugRetriever.",
    "code_links": [
      "https://github.com/liujm2001/CoAugRetriever"
    ],
    "comment": null
  },
  {
    "title": "Hierarchical Patch Compression for ColPali: Efficient Multi-Vector Document Retrieval with Dynamic Pruning and Quantization",
    "authors": "Duong Bach",
    "published": "2025-06-19",
    "arxiv_id": "2506.21601v1",
    "url": "http://arxiv.org/abs/2506.21601v1",
    "pdf_url": "http://arxiv.org/pdf/2506.21601v1",
    "category": "information_retrieval",
    "primary_category": "cs.IR",
    "abstract": "Multi-vector document retrieval systems, such as ColPali, excel in\nfine-grained matching for complex queries but incur significant storage and\ncomputational costs due to their reliance on high-dimensional patch embeddings\nand late-interaction scoring. To address these challenges, we propose\nHPC-ColPali, a Hierarchical Patch Compression framework that enhances the\nefficiency of ColPali while preserving its retrieval accuracy. Our approach\nintegrates three innovative techniques: (1) K-Means quantization, which\ncompresses patch embeddings into 1-byte centroid indices, achieving up to\n32$\\times$ storage reduction; (2) attention-guided dynamic pruning, utilizing\nVision-Language Model attention weights to retain only the top-$p\\%$ most\nsalient patches, reducing late-interaction computation by up to 60\\% with less\nthan 2\\% nDCG@10 loss; and (3) optional binary encoding of centroid indices\ninto $b$-bit strings ($b=\\lceil\\log_2 K\\rceil$), enabling rapid Hamming\ndistance-based similarity search for resource-constrained environments.\nEvaluated on the ViDoRe and SEC-Filings datasets, HPC-ColPali achieves 30--50\\%\nlower query latency under HNSW indexing while maintaining high retrieval\nprecision. When integrated into a Retrieval-Augmented Generation pipeline for\nlegal summarization, it reduces hallucination rates by 30\\% and halves\nend-to-end latency. These advancements establish HPC-ColPali as a scalable and\nefficient solution for multi-vector document retrieval across diverse\napplications. Code is available at https://github.com/DngBack/HPC-ColPali.",
    "code_links": [
      "https://github.com/DngBack/HPC-ColPali"
    ],
    "comment": "9 pages"
  },
  {
    "title": "Schema-R1: A reasoning training approach for schema linking in Text-to-SQL Task",
    "authors": "Wuzhenghong Wen, Su Pan, yuwei Sun",
    "published": "2025-06-13",
    "arxiv_id": "2506.11986v1",
    "url": "http://arxiv.org/abs/2506.11986v1",
    "pdf_url": "http://arxiv.org/pdf/2506.11986v1",
    "category": "databases",
    "primary_category": "cs.AI",
    "abstract": "Schema linking is a critical step in Text-to-SQL task, aiming to accurately\npredict the table names and column names required for the SQL query based on\nthe given question. However, current fine-tuning approaches for schema linking\nmodels employ a rote-learning paradigm, excessively optimizing for ground truth\nschema linking outcomes while compromising reasoning ability. This limitation\narises because of the difficulty in acquiring a high-quality reasoning sample\nfor downstream tasks. To address this, we propose Schema-R1, a reasoning schema\nlinking model trained using reinforcement learning. Specifically, Schema-R1\nconsists of three key steps: constructing small batches of high-quality\nreasoning samples, supervised fine-tuning for cold-start initialization, and\nrule-based reinforcement learning training. The final results demonstrate that\nour method effectively enhances the reasoning ability of the schema linking\nmodel, achieving a 10\\% improvement in filter accuracy compared to the existing\nmethod. Our code is available at https://github.com/hongWin/Schema-R1/.",
    "code_links": [
      "https://github.com/hongWin/Schema-R1"
    ],
    "comment": "11 pages, 3 figures, conference"
  },
  {
    "title": "Bridging RDF Knowledge Graphs with Graph Neural Networks for Semantically-Rich Recommender Systems",
    "authors": "Michael Färber, David Lamprecht, Yuni Susanti",
    "published": "2025-06-10",
    "arxiv_id": "2506.08743v1",
    "url": "http://arxiv.org/abs/2506.08743v1",
    "pdf_url": "http://arxiv.org/pdf/2506.08743v1",
    "category": "databases",
    "primary_category": "cs.IR",
    "abstract": "Graph Neural Networks (GNNs) have substantially advanced the field of\nrecommender systems. However, despite the creation of more than a thousand\nknowledge graphs (KGs) under the W3C standard RDF, their rich semantic\ninformation has not yet been fully leveraged in GNN-based recommender systems.\nTo address this gap, we propose a comprehensive integration of RDF KGs with\nGNNs that utilizes both the topological information from RDF object properties\nand the content information from RDF datatype properties. Our main focus is an\nin-depth evaluation of various GNNs, analyzing how different semantic feature\ninitializations and types of graph structure heterogeneity influence their\nperformance in recommendation tasks. Through experiments across multiple\nrecommendation scenarios involving multi-million-node RDF graphs, we\ndemonstrate that harnessing the semantic richness of RDF KGs significantly\nimproves recommender systems and lays the groundwork for GNN-based recommender\nsystems for the Linked Open Data cloud. The code and data are available on our\nGitHub repository: https://github.com/davidlamprecht/rdf-gnn-recommendation",
    "code_links": [
      "https://github.com/davidlamprecht/rdf-gnn-recommendation"
    ],
    "comment": "Accepted at DASFAA 2025"
  },
  {
    "title": "KramaBench: A Benchmark for AI Systems on Data-to-Insight Pipelines over Data Lakes",
    "authors": "Eugenie Lai, Gerardo Vitagliano, Ziyu Zhang, Sivaprasad Sudhir, Om Chabra, Anna Zeng, Anton A. Zabreyko, Chenning Li, Ferdi Kossmann, Jialin Ding, Jun Chen, Markos Markakis, Matthew Russo, Weiyang Wang, Ziniu Wu, Michael J. Cafarella, Lei Cao, Samuel Madden, Tim Kraska",
    "published": "2025-06-06",
    "arxiv_id": "2506.06541v1",
    "url": "http://arxiv.org/abs/2506.06541v1",
    "pdf_url": "http://arxiv.org/pdf/2506.06541v1",
    "category": "databases",
    "primary_category": "cs.DB",
    "abstract": "Constructing real-world data-to-insight pipelines often involves data\nextraction from data lakes, data integration across heterogeneous data sources,\nand diverse operations from data cleaning to analysis. The design and\nimplementation of data science pipelines require domain knowledge, technical\nexpertise, and even project-specific insights. AI systems have shown remarkable\nreasoning, coding, and understanding capabilities. However, it remains unclear\nto what extent these capabilities translate into successful design and\nexecution of such complex pipelines. We introduce KRAMABENCH: a benchmark\ncomposed of 104 manually-curated real-world data science pipelines spanning\n1700 data files from 24 data sources in 6 different domains. We show that these\npipelines test the end-to-end capabilities of AI systems on data processing,\nrequiring data discovery, wrangling and cleaning, efficient processing,\nstatistical reasoning, and orchestrating data processing steps given a\nhigh-level task. Our evaluation tests 5 general models and 3 code generation\nmodels using our reference framework, DS-GURU, which instructs the AI model to\ndecompose a question into a sequence of subtasks, reason through each step, and\nsynthesize Python code that implements the proposed design. Our results on\nKRAMABENCH show that, although the models are sufficiently capable of solving\nwell-specified data science code generation tasks, when extensive data\nprocessing and domain knowledge are required to construct real-world data\nscience pipelines, existing out-of-box models fall short. Progress on\nKramaBench represents crucial steps towards developing autonomous data science\nagents for real-world applications. Our code, reference framework, and data are\navailable at https://github.com/mitdbg/KramaBench.",
    "code_links": [
      "https://github.com/mitdbg/KramaBench"
    ],
    "comment": null
  },
  {
    "title": "MMTU: A Massive Multi-Task Table Understanding and Reasoning Benchmark",
    "authors": "Junjie Xing, Yeye He, Mengyu Zhou, Haoyu Dong, Shi Han, Lingjiao Chen, Dongmei Zhang, Surajit Chaudhuri, H. V. Jagadish",
    "published": "2025-06-05",
    "arxiv_id": "2506.05587v1",
    "url": "http://arxiv.org/abs/2506.05587v1",
    "pdf_url": "http://arxiv.org/pdf/2506.05587v1",
    "category": "databases",
    "primary_category": "cs.AI",
    "abstract": "Tables and table-based use cases play a crucial role in many important\nreal-world applications, such as spreadsheets, databases, and computational\nnotebooks, which traditionally require expert-level users like data engineers,\ndata analysts, and database administrators to operate. Although LLMs have shown\nremarkable progress in working with tables (e.g., in spreadsheet and database\ncopilot scenarios), comprehensive benchmarking of such capabilities remains\nlimited. In contrast to an extensive and growing list of NLP benchmarks,\nevaluations of table-related tasks are scarce, and narrowly focus on tasks like\nNL-to-SQL and Table-QA, overlooking the broader spectrum of real-world tasks\nthat professional users face. This gap limits our understanding and model\nprogress in this important area.\n  In this work, we introduce MMTU, a large-scale benchmark with over 30K\nquestions across 25 real-world table tasks, designed to comprehensively\nevaluate models ability to understand, reason, and manipulate real tables at\nthe expert-level. These tasks are drawn from decades' worth of computer science\nresearch on tabular data, with a focus on complex table tasks faced by\nprofessional users. We show that MMTU require a combination of skills --\nincluding table understanding, reasoning, and coding -- that remain challenging\nfor today's frontier models, where even frontier reasoning models like OpenAI\no4-mini and DeepSeek R1 score only around 60%, suggesting significant room for\nimprovement. We highlight key findings in our evaluation using MMTU and hope\nthat this benchmark drives further advances in understanding and developing\nfoundation models for structured data processing and analysis. Our code and\ndata are available at https://github.com/MMTU-Benchmark/MMTU and\nhttps://huggingface.co/datasets/MMTU-benchmark/MMTU.",
    "code_links": [
      "https://github.com/MMTU-Benchmark/MMTU"
    ],
    "comment": null
  },
  {
    "title": "VecFlow: A High-Performance Vector Data Management System for Filtered-Search on GPUs",
    "authors": "Jingyi Xi, Chenghao Mo, Benjamin Karsin, Artem Chirkin, Mingqin Li, Minjia Zhang",
    "published": "2025-06-01",
    "arxiv_id": "2506.00812v1",
    "url": "http://arxiv.org/abs/2506.00812v1",
    "pdf_url": "http://arxiv.org/pdf/2506.00812v1",
    "category": "databases",
    "primary_category": "cs.DB",
    "abstract": "Vector search and database systems have become a keystone component in many\nAI applications. While many prior research has investigated how to accelerate\nthe performance of generic vector search, emerging AI applications require\nrunning more sophisticated vector queries efficiently, such as vector search\nwith attribute filters. Unfortunately, recent filtered-ANNS solutions are\nprimarily designed for CPUs, with few exploration and limited performance of\nfiltered-ANNS that take advantage of the massive parallelism offered by GPUs.\nIn this paper, we present VecFlow, a novel high-performance vector filtered\nsearch system that achieves unprecedented high throughput and recall while\nobtaining low latency for filtered-ANNS on GPUs. We propose a novel\nlabel-centric indexing and search algorithm that significantly improves the\nselectivity of ANNS with filters. In addition to algorithmic level\noptimization, we provide architectural-aware optimization for VecFlow's\nfunctional modules, effectively supporting both small batch and large batch\nqueries, and single-label and multi-label query processing. Experimental\nresults on NVIDIA A100 GPU over several public available datasets validate that\nVecFlow achieves 5 million QPS for recall 90%, outperforming state-of-the-art\nCPU-based solutions such as Filtered-DiskANN by up to 135 times. Alternatively,\nVecFlow can easily extend its support to high recall 99% regime, whereas strong\nGPU-based baselines plateau at around 80% recall. The source code is available\nat https://github.com/Supercomputing-System-AI-Lab/VecFlow.",
    "code_links": [
      "https://github.com/Supercomputing-System-AI-Lab/VecFlow"
    ],
    "comment": null
  }
]