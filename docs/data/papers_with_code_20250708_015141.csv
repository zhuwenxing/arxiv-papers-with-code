title,authors,published,arxiv_id,url,pdf_url,category,primary_category,abstract,code_links,comment
Decoupled Planning and Execution: A Hierarchical Reasoning Framework for Deep Search,"Jiajie Jin, Xiaoxi Li, Guanting Dong, Yuyao Zhang, Yutao Zhu, Yang Zhao, Hongjin Qian, Zhicheng Dou",2025-07-03,2507.02652v1,http://arxiv.org/abs/2507.02652v1,http://arxiv.org/pdf/2507.02652v1,information_retrieval,cs.AI,"Complex information needs in real-world search scenarios demand deep
reasoning and knowledge synthesis across diverse sources, which traditional
retrieval-augmented generation (RAG) pipelines struggle to address effectively.
Current reasoning-based approaches suffer from a fundamental limitation: they
use a single model to handle both high-level planning and detailed execution,
leading to inefficient reasoning and limited scalability. In this paper, we
introduce HiRA, a hierarchical framework that separates strategic planning from
specialized execution. Our approach decomposes complex search tasks into
focused subtasks, assigns each subtask to domain-specific agents equipped with
external tools and reasoning capabilities, and coordinates the results through
a structured integration mechanism. This separation prevents execution details
from disrupting high-level reasoning while enabling the system to leverage
specialized expertise for different types of information processing.
Experiments on four complex, cross-modal deep search benchmarks demonstrate
that HiRA significantly outperforms state-of-the-art RAG and agent-based
systems. Our results show improvements in both answer quality and system
efficiency, highlighting the effectiveness of decoupled planning and execution
for multi-step information seeking tasks. Our code is available at
https://github.com/ignorejjj/HiRA.",https://github.com/ignorejjj/HiRA,9 pages
Listwise Preference Alignment Optimization for Tail Item Recommendation,"Zihao Li, Chao Yang, Tong Zhang, Yakun Chen, Xianzhi Wang, Guandong Xu, Daoyi Dong",2025-07-03,2507.02255v1,http://arxiv.org/abs/2507.02255v1,http://arxiv.org/pdf/2507.02255v1,information_retrieval,cs.IR,"Preference alignment has achieved greater success on Large Language Models
(LLMs) and drawn broad interest in recommendation research. Existing preference
alignment methods for recommendation either require explicit reward modeling or
only support pairwise preference comparison. The former directly increases
substantial computational costs, while the latter hinders training efficiency
on negative samples. Moreover, no existing effort has explored preference
alignment solutions for tail-item recommendation. To bridge the above gaps, we
propose LPO4Rec, which extends the Bradley-Terry model from pairwise comparison
to listwise comparison, to improve the efficiency of model training.
Specifically, we derive a closed form optimal policy to enable more efficient
and effective training without explicit reward modeling. We also present an
adaptive negative sampling and reweighting strategy to prioritize tail items
during optimization and enhance performance in tail-item recommendations.
Besides, we theoretically prove that optimizing the listwise preference
optimization (LPO) loss is equivalent to maximizing the upper bound of the
optimal reward. Our experiments on three public datasets show that our method
outperforms 10 baselines by a large margin, achieving up to 50% performance
improvement while reducing 17.9% GPU memory usage when compared with direct
preference optimization (DPO) in tail-item recommendation. Our code is
available at https://github.com/Yuhanleeee/LPO4Rec.",https://github.com/Yuhanleeee/LPO4Rec,
Confidence and Stability of Global and Pairwise Scores in NLP Evaluation,"Georgii Levtsov, Dmitry Ustalov",2025-07-02,2507.01633v1,http://arxiv.org/abs/2507.01633v1,http://arxiv.org/pdf/2507.01633v1,information_retrieval,cs.CL,"With the advent of highly capable instruction-tuned neural language models,
benchmarking in natural language processing (NLP) is increasingly shifting
towards pairwise comparison leaderboards, such as LMSYS Arena, from traditional
global pointwise scores (e.g., GLUE, BIG-bench, SWE-bench). This paper
empirically investigates the strengths and weaknesses of both global scores and
pairwise comparisons to aid decision-making in selecting appropriate model
evaluation strategies. Through computational experiments on synthetic and
real-world datasets using standard global metrics and the popular Bradley-Terry
model for pairwise comparisons, we found that while global scores provide more
reliable overall rankings, they can underestimate strong models with rare,
significant errors or low confidence. Conversely, pairwise comparisons are
particularly effective for identifying strong contenders among models with
lower global scores, especially where quality metrics are hard to define (e.g.,
text generation), though they require more comparisons to converge if ties are
frequent. Our code and data are available at
https://github.com/HSPyroblast/srw-ranking under a permissive license.",https://github.com/HSPyroblast/srw-ranking,"8 pages, accepted at ACL SRW 2025"
Uncertainty-Aware Complex Scientific Table Data Extraction,"Kehinde Ajayi, Yi He, Jian Wu",2025-07-02,2507.02009v1,http://arxiv.org/abs/2507.02009v1,http://arxiv.org/pdf/2507.02009v1,information_retrieval,cs.IR,"Table structure recognition (TSR) and optical character recognition (OCR)
play crucial roles in extracting structured data from tables in scientific
documents. However, existing extraction frameworks built on top of TSR and OCR
methods often fail to quantify the uncertainties of extracted results. To
obtain highly accurate data for scientific domains, all extracted data must be
manually verified, which can be time-consuming and labor-intensive. We propose
a framework that performs uncertainty-aware data extraction for complex
scientific tables, built on conformal prediction, a model-agnostic method for
uncertainty quantification (UQ). We explored various uncertainty scoring
methods to aggregate the uncertainties introduced by TSR and OCR. We rigorously
evaluated the framework using a standard benchmark and an in-house dataset
consisting of complex scientific tables in six scientific domains. The results
demonstrate the effectiveness of using UQ for extraction error detection, and
by manually verifying only 47\% of extraction results, the data quality can be
improved by 30\%. Our work quantitatively demonstrates the role of UQ with the
potential of improving the efficiency in the human-machine cooperation process
to obtain scientifically usable data from complex tables in scientific
documents. All code and data are available on GitHub at
https://github.com/lamps-lab/TSR-OCR-UQ/tree/main.",https://github.com/lamps-lab/TSR-OCR-UQ,
Why Multi-Interest Fairness Matters: Hypergraph Contrastive Multi-Interest Learning for Fair Conversational Recommender System,"Yongsen Zheng, Zongxuan Xie, Guohua Wang, Ziyao Liu, Liang Lin, Kwok-Yan Lam",2025-07-01,2507.02000v1,http://arxiv.org/abs/2507.02000v1,http://arxiv.org/pdf/2507.02000v1,information_retrieval,cs.IR,"Unfairness is a well-known challenge in Recommender Systems (RSs), often
resulting in biased outcomes that disadvantage users or items based on
attributes such as gender, race, age, or popularity. Although some approaches
have started to improve fairness recommendation in offline or static contexts,
the issue of unfairness often exacerbates over time, leading to significant
problems like the Matthew effect, filter bubbles, and echo chambers. To address
these challenges, we proposed a novel framework, Hypergraph Contrastive
Multi-Interest Learning for Fair Conversational Recommender System (HyFairCRS),
aiming to promote multi-interest diversity fairness in dynamic and interactive
Conversational Recommender Systems (CRSs). HyFairCRS first captures a wide
range of user interests by establishing diverse hypergraphs through contrastive
learning. These interests are then utilized in conversations to generate
informative responses and ensure fair item predictions within the dynamic
user-system feedback loop. Experiments on two CRS-based datasets show that
HyFairCRS achieves a new state-of-the-art performance while effectively
alleviating unfairness. Our code is available at
https://github.com/zysensmile/HyFairCRS.",https://github.com/zysensmile/HyFairCRS,
MassTool: A Multi-Task Search-Based Tool Retrieval Framework for Large Language Models,"Jianghao Lin, Xinyuan Wang, Xinyi Dai, Menghui Zhu, Bo Chen, Ruiming Tang, Yong Yu, Weinan Zhang",2025-07-01,2507.00487v2,http://arxiv.org/abs/2507.00487v2,http://arxiv.org/pdf/2507.00487v2,information_retrieval,cs.IR,"Tool retrieval is a critical component in enabling large language models
(LLMs) to interact effectively with external tools. It aims to precisely filter
the massive tools into a small set of candidates for the downstream
tool-augmented LLMs. However, most existing approaches primarily focus on
optimizing tool representations, often neglecting the importance of precise
query comprehension. To address this gap, we introduce MassTool, a multi-task
search-based framework designed to enhance both query representation and tool
retrieval accuracy. MassTool employs a two-tower architecture: a tool usage
detection tower that predicts the need for function calls, and a tool retrieval
tower that leverages a query-centric graph convolution network (QC-GCN) for
effective query-tool matching. It also incorporates search-based user intent
modeling (SUIM) to handle diverse and out-of-distribution queries, alongside an
adaptive knowledge transfer (AdaKT) module for efficient multi-task learning.
By jointly optimizing tool usage detection loss, list-wise retrieval loss, and
contrastive regularization loss, MassTool establishes a robust dual-step
sequential decision-making pipeline for precise query understanding. Extensive
experiments demonstrate its effectiveness in improving retrieval accuracy. Our
code is available at https://github.com/wxydada/MassTool.",https://github.com/wxydada/MassTool,
Thought-Augmented Planning for LLM-Powered Interactive Recommender Agent,"Haocheng Yu, Yaxiong Wu, Hao Wang, Wei Guo, Yong Liu, Yawen Li, Yuyang Ye, Junping Du, Enhong Chen",2025-06-30,2506.23485v1,http://arxiv.org/abs/2506.23485v1,http://arxiv.org/pdf/2506.23485v1,information_retrieval,cs.CL,"Interactive recommendation is a typical information-seeking task that allows
users to interactively express their needs through natural language and obtain
personalized recommendations. Large language model-powered (LLM-powered) agents
have become a new paradigm in interactive recommendations, effectively
capturing users' real-time needs and enhancing personalized experiences.
However, due to limited planning and generalization capabilities, existing
formulations of LLM-powered interactive recommender agents struggle to
effectively address diverse and complex user intents, such as intuitive,
unrefined, or occasionally ambiguous requests. To tackle this challenge, we
propose a novel thought-augmented interactive recommender agent system (TAIRA)
that addresses complex user intents through distilled thought patterns.
Specifically, TAIRA is designed as an LLM-powered multi-agent system featuring
a manager agent that orchestrates recommendation tasks by decomposing user
needs and planning subtasks, with its planning capacity strengthened through
Thought Pattern Distillation (TPD), a thought-augmentation method that extracts
high-level thoughts from the agent's and human experts' experiences. Moreover,
we designed a set of user simulation schemes to generate personalized queries
of different difficulties and evaluate the recommendations based on specific
datasets. Through comprehensive experiments conducted across multiple datasets,
TAIRA exhibits significantly enhanced performance compared to existing methods.
Notably, TAIRA shows a greater advantage on more challenging tasks while
generalizing effectively on novel tasks, further validating its superiority in
managing complex user intents within interactive recommendation systems. The
code is publicly available at:https://github.com/Alcein/TAIRA.",https://github.com/Alcein/TAIRA,
JointRank: Rank Large Set with Single Pass,Evgeny Dedov,2025-06-27,2506.22262v1,http://arxiv.org/abs/2506.22262v1,http://arxiv.org/pdf/2506.22262v1,information_retrieval,cs.IR,"Efficiently ranking relevant items from large candidate pools is a
cornerstone of modern information retrieval systems -- such as web search,
recommendation, and retrieval-augmented generation. Listwise rerankers, which
improve relevance by jointly considering multiple candidates, are often limited
in practice: either by model input size constraints, or by degraded quality
when processing large sets. We propose a model-agnostic method for fast
reranking large sets that exceed a model input limits. The method first
partitions candidate items into overlapping blocks, each of which is ranked
independently in parallel. Implicit pairwise comparisons are then derived from
these local rankings. Finally, these comparisons are aggregated to construct a
global ranking using algorithms such as Winrate or PageRank. Experiments on
TREC DL-2019 show that our method achieves an nDCG@10 of 70.88 compared to the
57.68 for full-context listwise approach using gpt-4.1-mini as long-context
model, while reducing latency from 21 to 8 seconds.
  The implementation of the algorithm and the experiments is available in the
repository: https://github.com/V3RGANz/jointrank",https://github.com/V3RGANz/jointrank,ICTIR'25 Accepted
skLEP: A Slovak General Language Understanding Benchmark,"Marek Šuppa, Andrej Ridzik, Daniel Hládek, Tomáš Javůrek, Viktória Ondrejová, Kristína Sásiková, Martin Tamajka, Marián Šimko",2025-06-26,2506.21508v1,http://arxiv.org/abs/2506.21508v1,http://arxiv.org/pdf/2506.21508v1,information_retrieval,cs.CL,"In this work, we introduce skLEP, the first comprehensive benchmark
specifically designed for evaluating Slovak natural language understanding
(NLU) models. We have compiled skLEP to encompass nine diverse tasks that span
token-level, sentence-pair, and document-level challenges, thereby offering a
thorough assessment of model capabilities. To create this benchmark, we curated
new, original datasets tailored for Slovak and meticulously translated
established English NLU resources. Within this paper, we also present the first
systematic and extensive evaluation of a wide array of Slovak-specific,
multilingual, and English pre-trained language models using the skLEP tasks.
Finally, we also release the complete benchmark data, an open-source toolkit
facilitating both fine-tuning and evaluation of models, and a public
leaderboard at https://github.com/slovak-nlp/sklep in the hopes of fostering
reproducibility and drive future research in Slovak NLU.",https://github.com/slovak-nlp/sklep,ACL 2025 Findings
Small Encoders Can Rival Large Decoders in Detecting Groundedness,"Istabrak Abbes, Gabriele Prato, Quentin Fournier, Fernando Rodriguez, Alaa Boukhary, Adam Elwood, Sarath Chandar",2025-06-26,2506.21288v1,http://arxiv.org/abs/2506.21288v1,http://arxiv.org/pdf/2506.21288v1,information_retrieval,cs.CL,"Augmenting large language models (LLMs) with external context significantly
improves their performance in natural language processing (NLP) tasks. However,
LLMs struggle to answer queries reliably when the provided context lacks
information, often resorting to ungrounded speculation or internal knowledge.
Groundedness - generating responses strictly supported by the context - is
essential for ensuring factual consistency and trustworthiness. This study
focuses on detecting whether a given query is grounded in a document provided
in context before the costly answer generation by LLMs. Such a detection
mechanism can significantly reduce both inference time and resource
consumption. We show that lightweight, task specific encoder models such as
RoBERTa and NomicBERT, fine-tuned on curated datasets, can achieve accuracy
comparable to state-of-the-art LLMs, such as Llama3 8B and GPT4o, in
groundedness detection while reducing inference latency by orders of magnitude.
The code is available at : https://github.com/chandarlab/Hallucinate-less",https://github.com/chandarlab/Hallucinate-less,
Response Quality Assessment for Retrieval-Augmented Generation via Conditional Conformal Factuality,"Naihe Feng, Yi Sui, Shiyi Hou, Jesse C. Cresswell, Ga Wu",2025-06-26,2506.20978v1,http://arxiv.org/abs/2506.20978v1,http://arxiv.org/pdf/2506.20978v1,information_retrieval,cs.IR,"Existing research on Retrieval-Augmented Generation (RAG) primarily focuses
on improving overall question-answering accuracy, often overlooking the quality
of sub-claims within generated responses. Recent methods that attempt to
improve RAG trustworthiness, such as through auto-evaluation metrics, lack
probabilistic guarantees or require ground truth answers. To address these
limitations, we propose Conformal-RAG, a novel framework inspired by recent
applications of conformal prediction (CP) on large language models (LLMs).
Conformal-RAG leverages CP and internal information from the RAG mechanism to
offer statistical guarantees on response quality. It ensures group-conditional
coverage spanning multiple sub-domains without requiring manual labelling of
conformal sets, making it suitable for complex RAG applications. Compared to
existing RAG auto-evaluation methods, Conformal-RAG offers statistical
guarantees on the quality of refined sub-claims, ensuring response reliability
without the need for ground truth answers. Additionally, our experiments
demonstrate that by leveraging information from the RAG system, Conformal-RAG
retains up to 60\% more high-quality sub-claims from the response compared to
direct applications of CP to LLMs, while maintaining the same reliability
guarantee.",https://github.com/n4feng/ResponseQualityAssessment,"Accepted by SIGIR 2025 short paper, 5 pages, Code is available at
  https://github.com/n4feng/ResponseQualityAssessment"
EraRAG: Efficient and Incremental Retrieval Augmented Generation for Growing Corpora,"Fangyuan Zhang, Zhengjun Huang, Yingli Zhou, Qintian Guo, Zhixun Li, Wensheng Luo, Di Jiang, Yixiang Fang, Xiaofang Zhou",2025-06-26,2506.20963v2,http://arxiv.org/abs/2506.20963v2,http://arxiv.org/pdf/2506.20963v2,information_retrieval,cs.IR,"Graph-based Retrieval-Augmented Generation (Graph-RAG) enhances large
language models (LLMs) by structuring retrieval over an external corpus.
However, existing approaches typically assume a static corpus, requiring
expensive full-graph reconstruction whenever new documents arrive, limiting
their scalability in dynamic, evolving environments. To address these
limitations, we introduce EraRAG, a novel multi-layered Graph-RAG framework
that supports efficient and scalable dynamic updates. Our method leverages
hyperplane-based Locality-Sensitive Hashing (LSH) to partition and organize the
original corpus into hierarchical graph structures, enabling efficient and
localized insertions of new data without disrupting the existing topology. The
design eliminates the need for retraining or costly recomputation while
preserving high retrieval accuracy and low latency. Experiments on large-scale
benchmarks demonstrate that EraRag achieves up to an order of magnitude
reduction in update time and token consumption compared to existing Graph-RAG
systems, while providing superior accuracy performance. This work offers a
practical path forward for RAG systems that must operate over continually
growing corpora, bridging the gap between retrieval efficiency and
adaptability. Our code and data are available at
https://github.com/EverM0re/EraRAG-Official.",https://github.com/EverM0re/EraRAG-Official,Under review
RAG-VisualRec: An Open Resource for Vision- and Text-Enhanced Retrieval-Augmented Generation in Recommendation,"Ali Tourani, Fatemeh Nazary, Yashar Deldjoo",2025-06-25,2506.20817v1,http://arxiv.org/abs/2506.20817v1,http://arxiv.org/pdf/2506.20817v1,information_retrieval,cs.IR,"This paper addresses the challenge of developing multimodal recommender
systems for the movie domain, where limited metadata (e.g., title, genre) often
hinders the generation of robust recommendations. We introduce a resource that
combines LLM-generated plot descriptions with trailer-derived visual embeddings
in a unified pipeline supporting both Retrieval-Augmented Generation (RAG) and
collaborative filtering. Central to our approach is a data augmentation step
that transforms sparse metadata into richer textual signals, alongside fusion
strategies (e.g., PCA, CCA) that integrate visual cues. Experimental
evaluations demonstrate that CCA-based fusion significantly boosts recall
compared to unimodal baselines, while an LLM-driven re-ranking step further
improves NDCG, particularly in scenarios with limited textual data. By
releasing this framework, we invite further exploration of multi-modal
recommendation techniques tailored to cold-start, novelty-focused, and
domain-specific settings. All code, data, and detailed documentation are
publicly available at: https://github.com/RecSys-lab/RAG-VisualRec",https://github.com/RecSys-lab/RAG-VisualRec,"20 pages, 6 figures, 5 tables"
ReCode: Updating Code API Knowledge with Reinforcement Learning,"Haoze Wu, Yunzhi Yao, Wenhao Yu, Huajun Chen, Ningyu Zhang",2025-06-25,2506.20495v1,http://arxiv.org/abs/2506.20495v1,http://arxiv.org/pdf/2506.20495v1,information_retrieval,cs.CL,"Large Language Models (LLMs) exhibit remarkable code generation capabilities
but falter when adapting to frequent updates in external library APIs. This
critical limitation, stemming from reliance on outdated API knowledge from
their training data, even with access to current documentation, impedes
reliable code generation in dynamic environments. To tackle this issue, we
propose ReCode (rule-based Reinforcement learning for Code Update), a novel
framework that mimics human programmer adaptation to API changes. Specifically,
we construct a dataset of approximately 2,000 data entries to train the LLMs to
perform version migration based on updated information. Then, we introduce a
modified string similarity metric for code evaluation as the reward for
reinforcement learning. Our experiments demonstrate that ReCode substantially
boosts LLMs' code generation performance in dynamic API scenarios, especially
on the unseen CodeUpdateArena task. Crucially, compared to supervised
fine-tuning, ReCode has less impact on LLMs' general code generation abilities.
We apply ReCode on various LLMs and reinforcement learning algorithms (GRPO and
DAPO), all achieving consistent improvements. Notably, after training,
Qwen2.5-Coder-7B outperforms that of the 32B parameter code instruction-tuned
model and the reasoning model with the same architecture. Code is available at
https://github.com/zjunlp/ReCode.",https://github.com/zjunlp/ReCode,Work in progress
CoVE: Compressed Vocabulary Expansion Makes Better LLM-based Recommender Systems,"Haochen Zhang, Tianyi Zhang, Junze Yin, Oren Gal, Anshumali Shrivastava, Vladimir Braverman",2025-06-24,2506.19993v1,http://arxiv.org/abs/2506.19993v1,http://arxiv.org/pdf/2506.19993v1,information_retrieval,cs.IR,"Recommender systems play a pivotal role in providing relevant content to
users. With the rapid development of large language models (LLMs), researchers
have begun utilizing LLMs to build more powerful recommender systems. However,
existing approaches that focus on aligning LLMs with recommendation tasks do
not fully leverage their sequential information processing capabilities,
leading to suboptimal performance.
  In this paper, we propose a novel system called compressed vocabulary
expansion (CoVE). In CoVE, each item is assigned a unique ID within the
expanded vocabulary. Our framework effectively capitalizes on sequence
understanding abilities of LLMs, significantly enhancing their performance on
recommendation tasks. Additionally, we compress the embedding layer, making
CoVE practical for large-scale industrial applications. The effectiveness and
performance of CoVE are demonstrated through comprehensive experiments on
multiple recommendation datasets and comparisons with prior works. Our code can
be found at https://github.com/HaochenZhang717/CoVE-official-Repo.",https://github.com/HaochenZhang717/CoVE-official-Repo,Accepted by ACL 2025 Findings
Schema-R1: A reasoning training approach for schema linking in Text-to-SQL Task,"Wuzhenghong Wen, Su Pan, yuwei Sun",2025-06-13,2506.11986v1,http://arxiv.org/abs/2506.11986v1,http://arxiv.org/pdf/2506.11986v1,databases,cs.AI,"Schema linking is a critical step in Text-to-SQL task, aiming to accurately
predict the table names and column names required for the SQL query based on
the given question. However, current fine-tuning approaches for schema linking
models employ a rote-learning paradigm, excessively optimizing for ground truth
schema linking outcomes while compromising reasoning ability. This limitation
arises because of the difficulty in acquiring a high-quality reasoning sample
for downstream tasks. To address this, we propose Schema-R1, a reasoning schema
linking model trained using reinforcement learning. Specifically, Schema-R1
consists of three key steps: constructing small batches of high-quality
reasoning samples, supervised fine-tuning for cold-start initialization, and
rule-based reinforcement learning training. The final results demonstrate that
our method effectively enhances the reasoning ability of the schema linking
model, achieving a 10\% improvement in filter accuracy compared to the existing
method. Our code is available at https://github.com/hongWin/Schema-R1/.",https://github.com/hongWin/Schema-R1,"11 pages, 3 figures, conference"
Bridging RDF Knowledge Graphs with Graph Neural Networks for Semantically-Rich Recommender Systems,"Michael Färber, David Lamprecht, Yuni Susanti",2025-06-10,2506.08743v1,http://arxiv.org/abs/2506.08743v1,http://arxiv.org/pdf/2506.08743v1,databases,cs.IR,"Graph Neural Networks (GNNs) have substantially advanced the field of
recommender systems. However, despite the creation of more than a thousand
knowledge graphs (KGs) under the W3C standard RDF, their rich semantic
information has not yet been fully leveraged in GNN-based recommender systems.
To address this gap, we propose a comprehensive integration of RDF KGs with
GNNs that utilizes both the topological information from RDF object properties
and the content information from RDF datatype properties. Our main focus is an
in-depth evaluation of various GNNs, analyzing how different semantic feature
initializations and types of graph structure heterogeneity influence their
performance in recommendation tasks. Through experiments across multiple
recommendation scenarios involving multi-million-node RDF graphs, we
demonstrate that harnessing the semantic richness of RDF KGs significantly
improves recommender systems and lays the groundwork for GNN-based recommender
systems for the Linked Open Data cloud. The code and data are available on our
GitHub repository: https://github.com/davidlamprecht/rdf-gnn-recommendation",https://github.com/davidlamprecht/rdf-gnn-recommendation,Accepted at DASFAA 2025
