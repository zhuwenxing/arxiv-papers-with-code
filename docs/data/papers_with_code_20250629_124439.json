[
  {
    "title": "skLEP: A Slovak General Language Understanding Benchmark",
    "authors": "Marek Šuppa, Andrej Ridzik, Daniel Hládek, Tomáš Javůrek, Viktória Ondrejová, Kristína Sásiková, Martin Tamajka, Marián Šimko",
    "published": "2025-06-26",
    "arxiv_id": "2506.21508v1",
    "url": "http://arxiv.org/abs/2506.21508v1",
    "pdf_url": "http://arxiv.org/pdf/2506.21508v1",
    "category": "information_retrieval",
    "primary_category": "cs.CL",
    "abstract": "In this work, we introduce skLEP, the first comprehensive benchmark\nspecifically designed for evaluating Slovak natural language understanding\n(NLU) models. We have compiled skLEP to encompass nine diverse tasks that span\ntoken-level, sentence-pair, and document-level challenges, thereby offering a\nthorough assessment of model capabilities. To create this benchmark, we curated\nnew, original datasets tailored for Slovak and meticulously translated\nestablished English NLU resources. Within this paper, we also present the first\nsystematic and extensive evaluation of a wide array of Slovak-specific,\nmultilingual, and English pre-trained language models using the skLEP tasks.\nFinally, we also release the complete benchmark data, an open-source toolkit\nfacilitating both fine-tuning and evaluation of models, and a public\nleaderboard at https://github.com/slovak-nlp/sklep in the hopes of fostering\nreproducibility and drive future research in Slovak NLU.",
    "code_links": [
      "https://github.com/slovak-nlp/sklep"
    ],
    "comment": "ACL 2025 Findings"
  },
  {
    "title": "Small Encoders Can Rival Large Decoders in Detecting Groundedness",
    "authors": "Istabrak Abbes, Gabriele Prato, Quentin Fournier, Fernando Rodriguez, Alaa Boukhary, Adam Elwood, Sarath Chandar",
    "published": "2025-06-26",
    "arxiv_id": "2506.21288v1",
    "url": "http://arxiv.org/abs/2506.21288v1",
    "pdf_url": "http://arxiv.org/pdf/2506.21288v1",
    "category": "information_retrieval",
    "primary_category": "cs.CL",
    "abstract": "Augmenting large language models (LLMs) with external context significantly\nimproves their performance in natural language processing (NLP) tasks. However,\nLLMs struggle to answer queries reliably when the provided context lacks\ninformation, often resorting to ungrounded speculation or internal knowledge.\nGroundedness - generating responses strictly supported by the context - is\nessential for ensuring factual consistency and trustworthiness. This study\nfocuses on detecting whether a given query is grounded in a document provided\nin context before the costly answer generation by LLMs. Such a detection\nmechanism can significantly reduce both inference time and resource\nconsumption. We show that lightweight, task specific encoder models such as\nRoBERTa and NomicBERT, fine-tuned on curated datasets, can achieve accuracy\ncomparable to state-of-the-art LLMs, such as Llama3 8B and GPT4o, in\ngroundedness detection while reducing inference latency by orders of magnitude.\nThe code is available at : https://github.com/chandarlab/Hallucinate-less",
    "code_links": [
      "https://github.com/chandarlab/Hallucinate-less"
    ],
    "comment": null
  },
  {
    "title": "Response Quality Assessment for Retrieval-Augmented Generation via Conditional Conformal Factuality",
    "authors": "Naihe Feng, Yi Sui, Shiyi Hou, Jesse C. Cresswell, Ga Wu",
    "published": "2025-06-26",
    "arxiv_id": "2506.20978v1",
    "url": "http://arxiv.org/abs/2506.20978v1",
    "pdf_url": "http://arxiv.org/pdf/2506.20978v1",
    "category": "information_retrieval",
    "primary_category": "cs.IR",
    "abstract": "Existing research on Retrieval-Augmented Generation (RAG) primarily focuses\non improving overall question-answering accuracy, often overlooking the quality\nof sub-claims within generated responses. Recent methods that attempt to\nimprove RAG trustworthiness, such as through auto-evaluation metrics, lack\nprobabilistic guarantees or require ground truth answers. To address these\nlimitations, we propose Conformal-RAG, a novel framework inspired by recent\napplications of conformal prediction (CP) on large language models (LLMs).\nConformal-RAG leverages CP and internal information from the RAG mechanism to\noffer statistical guarantees on response quality. It ensures group-conditional\ncoverage spanning multiple sub-domains without requiring manual labelling of\nconformal sets, making it suitable for complex RAG applications. Compared to\nexisting RAG auto-evaluation methods, Conformal-RAG offers statistical\nguarantees on the quality of refined sub-claims, ensuring response reliability\nwithout the need for ground truth answers. Additionally, our experiments\ndemonstrate that by leveraging information from the RAG system, Conformal-RAG\nretains up to 60\\% more high-quality sub-claims from the response compared to\ndirect applications of CP to LLMs, while maintaining the same reliability\nguarantee.",
    "code_links": [
      "https://github.com/n4feng/ResponseQualityAssessment"
    ],
    "comment": "Accepted by SIGIR 2025 short paper, 5 pages, Code is available at\n  https://github.com/n4feng/ResponseQualityAssessment"
  },
  {
    "title": "EraRAG: Efficient and Incremental Retrieval Augmented Generation for Growing Corpora",
    "authors": "Fangyuan Zhang, Zhengjun Huang, Yingli Zhou, Qintian Guo, Zhixun Li, Wensheng Luo, Di Jiang, Yixiang Fang, Xiaofang Zhou",
    "published": "2025-06-26",
    "arxiv_id": "2506.20963v1",
    "url": "http://arxiv.org/abs/2506.20963v1",
    "pdf_url": "http://arxiv.org/pdf/2506.20963v1",
    "category": "information_retrieval",
    "primary_category": "cs.IR",
    "abstract": "Graph-based Retrieval-Augmented Generation (Graph-RAG) enhances large\nlanguage models (LLMs) by structuring retrieval over an external corpus.\nHowever, existing approaches typically assume a static corpus, requiring\nexpensive full-graph reconstruction whenever new documents arrive, limiting\ntheir scalability in dynamic, evolving environments. To address these\nlimitations, we introduce EraRAG, a novel multi-layered Graph-RAG framework\nthat supports efficient and scalable dynamic updates. Our method leverages\nhyperplane-based Locality-Sensitive Hashing (LSH) to partition and organize the\noriginal corpus into hierarchical graph structures, enabling efficient and\nlocalized insertions of new data without disrupting the existing topology. The\ndesign eliminates the need for retraining or costly recomputation while\npreserving high retrieval accuracy and low latency. Experiments on large-scale\nbenchmarks demonstrate that EraRag achieves up to an order of magnitude\nreduction in update time and token consumption compared to existing Graph-RAG\nsystems, while providing superior accuracy performance. This work offers a\npractical path forward for RAG systems that must operate over continually\ngrowing corpora, bridging the gap between retrieval efficiency and\nadaptability. Our code and data are available at\nhttps://github.com/EverM0re/EraRAG-Official.",
    "code_links": [
      "https://github.com/EverM0re/EraRAG-Official"
    ],
    "comment": "Under review"
  },
  {
    "title": "RAG-VisualRec: An Open Resource for Vision- and Text-Enhanced Retrieval-Augmented Generation in Recommendation",
    "authors": "Ali Tourani, Fatemeh Nazary, Yashar Deldjoo",
    "published": "2025-06-25",
    "arxiv_id": "2506.20817v1",
    "url": "http://arxiv.org/abs/2506.20817v1",
    "pdf_url": "http://arxiv.org/pdf/2506.20817v1",
    "category": "information_retrieval",
    "primary_category": "cs.IR",
    "abstract": "This paper addresses the challenge of developing multimodal recommender\nsystems for the movie domain, where limited metadata (e.g., title, genre) often\nhinders the generation of robust recommendations. We introduce a resource that\ncombines LLM-generated plot descriptions with trailer-derived visual embeddings\nin a unified pipeline supporting both Retrieval-Augmented Generation (RAG) and\ncollaborative filtering. Central to our approach is a data augmentation step\nthat transforms sparse metadata into richer textual signals, alongside fusion\nstrategies (e.g., PCA, CCA) that integrate visual cues. Experimental\nevaluations demonstrate that CCA-based fusion significantly boosts recall\ncompared to unimodal baselines, while an LLM-driven re-ranking step further\nimproves NDCG, particularly in scenarios with limited textual data. By\nreleasing this framework, we invite further exploration of multi-modal\nrecommendation techniques tailored to cold-start, novelty-focused, and\ndomain-specific settings. All code, data, and detailed documentation are\npublicly available at: https://github.com/RecSys-lab/RAG-VisualRec",
    "code_links": [
      "https://github.com/RecSys-lab/RAG-VisualRec"
    ],
    "comment": "20 pages, 6 figures, 5 tables"
  },
  {
    "title": "ReCode: Updating Code API Knowledge with Reinforcement Learning",
    "authors": "Haoze Wu, Yunzhi Yao, Wenhao Yu, Huajun Chen, Ningyu Zhang",
    "published": "2025-06-25",
    "arxiv_id": "2506.20495v1",
    "url": "http://arxiv.org/abs/2506.20495v1",
    "pdf_url": "http://arxiv.org/pdf/2506.20495v1",
    "category": "information_retrieval",
    "primary_category": "cs.CL",
    "abstract": "Large Language Models (LLMs) exhibit remarkable code generation capabilities\nbut falter when adapting to frequent updates in external library APIs. This\ncritical limitation, stemming from reliance on outdated API knowledge from\ntheir training data, even with access to current documentation, impedes\nreliable code generation in dynamic environments. To tackle this issue, we\npropose ReCode (rule-based Reinforcement learning for Code Update), a novel\nframework that mimics human programmer adaptation to API changes. Specifically,\nwe construct a dataset of approximately 2,000 data entries to train the LLMs to\nperform version migration based on updated information. Then, we introduce a\nmodified string similarity metric for code evaluation as the reward for\nreinforcement learning. Our experiments demonstrate that ReCode substantially\nboosts LLMs' code generation performance in dynamic API scenarios, especially\non the unseen CodeUpdateArena task. Crucially, compared to supervised\nfine-tuning, ReCode has less impact on LLMs' general code generation abilities.\nWe apply ReCode on various LLMs and reinforcement learning algorithms (GRPO and\nDAPO), all achieving consistent improvements. Notably, after training,\nQwen2.5-Coder-7B outperforms that of the 32B parameter code instruction-tuned\nmodel and the reasoning model with the same architecture. Code is available at\nhttps://github.com/zjunlp/ReCode.",
    "code_links": [
      "https://github.com/zjunlp/ReCode"
    ],
    "comment": "Work in progress"
  }
]