[
  {
    "title": "MultiRAG: A Knowledge-guided Framework for Mitigating Hallucination in Multi-source Retrieval Augmented Generation",
    "authors": "Wenlong Wu, Haofen Wang, Bohan Li, Peixuan Huang, Xinzhe Zhao, Lei Liang",
    "published": "2025-08-05",
    "arxiv_id": "2508.03553v1",
    "url": "http://arxiv.org/abs/2508.03553v1",
    "pdf_url": "http://arxiv.org/pdf/2508.03553v1",
    "category": "information_retrieval",
    "primary_category": "cs.IR",
    "abstract": "Retrieval Augmented Generation (RAG) has emerged as a promising solution to\naddress hallucination issues in Large Language Models (LLMs). However, the\nintegration of multiple retrieval sources, while potentially more informative,\nintroduces new challenges that can paradoxically exacerbate hallucination\nproblems. These challenges manifest primarily in two aspects: the sparse\ndistribution of multi-source data that hinders the capture of logical\nrelationships and the inherent inconsistencies among different sources that\nlead to information conflicts. To address these challenges, we propose\nMultiRAG, a novel framework designed to mitigate hallucination in multi-source\nretrieval-augmented generation through knowledge-guided approaches. Our\nframework introduces two key innovations: (1) a knowledge construction module\nthat employs multi-source line graphs to efficiently aggregate logical\nrelationships across different knowledge sources, effectively addressing the\nsparse data distribution issue; and (2) a sophisticated retrieval module that\nimplements a multi-level confidence calculation mechanism, performing both\ngraph-level and node-level assessments to identify and eliminate unreliable\ninformation nodes, thereby reducing hallucinations caused by inter-source\ninconsistencies. Extensive experiments on four multi-domain query datasets and\ntwo multi-hop QA datasets demonstrate that MultiRAG significantly enhances the\nreliability and efficiency of knowledge retrieval in complex multi-source\nscenarios. \\textcolor{blue}{Our code is available in\nhttps://github.com/wuwenlong123/MultiRAG.",
    "code_links": [
      "https://github.com/wuwenlong123/MultiRAG"
    ],
    "comment": "Accepted by ICDE 2025 Research Paper"
  },
  {
    "title": "fact check AI at SemEval-2025 Task 7: Multilingual and Crosslingual Fact-checked Claim Retrieval",
    "authors": "Pranshu Rastogi",
    "published": "2025-08-05",
    "arxiv_id": "2508.03475v1",
    "url": "http://arxiv.org/abs/2508.03475v1",
    "pdf_url": "http://arxiv.org/pdf/2508.03475v1",
    "category": "information_retrieval",
    "primary_category": "cs.CL",
    "abstract": "SemEval-2025 Task 7: Multilingual and Crosslingual Fact-Checked Claim\nRetrieval is approached as a Learning-to-Rank task using a bi-encoder model\nfine-tuned from a pre-trained transformer optimized for sentence similarity.\nTraining used both the source languages and their English translations for\nmultilingual retrieval and only English translations for cross-lingual\nretrieval. Using lightweight models with fewer than 500M parameters and\ntraining on Kaggle T4 GPUs, the method achieved 92% Success@10 in multilingual\nand 80% Success@10 in 5th in crosslingual and 10th in multilingual tracks.",
    "code_links": [
      "https://github.com/pranshurastogi29/SemEval-2025-ACL-Multi-and-Crosslingual-Retrieval-using-Bi-encoders"
    ],
    "comment": "7 pages, 6 tables. Code available at\n  https://github.com/pranshurastogi29/SemEval-2025-ACL-Multi-and-Crosslingual-Retrieval-using-Bi-encoders"
  },
  {
    "title": "Hubness Reduction with Dual Bank Sinkhorn Normalization for Cross-Modal Retrieval",
    "authors": "Zhengxin Pan, Haishuai Wang, Fangyu Wu, Peng Zhang, Jiajun Bu",
    "published": "2025-08-04",
    "arxiv_id": "2508.02538v1",
    "url": "http://arxiv.org/abs/2508.02538v1",
    "pdf_url": "http://arxiv.org/pdf/2508.02538v1",
    "category": "information_retrieval",
    "primary_category": "cs.IR",
    "abstract": "The past decade has witnessed rapid advancements in cross-modal retrieval,\nwith significant progress made in accurately measuring the similarity between\ncross-modal pairs. However, the persistent hubness problem, a phenomenon where\na small number of targets frequently appear as nearest neighbors to numerous\nqueries, continues to hinder the precision of similarity measurements. Despite\nseveral proposed methods to reduce hubness, their underlying mechanisms remain\npoorly understood. To bridge this gap, we analyze the widely-adopted Inverted\nSoftmax approach and demonstrate its effectiveness in balancing target\nprobabilities during retrieval. Building on these insights, we propose a\nprobability-balancing framework for more effective hubness reduction. We\ncontend that balancing target probabilities alone is inadequate and, therefore,\nextend the framework to balance both query and target probabilities by\nintroducing Sinkhorn Normalization (SN). Notably, we extend SN to scenarios\nwhere the true query distribution is unknown, showing that current methods,\nwhich rely solely on a query bank to estimate target hubness, produce\nsuboptimal results due to a significant distributional gap between the query\nbank and targets. To mitigate this issue, we introduce Dual Bank Sinkhorn\nNormalization (DBSN), incorporating a corresponding target bank alongside the\nquery bank to narrow this distributional gap. Our comprehensive evaluation\nacross various cross-modal retrieval tasks, including image-text retrieval,\nvideo-text retrieval, and audio-text retrieval, demonstrates consistent\nperformance improvements, validating the effectiveness of both SN and DBSN. All\ncodes are publicly available at https://github.com/ppanzx/DBSN.",
    "code_links": [
      "https://github.com/ppanzx/DBSN"
    ],
    "comment": "ACMMM 2025"
  },
  {
    "title": "Beyond Chunks and Graphs: Retrieval-Augmented Generation through Triplet-Driven Thinking",
    "authors": "Shengbo Gong, Xianfeng Tang, Carl Yang, Wei jin",
    "published": "2025-08-04",
    "arxiv_id": "2508.02435v1",
    "url": "http://arxiv.org/abs/2508.02435v1",
    "pdf_url": "http://arxiv.org/pdf/2508.02435v1",
    "category": "information_retrieval",
    "primary_category": "cs.IR",
    "abstract": "Retrieval-augmented generation (RAG) is critical for reducing hallucinations\nand incorporating external knowledge into Large Language Models (LLMs).\nHowever, advanced RAG systems face a trade-off between performance and\nefficiency. Multi-round RAG approaches achieve strong reasoning but incur\nexcessive LLM calls and token costs, while Graph RAG methods suffer from\ncomputationally expensive, error-prone graph construction and retrieval\nredundancy. To address these challenges, we propose T$^2$RAG, a novel framework\nthat operates on a simple, graph-free knowledge base of atomic triplets.\nT$^2$RAG leverages an LLM to decompose questions into searchable triplets with\nplaceholders, which it then iteratively resolves by retrieving evidence from\nthe triplet database. Empirical results show that T$^2$RAG significantly\noutperforms state-of-the-art multi-round and Graph RAG methods, achieving an\naverage performance gain of up to 11\\% across six datasets while reducing\nretrieval costs by up to 45\\%. Our code is available at\nhttps://github.com/rockcor/T2RAG",
    "code_links": [
      "https://github.com/rockcor/T2RAG"
    ],
    "comment": "19 pages"
  },
  {
    "title": "Uni-Layout: Integrating Human Feedback in Unified Layout Generation and Evaluation",
    "authors": "Shuo Lu, Yanyin Chen, Wei Feng, Jiahao Fan, Fengheng Li, Zheng Zhang, Jingjing Lv, Junjie Shen, Ching Law, Jian Liang",
    "published": "2025-08-04",
    "arxiv_id": "2508.02374v1",
    "url": "http://arxiv.org/abs/2508.02374v1",
    "pdf_url": "http://arxiv.org/pdf/2508.02374v1",
    "category": "information_retrieval",
    "primary_category": "cs.CV",
    "abstract": "Layout generation plays a crucial role in enhancing both user experience and\ndesign efficiency. However, current approaches suffer from task-specific\ngeneration capabilities and perceptually misaligned evaluation metrics, leading\nto limited applicability and ineffective measurement. In this paper, we propose\n\\textit{Uni-Layout}, a novel framework that achieves unified generation,\nhuman-mimicking evaluation and alignment between the two. For universal\ngeneration, we incorporate various layout tasks into a single taxonomy and\ndevelop a unified generator that handles background or element contents\nconstrained tasks via natural language prompts. To introduce human feedback for\nthe effective evaluation of layouts, we build \\textit{Layout-HF100k}, the first\nlarge-scale human feedback dataset with 100,000 expertly annotated layouts.\nBased on \\textit{Layout-HF100k}, we introduce a human-mimicking evaluator that\nintegrates visual and geometric information, employing a Chain-of-Thought\nmechanism to conduct qualitative assessments alongside a confidence estimation\nmodule to yield quantitative measurements. For better alignment between the\ngenerator and the evaluator, we integrate them into a cohesive system by\nadopting Dynamic-Margin Preference Optimization (DMPO), which dynamically\nadjusts margins based on preference strength to better align with human\njudgments. Extensive experiments show that \\textit{Uni-Layout} significantly\noutperforms both task-specific and general-purpose methods. Our code is\npublicly available at https://github.com/JD-GenX/Uni-Layout.",
    "code_links": [
      "https://github.com/JD-GenX/Uni-Layout"
    ],
    "comment": "Accepted to ACM MM 2025"
  },
  {
    "title": "I2CR: Intra- and Inter-modal Collaborative Reflections for Multimodal Entity Linking",
    "authors": "Ziyan Liu, Junwen Li, Kaiwen Li, Tong Ruan, Chao Wang, Xinyan He, Zongyu Wang, Xuezhi Cao, Jingping Liu",
    "published": "2025-08-04",
    "arxiv_id": "2508.02243v1",
    "url": "http://arxiv.org/abs/2508.02243v1",
    "pdf_url": "http://arxiv.org/pdf/2508.02243v1",
    "category": "information_retrieval",
    "primary_category": "cs.CV",
    "abstract": "Multimodal entity linking plays a crucial role in a wide range of\napplications. Recent advances in large language model-based methods have become\nthe dominant paradigm for this task, effectively leveraging both textual and\nvisual modalities to enhance performance. Despite their success, these methods\nstill face two challenges, including unnecessary incorporation of image data in\ncertain scenarios and the reliance only on a one-time extraction of visual\nfeatures, which can undermine their effectiveness and accuracy. To address\nthese challenges, we propose a novel LLM-based framework for the multimodal\nentity linking task, called Intra- and Inter-modal Collaborative Reflections.\nThis framework prioritizes leveraging text information to address the task.\nWhen text alone is insufficient to link the correct entity through intra- and\ninter-modality evaluations, it employs a multi-round iterative strategy that\nintegrates key visual clues from various aspects of the image to support\nreasoning and enhance matching accuracy. Extensive experiments on three widely\nused public datasets demonstrate that our framework consistently outperforms\ncurrent state-of-the-art methods in the task, achieving improvements of 3.2%,\n5.1%, and 1.6%, respectively. Our code is available at\nhttps://github.com/ziyan-xiaoyu/I2CR/.",
    "code_links": [
      "https://github.com/ziyan-xiaoyu/I2CR"
    ],
    "comment": "10 pages, 6 figures, accepted by ACMMM 2025"
  },
  {
    "title": "CM$^3$: Calibrating Multimodal Recommendation",
    "authors": "Xin Zhou, Yongjie Wang, Zhiqi Shen",
    "published": "2025-08-02",
    "arxiv_id": "2508.01226v1",
    "url": "http://arxiv.org/abs/2508.01226v1",
    "pdf_url": "http://arxiv.org/pdf/2508.01226v1",
    "category": "information_retrieval",
    "primary_category": "cs.IR",
    "abstract": "Alignment and uniformity are fundamental principles within the domain of\ncontrastive learning. In recommender systems, prior work has established that\noptimizing the Bayesian Personalized Ranking (BPR) loss contributes to the\nobjectives of alignment and uniformity. Specifically, alignment aims to draw\ntogether the representations of interacting users and items, while uniformity\nmandates a uniform distribution of user and item embeddings across a unit\nhypersphere. This study revisits the alignment and uniformity properties within\nthe context of multimodal recommender systems, revealing a proclivity among\nextant models to prioritize uniformity to the detriment of alignment. Our\nhypothesis challenges the conventional assumption of equitable item treatment\nthrough a uniformity loss, proposing a more nuanced approach wherein items with\nsimilar multimodal attributes converge toward proximal representations within\nthe hyperspheric manifold. Specifically, we leverage the inherent similarity\nbetween items' multimodal data to calibrate their uniformity distribution,\nthereby inducing a more pronounced repulsive force between dissimilar entities\nwithin the embedding space. A theoretical analysis elucidates the relationship\nbetween this calibrated uniformity loss and the conventional uniformity\nfunction. Moreover, to enhance the fusion of multimodal features, we introduce\na Spherical B\\'ezier method designed to integrate an arbitrary number of\nmodalities while ensuring that the resulting fused features are constrained to\nthe same hyperspherical manifold. Empirical evaluations conducted on five\nreal-world datasets substantiate the superiority of our approach over competing\nbaselines. We also shown that the proposed methods can achieve up to a 5.4%\nincrease in NDCG@20 performance via the integration of MLLM-extracted features.\nSource code is available at: https://github.com/enoche/CM3.",
    "code_links": [
      "https://github.com/enoche/CM3"
    ],
    "comment": "Working Paper: https://github.com/enoche/CM3"
  },
  {
    "title": "DBAIOps: A Reasoning LLM-Enhanced Database Operation and Maintenance System using Knowledge Graphs",
    "authors": "Wei Zhou, Peng Sun, Xuanhe Zhou, Qianglei Zang, Ji Xu, Tieying Zhang, Guoliang Li, Fan Wu",
    "published": "2025-08-02",
    "arxiv_id": "2508.01136v1",
    "url": "http://arxiv.org/abs/2508.01136v1",
    "pdf_url": "http://arxiv.org/pdf/2508.01136v1",
    "category": "information_retrieval",
    "primary_category": "cs.DB",
    "abstract": "The operation and maintenance (O&M) of database systems is critical to\nensuring system availability and performance, typically requiring expert\nexperience (e.g., identifying metric-to-anomaly relations) for effective\ndiagnosis and recovery. However, existing automatic database O&M methods,\nincluding commercial products, cannot effectively utilize expert experience. On\nthe one hand, rule-based methods only support basic O&M tasks (e.g.,\nmetric-based anomaly detection), which are mostly numerical equations and\ncannot effectively incorporate literal O&M experience (e.g., troubleshooting\nguidance in manuals). On the other hand, LLM-based methods, which retrieve\nfragmented information (e.g., standard documents + RAG), often generate\ninaccurate or generic results. To address these limitations, we present\nDBAIOps, a novel hybrid database O&M system that combines reasoning LLMs with\nknowledge graphs to achieve DBA-style diagnosis. First, DBAIOps introduces a\nheterogeneous graph model for representing the diagnosis experience, and\nproposes a semi-automatic graph construction algorithm to build that graph from\nthousands of documents. Second, DBAIOps develops a collection of (800+)\nreusable anomaly models that identify both directly alerted metrics and\nimplicitly correlated experience and metrics. Third, for each anomaly, DBAIOps\nproposes a two-stage graph evolution mechanism to explore relevant diagnosis\npaths and identify missing relations automatically. It then leverages a\nreasoning LLM (e.g., DeepSeek-R1) to infer root causes and generate clear\ndiagnosis reports for both DBAs and common users. Our evaluation over four\nmainstream database systems (Oracle, MySQL, PostgreSQL, and DM8) demonstrates\nthat DBAIOps outperforms state-of-the-art baselines, 34.85% and 47.22% higher\nin root cause and human evaluation accuracy, respectively.",
    "code_links": [
      "https://github.com/weAIDB/DBAIOps"
    ],
    "comment": "DBAIOps supports 25 database systems and has been deployed in 20\n  real-world scenarios, covering domains like finance, energy, and healthcare.\n  See website at: https://www.dbaiops.com; See code at:\n  https://github.com/weAIDB/DBAIOps/"
  },
  {
    "title": "MAO-ARAG: Multi-Agent Orchestration for Adaptive Retrieval-Augmented Generation",
    "authors": "Yiqun Chen, Erhan Zhang, Lingyong Yan, Shuaiqiang Wang, Jizhou Huang, Dawei Yin, Jiaxin Mao",
    "published": "2025-08-01",
    "arxiv_id": "2508.01005v1",
    "url": "http://arxiv.org/abs/2508.01005v1",
    "pdf_url": "http://arxiv.org/pdf/2508.01005v1",
    "category": "information_retrieval",
    "primary_category": "cs.CL",
    "abstract": "In question-answering (QA) systems, Retrieval-Augmented Generation (RAG) has\nbecome pivotal in enhancing response accuracy and reducing hallucination\nissues. The architecture of RAG systems varies significantly, encompassing\nsingle-round RAG, iterative RAG, and reasoning RAG, each tailored to address\ndifferent types of queries. Due to the varying complexity of real-world\nqueries, a fixed RAG pipeline often struggles to balance performance and cost\nefficiency across different queries. To address this challenge, we propose an\nadaptive RAG framework called MAO-ARAG, which leverages multi-agent\norchestration. Our adaptive RAG is conceived as a multi-turn framework.\nSpecifically, we define multiple executor agents, representing typical RAG\nmodules such as query reformulation agents, document selection agent, and\ngeneration agents. A planner agent intelligently selects and integrates the\nappropriate agents from these executors into a suitable workflow tailored for\neach query, striving for high-quality answers while maintaining reasonable\ncosts. During each turn, the planner agent is trained using reinforcement\nlearning, guided by an outcome-based reward (F1 score) and a cost-based\npenalty, continuously improving answer quality while keeping costs within a\nreasonable range. Experiments conducted on multiple QA datasets demonstrate\nthat our approach, which dynamically plans workflows for each query, not only\nachieves high answer quality but also maintains both cost and latency within\nacceptable limits.The code of MAO-ARAG is on\nhttps://github.com/chenyiqun/Agentic-RAG.",
    "code_links": [
      "https://github.com/chenyiqun/Agentic-RAG"
    ],
    "comment": null
  },
  {
    "title": "MetaAgent: Toward Self-Evolving Agent via Tool Meta-Learning",
    "authors": "Hongjin Qian, Zheng Liu",
    "published": "2025-08-01",
    "arxiv_id": "2508.00271v1",
    "url": "http://arxiv.org/abs/2508.00271v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00271v1",
    "category": "information_retrieval",
    "primary_category": "cs.AI",
    "abstract": "In this work, we propose MetaAgent, an agentic paradigm inspired by the\nprinciple of learning-by-doing, where expertise is developed through hands-on\npractice and continual self-improvement. MetaAgent starts with a minimal\nworkflow, equipped only with basic reasoning and adaptive help-seeking\nabilities. When a knowledge gap is encountered, MetaAgent generates natural\nlanguage help requests, which are routed to the most suitable external tool by\na dedicated tool router. As MetaAgent solves tasks, it continually conducts\nself-reflection and answer verification, distilling actionable experience into\nconcise texts that are dynamically incorporated into future task contexts.\nBesides, MetaAgent autonomously builds in-house tools and a persistent\nknowledge base by organizing its tool-use history, further enhancing its\nability to retrieve and integrate relevant information We term this continual,\ndata-driven process as \\textit{meta tool learning}, through which MetaAgent\nincrementally refines its reasoning and tool-use strategies, without changing\nmodel parameters or requiring further post-training. Evaluated on challenging\nknowledge discovery benchmarks, including GAIA, WebWalkerQA, and BrowseCamp,\nMetaAgent consistently outperforms workflow-based baselines and matches or\nexceeds end-to-end trained agents, demonstrating the promise of self-evolving\nagentic systems for robust, general-purpose knowledge discovery. We provide our\nsource codes in https://github.com/qhjqhj00/MetaAgent.",
    "code_links": [
      "https://github.com/qhjqhj00/MetaAgent"
    ],
    "comment": "Technical Report, 14 pages"
  },
  {
    "title": "Melody-Lyrics Matching with Contrastive Alignment Loss",
    "authors": "Changhong Wang, Michel Olvera, Gaël Richard",
    "published": "2025-07-31",
    "arxiv_id": "2508.00123v1",
    "url": "http://arxiv.org/abs/2508.00123v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00123v1",
    "category": "information_retrieval",
    "primary_category": "eess.AS",
    "abstract": "The connection between music and lyrics is far beyond semantic bonds.\nConceptual pairs in the two modalities such as rhythm and rhyme, note duration\nand syllabic stress, and structure correspondence, raise a compelling yet\nseldom-explored direction in the field of music information retrieval. In this\npaper, we present melody-lyrics matching (MLM), a new task which retrieves\npotential lyrics for a given symbolic melody from text sources. Rather than\ngenerating lyrics from scratch, MLM essentially exploits the relationships\nbetween melody and lyrics. We propose a self-supervised representation learning\nframework with contrastive alignment loss for melody and lyrics. This has the\npotential to leverage the abundance of existing songs with paired melody and\nlyrics. No alignment annotations are required. Additionally, we introduce\nsylphone, a novel representation for lyrics at syllable-level activated by\nphoneme identity and vowel stress. We demonstrate that our method can match\nmelody with coherent and singable lyrics with empirical results and intuitive\nexamples. We open source code and provide matching examples on the companion\nwebpage: https://github.com/changhongw/mlm.",
    "code_links": [
      "https://github.com/changhongw/mlm"
    ],
    "comment": "10 pages, 7 figures, 3 tables. This work has been submitted to the\n  IEEE for possible publication"
  },
  {
    "title": "Personalized Education with Ranking Alignment Recommendation",
    "authors": "Haipeng Liu, Yuxuan Liu, Ting Long",
    "published": "2025-07-31",
    "arxiv_id": "2507.23664v1",
    "url": "http://arxiv.org/abs/2507.23664v1",
    "pdf_url": "http://arxiv.org/pdf/2507.23664v1",
    "category": "information_retrieval",
    "primary_category": "cs.AI",
    "abstract": "Personalized question recommendation aims to guide individual students\nthrough questions to enhance their mastery of learning targets. Most previous\nmethods model this task as a Markov Decision Process and use reinforcement\nlearning to solve, but they struggle with efficient exploration, failing to\nidentify the best questions for each student during training. To address this,\nwe propose Ranking Alignment Recommendation (RAR), which incorporates\ncollaborative ideas into the exploration mechanism, enabling more efficient\nexploration within limited training episodes. Experiments show that RAR\neffectively improves recommendation performance, and our framework can be\napplied to any RL-based question recommender. Our code is available in\nhttps://github.com/wuming29/RAR.git.",
    "code_links": [
      "https://github.com/wuming29/RAR"
    ],
    "comment": null
  },
  {
    "title": "Not Just What, But When: Integrating Irregular Intervals to LLM for Sequential Recommendation",
    "authors": "Wei-Wei Du, Takuma Udagawa, Kei Tateno",
    "published": "2025-07-31",
    "arxiv_id": "2507.23209v1",
    "url": "http://arxiv.org/abs/2507.23209v1",
    "pdf_url": "http://arxiv.org/pdf/2507.23209v1",
    "category": "information_retrieval",
    "primary_category": "cs.IR",
    "abstract": "Time intervals between purchasing items are a crucial factor in sequential\nrecommendation tasks, whereas existing approaches focus on item sequences and\noften overlook by assuming the intervals between items are static. However,\ndynamic intervals serve as a dimension that describes user profiling on not\nonly the history within a user but also different users with the same item\nhistory. In this work, we propose IntervalLLM, a novel framework that\nintegrates interval information into LLM and incorporates the novel\ninterval-infused attention to jointly consider information of items and\nintervals. Furthermore, unlike prior studies that address the cold-start\nscenario only from the perspectives of users and items, we introduce a new\nviewpoint: the interval perspective to serve as an additional metric for\nevaluating recommendation methods on the warm and cold scenarios. Extensive\nexperiments on 3 benchmarks with both traditional- and LLM-based baselines\ndemonstrate that our IntervalLLM achieves not only 4.4% improvements in average\nbut also the best-performing warm and cold scenarios across all users, items,\nand the proposed interval perspectives. In addition, we observe that the cold\nscenario from the interval perspective experiences the most significant\nperformance drop among all recommendation methods. This finding underscores the\nnecessity of further research on interval-based cold challenges and our\nintegration of interval information in the realm of sequential recommendation\ntasks. Our code is available here:\nhttps://github.com/sony/ds-research-code/tree/master/recsys25-IntervalLLM.",
    "code_links": [
      "https://github.com/sony/ds-research-code"
    ],
    "comment": "Accepted by RecSys 2025 short paper track"
  },
  {
    "title": "Generative Recommendation with Semantic IDs: A Practitioner's Handbook",
    "authors": "Clark Mingxuan Ju, Liam Collins, Leonardo Neves, Bhuvesh Kumar, Louis Yufeng Wang, Tong Zhao, Neil Shah",
    "published": "2025-07-29",
    "arxiv_id": "2507.22224v1",
    "url": "http://arxiv.org/abs/2507.22224v1",
    "pdf_url": "http://arxiv.org/pdf/2507.22224v1",
    "category": "information_retrieval",
    "primary_category": "cs.IR",
    "abstract": "Generative recommendation (GR) has gained increasing attention for its\npromising performance compared to traditional models. A key factor contributing\nto the success of GR is the semantic ID (SID), which converts continuous\nsemantic representations (e.g., from large language models) into discrete ID\nsequences. This enables GR models with SIDs to both incorporate semantic\ninformation and learn collaborative filtering signals, while retaining the\nbenefits of discrete decoding. However, varied modeling techniques,\nhyper-parameters, and experimental setups in existing literature make direct\ncomparisons between GR proposals challenging. Furthermore, the absence of an\nopen-source, unified framework hinders systematic benchmarking and extension,\nslowing model iteration. To address this challenge, our work introduces and\nopen-sources a framework for Generative Recommendation with semantic ID, namely\nGRID, specifically designed for modularity to facilitate easy component\nswapping and accelerate idea iteration. Using GRID, we systematically\nexperiment with and ablate different components of GR models with SIDs on\npublic benchmarks. Our comprehensive experiments with GRID reveal that many\noverlooked architectural components in GR models with SIDs substantially impact\nperformance. This offers both novel insights and validates the utility of an\nopen-source platform for robust benchmarking and GR research advancement. GRID\nis open-sourced at https://github.com/snap-research/GRID.",
    "code_links": [
      "https://github.com/snap-research/GRID"
    ],
    "comment": null
  },
  {
    "title": "StructText: A Synthetic Table-to-Text Approach for Benchmark Generation with Multi-Dimensional Evaluation",
    "authors": "Satyananda Kashyap, Sola Shirai, Nandana Mihindukulasooriya, Horst Samulowitz",
    "published": "2025-07-28",
    "arxiv_id": "2507.21340v1",
    "url": "http://arxiv.org/abs/2507.21340v1",
    "pdf_url": "http://arxiv.org/pdf/2507.21340v1",
    "category": "information_retrieval",
    "primary_category": "cs.CL",
    "abstract": "Extracting structured information from text, such as key-value pairs that\ncould augment tabular data, is quite useful in many enterprise use cases.\nAlthough large language models (LLMs) have enabled numerous automated pipelines\nfor converting natural language into structured formats, there is still a lack\nof benchmarks for evaluating their extraction quality, especially in specific\ndomains or focused documents specific to a given organization. Building such\nbenchmarks by manual annotations is labour-intensive and limits the size and\nscalability of the benchmarks. In this work, we present StructText, an\nend-to-end framework for automatically generating high-fidelity benchmarks for\nkey-value extraction from text using existing tabular data. It uses available\ntabular data as structured ground truth, and follows a two-stage\n``plan-then-execute'' pipeline to synthetically generate corresponding\nnatural-language text. To ensure alignment between text and structured source,\nwe introduce a multi-dimensional evaluation strategy that combines (a)\nLLM-based judgments on factuality, hallucination, and coherence and (b)\nobjective extraction metrics measuring numeric and temporal accuracy. We\nevaluated the proposed method on 71,539 examples across 49 datasets. Results\nreveal that while LLMs achieve strong factual accuracy and avoid hallucination,\nthey struggle with narrative coherence in producing extractable text. Notably,\nmodels presume numerical and temporal information with high fidelity yet this\ninformation becomes embedded in narratives that resist automated extraction. We\nrelease a framework, including datasets, evaluation tools, and baseline\nextraction systems, to support continued research.",
    "code_links": [
      "https://github.com/ibm/struct-text"
    ],
    "comment": "Data available:\n  https://huggingface.co/datasets/ibm-research/struct-text and code available\n  at: https://github.com/ibm/struct-text"
  },
  {
    "title": "CRINN: Contrastive Reinforcement Learning for Approximate Nearest Neighbor Search",
    "authors": "Xiaoya Li, Xiaofei Sun, Albert Wang, Chris Shum, Jiwei Li",
    "published": "2025-08-04",
    "arxiv_id": "2508.02091v1",
    "url": "http://arxiv.org/abs/2508.02091v1",
    "pdf_url": "http://arxiv.org/pdf/2508.02091v1",
    "category": "databases",
    "primary_category": "cs.LG",
    "abstract": "Approximate nearest-neighbor search (ANNS) algorithms have become\nincreasingly critical for recent AI applications, particularly in\nretrieval-augmented generation (RAG) and agent-based LLM applications. In this\npaper, we present CRINN, a new paradigm for ANNS algorithms. CRINN treats ANNS\noptimization as a reinforcement learning problem where execution speed serves\nas the reward signal. This approach enables the automatic generation of\nprogressively faster ANNS implementations while maintaining accuracy\nconstraints. Our experimental evaluation demonstrates CRINN's effectiveness\nacross six widely-used NNS benchmark datasets. When compared against\nstate-of-the-art open-source ANNS algorithms, CRINN achieves best performance\non three of them (GIST-960-Euclidean, MNIST-784-Euclidean, and\nGloVe-25-angular), and tied for first place on two of them (SIFT-128-Euclidean\nand GloVe-25-angular). The implications of CRINN's success reach well beyond\nANNS optimization: It validates that LLMs augmented with reinforcement learning\ncan function as an effective tool for automating sophisticated algorithmic\noptimizations that demand specialized knowledge and labor-intensive manual\nrefinement.Code can be found at https://github.com/deepreinforce-ai/CRINN",
    "code_links": [
      "https://github.com/deepreinforce-ai/CRINN"
    ],
    "comment": "Preprint Version"
  },
  {
    "title": "DBAIOps: A Reasoning LLM-Enhanced Database Operation and Maintenance System using Knowledge Graphs",
    "authors": "Wei Zhou, Peng Sun, Xuanhe Zhou, Qianglei Zang, Ji Xu, Tieying Zhang, Guoliang Li, Fan Wu",
    "published": "2025-08-02",
    "arxiv_id": "2508.01136v1",
    "url": "http://arxiv.org/abs/2508.01136v1",
    "pdf_url": "http://arxiv.org/pdf/2508.01136v1",
    "category": "databases",
    "primary_category": "cs.DB",
    "abstract": "The operation and maintenance (O&M) of database systems is critical to\nensuring system availability and performance, typically requiring expert\nexperience (e.g., identifying metric-to-anomaly relations) for effective\ndiagnosis and recovery. However, existing automatic database O&M methods,\nincluding commercial products, cannot effectively utilize expert experience. On\nthe one hand, rule-based methods only support basic O&M tasks (e.g.,\nmetric-based anomaly detection), which are mostly numerical equations and\ncannot effectively incorporate literal O&M experience (e.g., troubleshooting\nguidance in manuals). On the other hand, LLM-based methods, which retrieve\nfragmented information (e.g., standard documents + RAG), often generate\ninaccurate or generic results. To address these limitations, we present\nDBAIOps, a novel hybrid database O&M system that combines reasoning LLMs with\nknowledge graphs to achieve DBA-style diagnosis. First, DBAIOps introduces a\nheterogeneous graph model for representing the diagnosis experience, and\nproposes a semi-automatic graph construction algorithm to build that graph from\nthousands of documents. Second, DBAIOps develops a collection of (800+)\nreusable anomaly models that identify both directly alerted metrics and\nimplicitly correlated experience and metrics. Third, for each anomaly, DBAIOps\nproposes a two-stage graph evolution mechanism to explore relevant diagnosis\npaths and identify missing relations automatically. It then leverages a\nreasoning LLM (e.g., DeepSeek-R1) to infer root causes and generate clear\ndiagnosis reports for both DBAs and common users. Our evaluation over four\nmainstream database systems (Oracle, MySQL, PostgreSQL, and DM8) demonstrates\nthat DBAIOps outperforms state-of-the-art baselines, 34.85% and 47.22% higher\nin root cause and human evaluation accuracy, respectively.",
    "code_links": [
      "https://github.com/weAIDB/DBAIOps"
    ],
    "comment": "DBAIOps supports 25 database systems and has been deployed in 20\n  real-world scenarios, covering domains like finance, energy, and healthcare.\n  See website at: https://www.dbaiops.com; See code at:\n  https://github.com/weAIDB/DBAIOps/"
  },
  {
    "title": "StructText: A Synthetic Table-to-Text Approach for Benchmark Generation with Multi-Dimensional Evaluation",
    "authors": "Satyananda Kashyap, Sola Shirai, Nandana Mihindukulasooriya, Horst Samulowitz",
    "published": "2025-07-28",
    "arxiv_id": "2507.21340v1",
    "url": "http://arxiv.org/abs/2507.21340v1",
    "pdf_url": "http://arxiv.org/pdf/2507.21340v1",
    "category": "databases",
    "primary_category": "cs.CL",
    "abstract": "Extracting structured information from text, such as key-value pairs that\ncould augment tabular data, is quite useful in many enterprise use cases.\nAlthough large language models (LLMs) have enabled numerous automated pipelines\nfor converting natural language into structured formats, there is still a lack\nof benchmarks for evaluating their extraction quality, especially in specific\ndomains or focused documents specific to a given organization. Building such\nbenchmarks by manual annotations is labour-intensive and limits the size and\nscalability of the benchmarks. In this work, we present StructText, an\nend-to-end framework for automatically generating high-fidelity benchmarks for\nkey-value extraction from text using existing tabular data. It uses available\ntabular data as structured ground truth, and follows a two-stage\n``plan-then-execute'' pipeline to synthetically generate corresponding\nnatural-language text. To ensure alignment between text and structured source,\nwe introduce a multi-dimensional evaluation strategy that combines (a)\nLLM-based judgments on factuality, hallucination, and coherence and (b)\nobjective extraction metrics measuring numeric and temporal accuracy. We\nevaluated the proposed method on 71,539 examples across 49 datasets. Results\nreveal that while LLMs achieve strong factual accuracy and avoid hallucination,\nthey struggle with narrative coherence in producing extractable text. Notably,\nmodels presume numerical and temporal information with high fidelity yet this\ninformation becomes embedded in narratives that resist automated extraction. We\nrelease a framework, including datasets, evaluation tools, and baseline\nextraction systems, to support continued research.",
    "code_links": [
      "https://github.com/ibm/struct-text"
    ],
    "comment": "Data available:\n  https://huggingface.co/datasets/ibm-research/struct-text and code available\n  at: https://github.com/ibm/struct-text"
  },
  {
    "title": "MH-GIN: Multi-scale Heterogeneous Graph-based Imputation Network for AIS Data (Extended Version)",
    "authors": "Hengyu Liu, Tianyi Li, Yuqiang He, Kristian Torp, Yushuai Li, Christian S. Jensen",
    "published": "2025-07-27",
    "arxiv_id": "2507.20362v1",
    "url": "http://arxiv.org/abs/2507.20362v1",
    "pdf_url": "http://arxiv.org/pdf/2507.20362v1",
    "category": "databases",
    "primary_category": "cs.LG",
    "abstract": "Location-tracking data from the Automatic Identification System, much of\nwhich is publicly available, plays a key role in a range of maritime safety and\nmonitoring applications. However, the data suffers from missing values that\nhamper downstream applications. Imputing the missing values is challenging\nbecause the values of different heterogeneous attributes are updated at diverse\nrates, resulting in the occurrence of multi-scale dependencies among\nattributes. Existing imputation methods that assume similar update rates across\nattributes are unable to capture and exploit such dependencies, limiting their\nimputation accuracy. We propose MH-GIN, a Multi-scale Heterogeneous Graph-based\nImputation Network that aims improve imputation accuracy by capturing\nmulti-scale dependencies. Specifically, MH-GIN first extracts multi-scale\ntemporal features for each attribute while preserving their intrinsic\nheterogeneous characteristics. Then, it constructs a multi-scale heterogeneous\ngraph to explicitly model dependencies between heterogeneous attributes to\nenable more accurate imputation of missing values through graph propagation.\nExperimental results on two real-world datasets find that MH-GIN is capable of\nan average 57% reduction in imputation errors compared to state-of-the-art\nmethods, while maintaining computational efficiency. The source code and\nimplementation details of MH-GIN are publicly available\nhttps://github.com/hyLiu1994/MH-GIN.",
    "code_links": [
      "https://github.com/hyLiu1994/MH-GIN"
    ],
    "comment": "18 pages, 4 figures"
  },
  {
    "title": "Graph-Structured Data Analysis of Component Failure in Autonomous Cargo Ships Based on Feature Fusion",
    "authors": "Zizhao Zhang, Tianxiang Zhao, Yu Sun, Liping Sun, Jichuan Kang",
    "published": "2025-07-18",
    "arxiv_id": "2507.13721v1",
    "url": "http://arxiv.org/abs/2507.13721v1",
    "pdf_url": "http://arxiv.org/pdf/2507.13721v1",
    "category": "databases",
    "primary_category": "cs.LG",
    "abstract": "To address the challenges posed by cascading reactions caused by component\nfailures in autonomous cargo ships (ACS) and the uncertainties in emergency\ndecision-making, this paper proposes a novel hybrid feature fusion framework\nfor constructing a graph-structured dataset of failure modes. By employing an\nimproved cuckoo search algorithm (HN-CSA), the literature retrieval efficiency\nis significantly enhanced, achieving improvements of 7.1% and 3.4% compared to\nthe NSGA-II and CSA search algorithms, respectively. A hierarchical feature\nfusion framework is constructed, using Word2Vec encoding to encode\nsubsystem/component features, BERT-KPCA to process failure modes/reasons, and\nSentence-BERT to quantify the semantic association between failure impact and\nemergency decision-making. The dataset covers 12 systems, 1,262 failure modes,\nand 6,150 propagation paths. Validation results show that the GATE-GNN model\nachieves a classification accuracy of 0.735, comparable to existing benchmarks.\nAdditionally, a silhouette coefficient of 0.641 indicates that the features are\nhighly distinguishable. In the label prediction results, the Shore-based\nMeteorological Service System achieved an F1 score of 0.93, demonstrating high\nprediction accuracy. This paper not only provides a solid foundation for\nfailure analysis in autonomous cargo ships but also offers reliable support for\nfault diagnosis, risk assessment, and intelligent decision-making systems. The\nlink to the dataset is\nhttps://github.com/wojiufukele/Graph-Structured-about-CSA.",
    "code_links": [
      "https://github.com/wojiufukele/Graph-Structured-about-CSA"
    ],
    "comment": null
  },
  {
    "title": "TOPJoin: A Context-Aware Multi-Criteria Approach for Joinable Column Search",
    "authors": "Harsha Kokel, Aamod Khatiwada, Tejaswini Pedapati, Haritha Ananthakrishnan, Oktie Hassanzadeh, Horst Samulowitz, Kavitha Srinivas",
    "published": "2025-07-15",
    "arxiv_id": "2507.11505v1",
    "url": "http://arxiv.org/abs/2507.11505v1",
    "pdf_url": "http://arxiv.org/pdf/2507.11505v1",
    "category": "databases",
    "primary_category": "cs.DB",
    "abstract": "One of the major challenges in enterprise data analysis is the task of\nfinding joinable tables that are conceptually related and provide meaningful\ninsights. Traditionally, joinable tables have been discovered through a search\nfor similar columns, where two columns are considered similar syntactically if\nthere is a set overlap or they are considered similar semantically if either\nthe column embeddings or value embeddings are closer in the embedding space.\nHowever, for enterprise data lakes, column similarity is not sufficient to\nidentify joinable columns and tables. The context of the query column is\nimportant. Hence, in this work, we first define context-aware column\njoinability. Then we propose a multi-criteria approach, called TOPJoin, for\njoinable column search. We evaluate TOPJoin against existing join search\nbaselines over one academic and one real-world join search benchmark. Through\nexperiments, we find that TOPJoin performs better on both benchmarks than the\nbaselines.",
    "code_links": [
      "https://github.com/IBM/ContextAwareJoin"
    ],
    "comment": "VLDB 2025 Workshop: Tabular Data Analysis (TaDA); The source code,\n  data, and/or other artifacts have been made available at\n  https://github.com/IBM/ContextAwareJoin"
  }
]