[
  {
    "title": "Rank-GRPO: Training LLM-based Conversational Recommender Systems with Reinforcement Learning",
    "authors": "Yaochen Zhu, Harald Steck, Dawen Liang, Yinhan He, Vito Ostuni, Jundong Li, Nathan Kallus",
    "published": "2025-10-23",
    "arxiv_id": "2510.20150v2",
    "url": "http://arxiv.org/abs/2510.20150v2",
    "pdf_url": "http://arxiv.org/pdf/2510.20150v2",
    "category": "information_retrieval",
    "primary_category": "cs.IR",
    "abstract": "Large language models (LLMs) are reshaping the recommender system paradigm by\nenabling users to express preferences and receive recommendations through\nconversations. Yet, aligning LLMs to the recommendation task remains\nchallenging: pretrained LLMs often generate out-of-catalog items, violate\nrequired output formats, and their ranking quality degrades sharply toward the\nend of the generated list. To this end, we propose ConvRec-R1, a two-stage\nframework for end-to-end training of LLM-based conversational recommender\nsystems. In Stage 1, we construct a behavioral-cloning dataset with a\nRemap-Reflect-Adjust pipeline, which produces high-quality, catalog-grounded\ndemonstrations from powerful blackbox LLMs to warm-start the RL training. In\nStage 2, we propose Rank-GRPO, a principled extension of group relative policy\noptimization (GRPO) tailored to tasks with rank-style outputs. Rank-GRPO treats\neach rank in the recommendation list as the unit instead of token (too\nfine-grained) or sequence (too coarse), redefining rewards to remove non-causal\ncredit assignment and introducing a rank-level importance ratio based on the\ngeometric mean of rank-wise token probabilities to stabilize policy updates.\nExperiments on the public Reddit-v2 dataset show that ConvRec-R1 converges\nfaster and achieves higher Recall and NDCG than GRPO-style baselines. Code and\ndatasets are released at https://github.com/yaochenzhu/Rank-GRPO.",
    "code_links": [
      "https://github.com/yaochenzhu/Rank-GRPO"
    ],
    "comment": null
  },
  {
    "title": "FACE: A General Framework for Mapping Collaborative Filtering Embeddings into LLM Tokens",
    "authors": "Chao Wang, Yixin Song, Jinhui Ye, Chuan Qin, Dazhong Shen, Lingfeng Liu, Xiang Wang, Yanyong Zhang",
    "published": "2025-10-17",
    "arxiv_id": "2510.15729v1",
    "url": "http://arxiv.org/abs/2510.15729v1",
    "pdf_url": "http://arxiv.org/pdf/2510.15729v1",
    "category": "information_retrieval",
    "primary_category": "cs.IR",
    "abstract": "Recently, large language models (LLMs) have been explored for integration\nwith collaborative filtering (CF)-based recommendation systems, which are\ncrucial for personalizing user experiences. However, a key challenge is that\nLLMs struggle to interpret the latent, non-semantic embeddings produced by CF\napproaches, limiting recommendation effectiveness and further applications. To\naddress this, we propose FACE, a general interpretable framework that maps CF\nembeddings into pre-trained LLM tokens. Specifically, we introduce a\ndisentangled projection module to decompose CF embeddings into concept-specific\nvectors, followed by a quantized autoencoder to convert continuous embeddings\ninto LLM tokens (descriptors). Then, we design a contrastive alignment\nobjective to ensure that the tokens align with corresponding textual signals.\nHence, the model-agnostic FACE framework achieves semantic alignment without\nfine-tuning LLMs and enhances recommendation performance by leveraging their\npre-trained capabilities. Empirical results on three real-world recommendation\ndatasets demonstrate performance improvements in benchmark models, with\ninterpretability studies confirming the interpretability of the descriptors.\nCode is available in https://github.com/YixinRoll/FACE.",
    "code_links": [
      "https://github.com/YixinRoll/FACE"
    ],
    "comment": "Accepted by NeurIPS 2025"
  },
  {
    "title": "GraphMind: Interactive Novelty Assessment System for Accelerating Scientific Discovery",
    "authors": "Italo Luis da Silva, Hanqi Yan, Lin Gui, Yulan He",
    "published": "2025-10-17",
    "arxiv_id": "2510.15706v1",
    "url": "http://arxiv.org/abs/2510.15706v1",
    "pdf_url": "http://arxiv.org/pdf/2510.15706v1",
    "category": "information_retrieval",
    "primary_category": "cs.IR",
    "abstract": "Large Language Models (LLMs) show strong reasoning and text generation\ncapabilities, prompting their use in scientific literature analysis, including\nnovelty assessment. While evaluating novelty of scientific papers is crucial\nfor peer review, it requires extensive knowledge of related work, something not\nall reviewers have. While recent work on LLM-assisted scientific literature\nanalysis supports literature comparison, existing approaches offer limited\ntransparency and lack mechanisms for result traceability via an information\nretrieval module. To address this gap, we introduce $\\textbf{GraphMind}$, an\neasy-to-use interactive web tool designed to assist users in evaluating the\nnovelty of scientific papers or drafted ideas. Specially, $\\textbf{GraphMind}$\nenables users to capture the main structure of a scientific paper, explore\nrelated ideas through various perspectives, and assess novelty via providing\nverifiable contextual insights. $\\textbf{GraphMind}$ enables users to annotate\nkey elements of a paper, explore related papers through various relationships,\nand assess novelty with contextual insight. This tool integrates external APIs\nsuch as arXiv and Semantic Scholar with LLMs to support annotation, extraction,\nretrieval and classification of papers. This combination provides users with a\nrich, structured view of a scientific idea's core contributions and its\nconnections to existing work. $\\textbf{GraphMind}$ is available at\nhttps://oyarsa.github.io/graphmind and a demonstration video at\nhttps://youtu.be/wKbjQpSvwJg. The source code is available at\nhttps://github.com/oyarsa/graphmind.",
    "code_links": [
      "https://github.com/oyarsa/graphmind"
    ],
    "comment": "9 pages, 6 figures, 3 tables, EMNLP 2025 Demo paper"
  },
  {
    "title": "Mixture of Experts Approaches in Dense Retrieval Tasks",
    "authors": "Effrosyni Sokli, Pranav Kasela, Georgios Peikos, Gabriella Pasi",
    "published": "2025-10-17",
    "arxiv_id": "2510.15683v1",
    "url": "http://arxiv.org/abs/2510.15683v1",
    "pdf_url": "http://arxiv.org/pdf/2510.15683v1",
    "category": "information_retrieval",
    "primary_category": "cs.IR",
    "abstract": "Dense Retrieval Models (DRMs) are a prominent development in Information\nRetrieval (IR). A key challenge with these neural Transformer-based models is\nthat they often struggle to generalize beyond the specific tasks and domains\nthey were trained on. To address this challenge, prior research in IR\nincorporated the Mixture-of-Experts (MoE) framework within each Transformer\nlayer of a DRM, which, though effective, substantially increased the number of\nadditional parameters. In this paper, we propose a more efficient design, which\nintroduces a single MoE block (SB-MoE) after the final Transformer layer. To\nassess the retrieval effectiveness of SB-MoE, we perform an empirical\nevaluation across three IR tasks. Our experiments involve two evaluation\nsetups, aiming to assess both in-domain effectiveness and the model's zero-shot\ngeneralizability. In the first setup, we fine-tune SB-MoE with four different\nunderlying DRMs on seven IR benchmarks and evaluate them on their respective\ntest sets. In the second setup, we fine-tune SB-MoE on MSMARCO and perform\nzero-shot evaluation on thirteen BEIR datasets. Additionally, we perform\nfurther experiments to analyze the model's dependency on its hyperparameters\n(i.e., the number of employed and activated experts) and investigate how this\nvariation affects SB-MoE's performance. The obtained results show that SB-MoE\nis particularly effective for DRMs with lightweight base models, such as\nTinyBERT and BERT-Small, consistently exceeding standard model fine-tuning\nacross benchmarks. For DRMs with more parameters, such as BERT-Base and\nContriever, our model requires a larger number of training samples to achieve\nimproved retrieval performance. Our code is available online at:\nhttps://github.com/FaySokli/SB-MoE.",
    "code_links": [
      "https://github.com/FaySokli/SB-MoE"
    ],
    "comment": "8 pages, 4 figures, 3 tables, reproducible code available at\n  https://github.com/FaySokli/SB-MoE , Accepted for publication in Proceedings\n  of the 2025 IEEE/WIC International Conference on Web Intelligence and\n  Intelligent Agent Technology (WI-IAT 2025)"
  },
  {
    "title": "BPL: Bias-adaptive Preference Distillation Learning for Recommender System",
    "authors": "SeongKu Kang, Jianxun Lian, Dongha Lee, Wonbin Kweon, Sanghwan Jang, Jaehyun Lee, Jindong Wang, Xing Xie, Hwanjo Yu",
    "published": "2025-10-17",
    "arxiv_id": "2510.16076v1",
    "url": "http://arxiv.org/abs/2510.16076v1",
    "pdf_url": "http://arxiv.org/pdf/2510.16076v1",
    "category": "information_retrieval",
    "primary_category": "cs.LG",
    "abstract": "Recommender systems suffer from biases that cause the collected feedback to\nincompletely reveal user preference. While debiasing learning has been\nextensively studied, they mostly focused on the specialized (called\ncounterfactual) test environment simulated by random exposure of items,\nsignificantly degrading accuracy in the typical (called factual) test\nenvironment based on actual user-item interactions. In fact, each test\nenvironment highlights the benefit of a different aspect: the counterfactual\ntest emphasizes user satisfaction in the long-terms, while the factual test\nfocuses on predicting subsequent user behaviors on platforms. Therefore, it is\ndesirable to have a model that performs well on both tests rather than only\none. In this work, we introduce a new learning framework, called Bias-adaptive\nPreference distillation Learning (BPL), to gradually uncover user preferences\nwith dual distillation strategies. These distillation strategies are designed\nto drive high performance in both factual and counterfactual test environments.\nEmploying a specialized form of teacher-student distillation from a biased\nmodel, BPL retains accurate preference knowledge aligned with the collected\nfeedback, leading to high performance in the factual test. Furthermore, through\nself-distillation with reliability filtering, BPL iteratively refines its\nknowledge throughout the training process. This enables the model to produce\nmore accurate predictions across a broader range of user-item combinations,\nthereby improving performance in the counterfactual test. Comprehensive\nexperiments validate the effectiveness of BPL in both factual and\ncounterfactual tests. Our implementation is accessible via:\nhttps://github.com/SeongKu-Kang/BPL.",
    "code_links": [
      "https://github.com/SeongKu-Kang/BPL"
    ],
    "comment": "\\c{opyright} 2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works"
  },
  {
    "title": "Structure-R1: Dynamically Leveraging Structural Knowledge in LLM Reasoning through Reinforcement Learning",
    "authors": "Junlin Wu, Xianrui Zhong, Jiashuo Sun, Bolian Li, Bowen Jin, Jiawei Han, Qingkai Zeng",
    "published": "2025-10-16",
    "arxiv_id": "2510.15191v1",
    "url": "http://arxiv.org/abs/2510.15191v1",
    "pdf_url": "http://arxiv.org/pdf/2510.15191v1",
    "category": "information_retrieval",
    "primary_category": "cs.CL",
    "abstract": "Large language models (LLMs) have demonstrated remarkable advances in\nreasoning capabilities. However, their performance remains constrained by\nlimited access to explicit and structured domain knowledge. Retrieval-Augmented\nGeneration (RAG) addresses this by incorporating external information as\ncontext to augment reasoning. Nevertheless, traditional RAG systems typically\noperate over unstructured and fragmented text, resulting in low information\ndensity and suboptimal reasoning. To overcome these limitations, we propose\n\\textsc{Structure-R1}, a novel framework that transforms retrieved content into\nstructured representations optimized for reasoning. Leveraging reinforcement\nlearning, \\textsc{Structure-R1} learns a content representation policy that\ndynamically generates and adapts structural formats based on the demands of\nmulti-step reasoning. Unlike prior methods that rely on fixed schemas, our\napproach adopts a generative paradigm capable of producing task-specific\nstructures tailored to individual queries. To ensure the quality and\nreliability of these representations, we introduce a self-reward structural\nverification mechanism that checks whether the generated structures are both\ncorrect and self-contained. Extensive experiments on seven knowledge-intensive\nbenchmarks show that \\textsc{Structure-R1} consistently achieves competitive\nperformance with a 7B-scale backbone model and matches the performance of much\nlarger models. Additionally, our theoretical analysis demonstrates how\nstructured representations enhance reasoning by improving information density\nand contextual clarity. Our code and data are available at:\nhttps://github.com/jlwu002/sr1.",
    "code_links": [
      "https://github.com/jlwu002/sr1"
    ],
    "comment": null
  },
  {
    "title": "DMRetriever: A Family of Models for Improved Text Retrieval in Disaster Management",
    "authors": "Kai Yin, Xiangjue Dong, Chengkai Liu, Allen Lin, Lingfeng Shi, Ali Mostafavi, James Caverlee",
    "published": "2025-10-16",
    "arxiv_id": "2510.15087v1",
    "url": "http://arxiv.org/abs/2510.15087v1",
    "pdf_url": "http://arxiv.org/pdf/2510.15087v1",
    "category": "information_retrieval",
    "primary_category": "cs.IR",
    "abstract": "Effective and efficient access to relevant information is essential for\ndisaster management. However, no retrieval model is specialized for disaster\nmanagement, and existing general-domain models fail to handle the varied search\nintents inherent to disaster management scenarios, resulting in inconsistent\nand unreliable performance. To this end, we introduce DMRetriever, the first\nseries of dense retrieval models (33M to 7.6B) tailored for this domain. It is\ntrained through a novel three-stage framework of bidirectional attention\nadaptation, unsupervised contrastive pre-training, and difficulty-aware\nprogressive instruction fine-tuning, using high-quality data generated through\nan advanced data refinement pipeline. Comprehensive experiments demonstrate\nthat DMRetriever achieves state-of-the-art (SOTA) performance across all six\nsearch intents at every model scale. Moreover, DMRetriever is highly\nparameter-efficient, with 596M model outperforming baselines over 13.3 X larger\nand 33M model exceeding baselines with only 7.6% of their parameters. All\ncodes, data, and checkpoints are available at\nhttps://github.com/KaiYin97/DMRETRIEVER",
    "code_links": [
      "https://github.com/KaiYin97/DMRETRIEVER"
    ],
    "comment": null
  },
  {
    "title": "Beyond Static LLM Policies: Imitation-Enhanced Reinforcement Learning for Recommendation",
    "authors": "Yi Zhang, Lili Xie, Ruihong Qiu, Jiajun Liu, Sen Wang",
    "published": "2025-10-15",
    "arxiv_id": "2510.13229v1",
    "url": "http://arxiv.org/abs/2510.13229v1",
    "pdf_url": "http://arxiv.org/pdf/2510.13229v1",
    "category": "information_retrieval",
    "primary_category": "cs.IR",
    "abstract": "Recommender systems (RecSys) have become critical tools for enhancing user\nengagement by delivering personalized content across diverse digital platforms.\nRecent advancements in large language models (LLMs) demonstrate significant\npotential for improving RecSys, primarily due to their exceptional\ngeneralization capabilities and sophisticated contextual understanding, which\nfacilitate the generation of flexible and interpretable recommendations.\nHowever, the direct deployment of LLMs as primary recommendation policies\npresents notable challenges, including persistent latency issues stemming from\nfrequent API calls and inherent model limitations such as hallucinations and\nbiases. To address these issues, this paper proposes a novel offline\nreinforcement learning (RL) framework that leverages imitation learning from\nLLM-generated trajectories. Specifically, inverse reinforcement learning is\nemployed to extract robust reward models from LLM demonstrations. This approach\nnegates the need for LLM fine-tuning, thereby substantially reducing\ncomputational overhead. Simultaneously, the RL policy is guided by the\ncumulative rewards derived from these demonstrations, effectively transferring\nthe semantic insights captured by the LLM. Comprehensive experiments conducted\non two benchmark datasets validate the effectiveness of the proposed method,\ndemonstrating superior performance when compared against state-of-the-art\nRL-based and in-context learning baselines. The code can be found at\nhttps://github.com/ArronDZhang/IL-Rec.",
    "code_links": [
      "https://github.com/ArronDZhang/IL-Rec"
    ],
    "comment": "ICDM 2025 Accepted Paper"
  },
  {
    "title": "ReMindRAG: Low-Cost LLM-Guided Knowledge Graph Traversal for Efficient RAG",
    "authors": "Yikuan Hu, Jifeng Zhu, Lanrui Tang, Chen Huang",
    "published": "2025-10-15",
    "arxiv_id": "2510.13193v2",
    "url": "http://arxiv.org/abs/2510.13193v2",
    "pdf_url": "http://arxiv.org/pdf/2510.13193v2",
    "category": "information_retrieval",
    "primary_category": "cs.IR",
    "abstract": "Knowledge graphs (KGs), with their structured representation capabilities,\noffer promising avenue for enhancing Retrieval Augmented Generation (RAG)\nsystems, leading to the development of KG-RAG systems. Nevertheless, existing\nmethods often struggle to achieve effective synergy between system\neffectiveness and cost efficiency, leading to neither unsatisfying performance\nnor excessive LLM prompt tokens and inference time. To this end, this paper\nproposes REMINDRAG, which employs an LLM-guided graph traversal featuring node\nexploration, node exploitation, and, most notably, memory replay, to improve\nboth system effectiveness and cost efficiency. Specifically, REMINDRAG\nmemorizes traversal experience within KG edge embeddings, mirroring the way\nLLMs \"memorize\" world knowledge within their parameters, but in a train-free\nmanner. We theoretically and experimentally confirm the effectiveness of\nREMINDRAG, demonstrating its superiority over existing baselines across various\nbenchmark datasets and LLM backbones. Our code is available at\nhttps://github.com/kilgrims/ReMindRAG.",
    "code_links": [
      "https://github.com/kilgrims/ReMindRAG"
    ],
    "comment": "Accepted by NeurIPS 2025"
  },
  {
    "title": "DynaQuery: A Self-Adapting Framework for Querying Structured and Multimodal Data",
    "authors": "Aymane Hassini",
    "published": "2025-10-20",
    "arxiv_id": "2510.18029v1",
    "url": "http://arxiv.org/abs/2510.18029v1",
    "pdf_url": "http://arxiv.org/pdf/2510.18029v1",
    "category": "databases",
    "primary_category": "cs.DB",
    "abstract": "The rise of Large Language Models (LLMs) has accelerated the long-standing\ngoal of enabling natural language querying over complex, hybrid databases. Yet,\nthis ambition exposes a dual challenge: reasoning jointly over structured,\nmulti-relational schemas and the semantic content of linked unstructured\nassets. To overcome this, we present DynaQuery - a unified, self-adapting\nframework that serves as a practical blueprint for next-generation \"Unbound\nDatabases.\" At the heart of DynaQuery lies the Schema Introspection and Linking\nEngine (SILE), a novel systems primitive that elevates schema linking to a\nfirst-class query planning phase. We conduct a rigorous, multi-benchmark\nempirical evaluation of this structure-aware architecture against the prevalent\nunstructured Retrieval-Augmented Generation (RAG) paradigm. Our results\ndemonstrate that the unstructured retrieval paradigm is architecturally\nsusceptible to catastrophic contextual failures, such as SCHEMA_HALLUCINATION,\nleading to unreliable query generation. In contrast, our SILE-based design\nestablishes a substantially more robust foundation, nearly eliminating this\nfailure mode. Moreover, end-to-end validation on a complex, newly curated\nbenchmark uncovers a key generalization principle: the transition from pure\nschema-awareness to holistic semantics-awareness. Taken together, our findings\nprovide a validated architectural basis for developing natural language\ndatabase interfaces that are robust, adaptable, and predictably consistent.",
    "code_links": [
      "https://github.com/aymanehassini/DynaQuery"
    ],
    "comment": "15 pages, 2 figures, 10 tables. Source code and experimental\n  artifacts are available at: https://github.com/aymanehassini/DynaQuery . The\n  'DynaQuery-Eval-5K' benchmark, introduced in this work, is also publicly\n  available at:\n  https://www.kaggle.com/datasets/aymanehassini/dynaquery-eval-5k-benchmark"
  },
  {
    "title": "DeepAnalyze: Agentic Large Language Models for Autonomous Data Science",
    "authors": "Shaolei Zhang, Ju Fan, Meihao Fan, Guoliang Li, Xiaoyong Du",
    "published": "2025-10-19",
    "arxiv_id": "2510.16872v1",
    "url": "http://arxiv.org/abs/2510.16872v1",
    "pdf_url": "http://arxiv.org/pdf/2510.16872v1",
    "category": "databases",
    "primary_category": "cs.AI",
    "abstract": "Autonomous data science, from raw data sources to analyst-grade deep research\nreports, has been a long-standing challenge, and is now becoming feasible with\nthe emergence of powerful large language models (LLMs). Recent workflow-based\ndata agents have shown promising results on specific data tasks but remain\nfundamentally limited in achieving fully autonomous data science due to their\nreliance on predefined workflows. In this paper, we introduce DeepAnalyze-8B,\nthe first agentic LLM designed for autonomous data science, capable of\nautomatically completing the end-toend pipeline from data sources to\nanalyst-grade deep research reports. To tackle high-complexity data science\ntasks, we propose a curriculum-based agentic training paradigm that emulates\nthe learning trajectory of human data scientists, enabling LLMs to\nprogressively acquire and integrate multiple capabilities in real-world\nenvironments. We also introduce a data-grounded trajectory synthesis framework\nthat constructs high-quality training data. Through agentic training,\nDeepAnalyze learns to perform a broad spectrum of data tasks, ranging from data\nquestion answering and specialized analytical tasks to open-ended data\nresearch. Experiments demonstrate that, with only 8B parameters, DeepAnalyze\noutperforms previous workflow-based agents built on most advanced proprietary\nLLMs. The model, code, and training data of DeepAnalyze are open-sourced,\npaving the way toward autonomous data science.",
    "code_links": [
      "https://github.com/ruc-datalab/DeepAnalyze"
    ],
    "comment": "Code: https://github.com/ruc-datalab/DeepAnalyze Model:\n  https://huggingface.co/RUC-DataLab/DeepAnalyze-8B"
  },
  {
    "title": "BenchPress: A Human-in-the-Loop Annotation System for Rapid Text-to-SQL Benchmark Curation",
    "authors": "Fabian Wenz, Omar Bouattour, Devin Yang, Justin Choi, Cecil Gregg, Nesime Tatbul, Çağatay Demiralp",
    "published": "2025-10-11",
    "arxiv_id": "2510.13853v1",
    "url": "http://arxiv.org/abs/2510.13853v1",
    "pdf_url": "http://arxiv.org/pdf/2510.13853v1",
    "category": "databases",
    "primary_category": "cs.CL",
    "abstract": "Large language models (LLMs) have been successfully applied to many tasks,\nincluding text-to-SQL generation. However, much of this work has focused on\npublicly available datasets, such as Fiben, Spider, and Bird. Our earlier work\nshowed that LLMs are much less effective in querying large private enterprise\ndata warehouses and released Beaver, the first private enterprise text-to-SQL\nbenchmark. To create Beaver, we leveraged SQL logs, which are often readily\navailable. However, manually annotating these logs to identify which natural\nlanguage questions they answer is a daunting task. Asking database\nadministrators, who are highly trained experts, to take on additional work to\nconstruct and validate corresponding natural language utterances is not only\nchallenging but also quite costly. To address this challenge, we introduce\nBenchPress, a human-in-the-loop system designed to accelerate the creation of\ndomain-specific text-to-SQL benchmarks. Given a SQL query, BenchPress uses\nretrieval-augmented generation (RAG) and LLMs to propose multiple natural\nlanguage descriptions. Human experts then select, rank, or edit these drafts to\nensure accuracy and domain alignment. We evaluated BenchPress on annotated\nenterprise SQL logs, demonstrating that LLM-assisted annotation drastically\nreduces the time and effort required to create high-quality benchmarks. Our\nresults show that combining human verification with LLM-generated suggestions\nenhances annotation accuracy, benchmark reliability, and model evaluation\nrobustness. By streamlining the creation of custom benchmarks, BenchPress\noffers researchers and practitioners a mechanism for assessing text-to-SQL\nmodels on a given domain-specific workload. BenchPress is freely available via\nour public GitHub repository at\nhttps://github.com/fabian-wenz/enterprise-txt2sql and is also accessible on our\nwebsite at http://dsg-mcgraw.csail.mit.edu:5000.",
    "code_links": [
      "https://github.com/fabian-wenz/enterprise-txt2sql"
    ],
    "comment": "CIDR'26"
  },
  {
    "title": "Regular Expression Indexing for Log Analysis. Extended Version",
    "authors": "Ling Zhang, Shaleen Deep, Jignesh M. Patel, Karthikeyan Sankaralingam",
    "published": "2025-10-11",
    "arxiv_id": "2510.10348v1",
    "url": "http://arxiv.org/abs/2510.10348v1",
    "pdf_url": "http://arxiv.org/pdf/2510.10348v1",
    "category": "databases",
    "primary_category": "cs.DB",
    "abstract": "In this paper, we present the design and architecture of REI, a novel system\nfor indexing log data for regular expression queries. Our main contribution is\nan $n$-gram-based indexing strategy and an efficient storage mechanism that\nresults in a speedup of up to 14x compared to state-of-the-art regex processing\nengines that do not use indexing, using only 2.1% of extra space. We perform a\ndetailed study that analyzes the space usage of the index and the improvement\nin workload execution time, uncovering interesting insights. Specifically, we\nshow that even an optimized implementation of strategies such as inverted\nindexing, which are widely used in text processing libraries, may lead to\nsuboptimal performance for regex indexing on log analysis tasks. Overall, the\nREI approach presented in this paper provides a significant boost when\nevaluating regular expression queries on log data. REI is also modular and can\nwork with existing regular expression packages, making it easy to deploy in a\nvariety of settings. The code of REI is available at\nhttps://github.com/mush-zhang/REI-Regular-Expression-Indexing.",
    "code_links": [
      "https://github.com/mush-zhang/REI-Regular-Expression-Indexing"
    ],
    "comment": null
  },
  {
    "title": "Efficient Mining of Low-Utility Sequential Patterns",
    "authors": "Jian Zhu, Zhidong Lin, Wensheng Gan, Ruichu Cai, Zhifeng Hao, Philip S. Yu",
    "published": "2025-10-11",
    "arxiv_id": "2510.10243v1",
    "url": "http://arxiv.org/abs/2510.10243v1",
    "pdf_url": "http://arxiv.org/pdf/2510.10243v1",
    "category": "databases",
    "primary_category": "cs.DB",
    "abstract": "Discovering valuable insights from rich data is a crucial task for\nexploratory data analysis. Sequential pattern mining (SPM) has found widespread\napplications across various domains. In recent years, low-utility sequential\npattern mining (LUSPM) has shown strong potential in applications such as\nintrusion detection and genomic sequence analysis. However, existing research\nin utility-based SPM focuses on high-utility sequential patterns, and the\ndefinitions and strategies used in high-utility SPM cannot be directly applied\nto LUSPM. Moreover, no algorithms have yet been developed specifically for\nmining low-utility sequential patterns. To address these problems, we formalize\nthe LUSPM problem, redefine sequence utility, and introduce a compact data\nstructure called the sequence-utility chain to efficiently record utility\ninformation. Furthermore, we propose three novel algorithm--LUSPM_b, LUSPM_s,\nand LUSPM_e--to discover the complete set of low-utility sequential patterns.\nLUSPM_b serves as an exhaustive baseline, while LUSPM_s and LUSPM_e build upon\nit, generating subsequences through shrinkage and extension operations,\nrespectively. In addition, we introduce the maximal non-mutually contained\nsequence set and incorporate multiple pruning strategies, which significantly\nreduce redundant operations in both LUSPM_s and LUSPM_e. Finally, extensive\nexperimental results demonstrate that both LUSPM_s and LUSPM_e substantially\noutperform LUSPM_b and exhibit excellent scalability. Notably, LUSPM_e achieves\nsuperior efficiency, requiring less runtime and memory consumption than\nLUSPM_s. Our code is available at https://github.com/Zhidong-Lin/LUSPM.",
    "code_links": [
      "https://github.com/Zhidong-Lin/LUSPM"
    ],
    "comment": "Preprint, 4 tables, 9 figures"
  },
  {
    "title": "Knowledge Graph-Guided Multi-Agent Distillation for Reliable Industrial Question Answering with Datasets",
    "authors": "Jiqun Pan, Zhenke Duan, Jiani Tu, Anzhi Cheng, Yanqing Wang",
    "published": "2025-10-03",
    "arxiv_id": "2510.06240v1",
    "url": "http://arxiv.org/abs/2510.06240v1",
    "pdf_url": "http://arxiv.org/pdf/2510.06240v1",
    "category": "databases",
    "primary_category": "cs.CL",
    "abstract": "Industrial question-answering (QA) systems require higher safety and\nreliability than general-purpose dialogue models, as errors in high-risk\nscenarios such as equipment fault diagnosis can have severe consequences.\nAlthough multi-agent large language models enhance reasoning depth, they suffer\nfrom uncontrolled iterations and unverifiable outputs, and conventional\ndistillation methods struggle to transfer collaborative reasoning capabilities\nto lightweight, deployable student models. To address these challenges, we\npropose Knowledge Graph-guided Multi-Agent System Distillation (KG-MASD). Our\napproach formulates distillation as a Markov Decision Process and incorporates\na knowledge graph as a verifiable structured prior to enrich state\nrepresentation and ensure convergence. By integrating collaborative reasoning\nwith knowledge grounding, KG-MASD generates high-confidence instruction-tuning\ndata and jointly distills reasoning depth and verifiability into compact\nstudent models suitable for edge deployment. Experiments on an industrial QA\ndataset show that KG-MASD improves accuracy by 2.4 per cent to 20.1 per cent\nover baselines and significantly enhances reliability, enabling trustworthy AI\ndeployment in safety-critical industrial scenarios. Code and data are available\nat https://github.com/erwinmsmith/KG-MAD/.",
    "code_links": [
      "https://github.com/erwinmsmith/KG-MAD"
    ],
    "comment": "41 pages, 12 figures, 6 tables"
  },
  {
    "title": "EMR-AGENT: Automating Cohort and Feature Extraction from EMR Databases",
    "authors": "Kwanhyung Lee, Sungsoo Hong, Joonhyung Park, Jeonghyeop Lim, Juhwan Choi, Donghwee Yoon, Eunho Yang",
    "published": "2025-10-01",
    "arxiv_id": "2510.00549v2",
    "url": "http://arxiv.org/abs/2510.00549v2",
    "pdf_url": "http://arxiv.org/pdf/2510.00549v2",
    "category": "databases",
    "primary_category": "cs.DB",
    "abstract": "Machine learning models for clinical prediction rely on structured data\nextracted from Electronic Medical Records (EMRs), yet this process remains\ndominated by hardcoded, database-specific pipelines for cohort definition,\nfeature selection, and code mapping. These manual efforts limit scalability,\nreproducibility, and cross-institutional generalization. To address this, we\nintroduce EMR-AGENT (Automated Generalized Extraction and Navigation Tool), an\nagent-based framework that replaces manual rule writing with dynamic, language\nmodel-driven interaction to extract and standardize structured clinical data.\nOur framework automates cohort selection, feature extraction, and code mapping\nthrough interactive querying of databases. Our modular agents iteratively\nobserve query results and reason over schema and documentation, using SQL not\njust for data retrieval but also as a tool for database observation and\ndecision making. This eliminates the need for hand-crafted, schema-specific\nlogic. To enable rigorous evaluation, we develop a benchmarking codebase for\nthree EMR databases (MIMIC-III, eICU, SICdb), including both seen and unseen\nschema settings. Our results demonstrate strong performance and generalization\nacross these databases, highlighting the feasibility of automating a process\npreviously thought to require expert-driven design. The code will be released\npublicly at https://github.com/AITRICS/EMR-AGENT/tree/main. For a\ndemonstration, please visit our anonymous demo page:\nhttps://anonymoususer-max600.github.io/EMR_AGENT/",
    "code_links": [
      "https://github.com/AITRICS/EMR-AGENT"
    ],
    "comment": "currently under submission to ICLR 2026"
  },
  {
    "title": "Graph2Region: Efficient Graph Similarity Learning with Structure and Scale Restoration",
    "authors": "Zhouyang Liu, Yixin Chen, Ning Liu, Jiezhong He, Dongsheng Li",
    "published": "2025-10-01",
    "arxiv_id": "2510.00394v1",
    "url": "http://arxiv.org/abs/2510.00394v1",
    "pdf_url": "http://arxiv.org/pdf/2510.00394v1",
    "category": "databases",
    "primary_category": "cs.LG",
    "abstract": "Graph similarity is critical in graph-related tasks such as graph retrieval,\nwhere metrics like maximum common subgraph (MCS) and graph edit distance (GED)\nare commonly used. However, exact computations of these metrics are known to be\nNP-Hard. Recent neural network-based approaches approximate the similarity\nscore in embedding spaces to alleviate the computational burden, but they\neither involve expensive pairwise node comparisons or fail to effectively\nutilize structural and scale information of graphs. To tackle these issues, we\npropose a novel geometric-based graph embedding method called Graph2Region\n(G2R). G2R represents nodes as closed regions and recovers their adjacency\npatterns within graphs in the embedding space. By incorporating the node\nfeatures and adjacency patterns of graphs, G2R summarizes graph regions, i.e.,\ngraph embeddings, where the shape captures the underlying graph structures and\nthe volume reflects the graph size. Consequently, the overlap between graph\nregions can serve as an approximation of MCS, signifying similar node regions\nand adjacency patterns. We further analyze the relationship between MCS and GED\nand propose using disjoint parts as a proxy for GED similarity. This analysis\nenables concurrent computation of MCS and GED, incorporating local and global\nstructural information. Experimental evaluation highlights G2R's competitive\nperformance in graph similarity computation. It achieves up to a 60.0\\%\nrelative accuracy improvement over state-of-the-art methods in MCS similarity\nlearning, while maintaining efficiency in both training and inference.\nMoreover, G2R showcases remarkable capability in predicting both MCS and GED\nsimilarities simultaneously, providing a holistic assessment of graph\nsimilarity. Code available at https://github.com/liuzhouyang/Graph2Region.",
    "code_links": [
      "https://github.com/liuzhouyang/Graph2Region"
    ],
    "comment": "Accepted by IEEE Transactions on Knowledge and Data Engineering"
  },
  {
    "title": "ActorDB: A Unified Database Model Integrating Single-Writer Actors, Incremental View Maintenance, and Zero-Trust Messaging",
    "authors": "Jun Kawasaki",
    "published": "2025-09-29",
    "arxiv_id": "2509.25285v1",
    "url": "http://arxiv.org/abs/2509.25285v1",
    "pdf_url": "http://arxiv.org/pdf/2509.25285v1",
    "category": "databases",
    "primary_category": "cs.DB",
    "abstract": "This paper presents ActorDB ( Dekigoto ) , a novel database architecture that\ntightly integrates a single-writer actor model for writes, Incremental View\nMaintenance (IVM), and a zero-trust security model as a core component. The\nprimary contribution of this work is the unification of these powerful but\ncomplex concepts into a single, cohesive system designed to reduce\narchitectural complexity for developers of modern, data-intensive applications.\nWe argue that by providing these capabilities out-of-the-box, ActorDB can offer\na more robust, secure, and developer-friendly platform compared to solutions\nthat require manual integration of separate systems for actor persistence,\nstream processing, and security. We present the core architecture, discuss the\ncritical trade-offs in its design, and define the performance criteria for a\nMinimum Viable Product (MVP) to validate our approach.",
    "code_links": [
      "https://github.com/com-junkawasaki/dekigoto"
    ],
    "comment": "7 pages, 1 table, 1 figures. Code and data available at\n  https://github.com/com-junkawasaki/dekigoto"
  },
  {
    "title": "Multilingual Text-to-SQL: Benchmarking the Limits of Language Models with Collaborative Language Agents",
    "authors": "Khanh Trinh Pham, Thu Huong Nguyen, Jun Jo, Quoc Viet Hung Nguyen, Thanh Tam Nguyen",
    "published": "2025-09-29",
    "arxiv_id": "2509.24405v1",
    "url": "http://arxiv.org/abs/2509.24405v1",
    "pdf_url": "http://arxiv.org/pdf/2509.24405v1",
    "category": "databases",
    "primary_category": "cs.CL",
    "abstract": "Text-to-SQL enables natural access to databases, yet most benchmarks are\nEnglish-only, limiting multilingual progress. We introduce MultiSpider 2.0,\nextending Spider 2.0 to eight languages (English, German, French, Spanish,\nPortuguese, Japanese, Chinese, Vietnamese). It preserves Spider 2.0's\nstructural difficulty while adding linguistic and dialectal variability,\ndemanding deeper reasoning for complex SQL. On this benchmark, state-of-the-art\nLLMs (such as DeepSeek-R1 and OpenAI o1) reach only 4\\% execution accuracy when\nrelying on intrinsic reasoning, versus 60\\% on MultiSpider 1.0. Therefore, we\nprovide a collaboration-driven language agents baseline that iteratively\nrefines queries, improving accuracy to 15\\%. These results reveal a substantial\nmultilingual gap and motivate methods that are robust across languages and\nready for real-world enterprise deployment. Our benchmark is available at\nhttps://github.com/phkhanhtrinh23/Multilingual_Text_to_SQL.",
    "code_links": [
      "https://github.com/phkhanhtrinh23/Multilingual_Text_to_SQL"
    ],
    "comment": null
  }
]