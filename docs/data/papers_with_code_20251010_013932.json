[
  {
    "title": "M3Retrieve: Benchmarking Multimodal Retrieval for Medicine",
    "authors": "Arkadeep Acharya, Akash Ghosh, Pradeepika Verma, Kitsuchart Pasupa, Sriparna Saha, Priti Singh",
    "published": "2025-10-08",
    "arxiv_id": "2510.06888v1",
    "url": "http://arxiv.org/abs/2510.06888v1",
    "pdf_url": "http://arxiv.org/pdf/2510.06888v1",
    "category": "information_retrieval",
    "primary_category": "cs.IR",
    "abstract": "With the increasing use of RetrievalAugmented Generation (RAG), strong\nretrieval models have become more important than ever. In healthcare,\nmultimodal retrieval models that combine information from both text and images\noffer major advantages for many downstream tasks such as question answering,\ncross-modal retrieval, and multimodal summarization, since medical data often\nincludes both formats. However, there is currently no standard benchmark to\nevaluate how well these models perform in medical settings. To address this\ngap, we introduce M3Retrieve, a Multimodal Medical Retrieval Benchmark.\nM3Retrieve, spans 5 domains,16 medical fields, and 4 distinct tasks, with over\n1.2 Million text documents and 164K multimodal queries, all collected under\napproved licenses. We evaluate leading multimodal retrieval models on this\nbenchmark to explore the challenges specific to different medical specialities\nand to understand their impact on retrieval performance. By releasing\nM3Retrieve, we aim to enable systematic evaluation, foster model innovation,\nand accelerate research toward building more capable and reliable multimodal\nretrieval systems for medical applications. The dataset and the baselines code\nare available in this github page https://github.com/AkashGhosh/M3Retrieve.",
    "code_links": [
      "https://github.com/AkashGhosh/M3Retrieve"
    ],
    "comment": "EMNLP Mains 2025"
  },
  {
    "title": "Are LLMs Reliable Rankers? Rank Manipulation via Two-Stage Token Optimization",
    "authors": "Tiancheng Xing, Jerry Li, Yixuan Du, Xiyang Hu",
    "published": "2025-10-08",
    "arxiv_id": "2510.06732v1",
    "url": "http://arxiv.org/abs/2510.06732v1",
    "pdf_url": "http://arxiv.org/pdf/2510.06732v1",
    "category": "information_retrieval",
    "primary_category": "cs.CL",
    "abstract": "Large language models (LLMs) are increasingly used as rerankers in\ninformation retrieval, yet their ranking behavior can be steered by small,\nnatural-sounding prompts. To expose this vulnerability, we present Rank\nAnything First (RAF), a two-stage token optimization method that crafts concise\ntextual perturbations to consistently promote a target item in LLM-generated\nrankings while remaining hard to detect. Stage 1 uses Greedy Coordinate\nGradient to shortlist candidate tokens at the current position by combining the\ngradient of the rank-target with a readability score; Stage 2 evaluates those\ncandidates under exact ranking and readability losses using an entropy-based\ndynamic weighting scheme, and selects a token via temperature-controlled\nsampling. RAF generates ranking-promoting prompts token-by-token, guided by\ndual objectives: maximizing ranking effectiveness and preserving linguistic\nnaturalness. Experiments across multiple LLMs show that RAF significantly\nboosts the rank of target items using naturalistic language, with greater\nrobustness than existing methods in both promoting target items and maintaining\nnaturalness. These findings underscore a critical security implication:\nLLM-based reranking is inherently susceptible to adversarial manipulation,\nraising new challenges for the trustworthiness and robustness of modern\nretrieval systems. Our code is available at: https://github.com/glad-lab/RAF.",
    "code_links": [
      "https://github.com/glad-lab/RAF"
    ],
    "comment": "10 pages, 3 figures"
  },
  {
    "title": "MARCO: A Cooperative Knowledge Transfer Framework for Personalized Cross-domain Recommendations",
    "authors": "Lili Xie, Yi Zhang, Ruihong Qiu, Jiajun Liu, Sen Wang",
    "published": "2025-10-06",
    "arxiv_id": "2510.04508v1",
    "url": "http://arxiv.org/abs/2510.04508v1",
    "pdf_url": "http://arxiv.org/pdf/2510.04508v1",
    "category": "information_retrieval",
    "primary_category": "cs.IR",
    "abstract": "Recommender systems frequently encounter data sparsity issues, particularly\nwhen addressing cold-start scenarios involving new users or items. Multi-source\ncross-domain recommendation (CDR) addresses these challenges by transferring\nvaluable knowledge from multiple source domains to enhance recommendations in a\ntarget domain. However, existing reinforcement learning (RL)-based CDR methods\ntypically rely on a single-agent framework, leading to negative transfer issues\ncaused by inconsistent domain contributions and inherent distributional\ndiscrepancies among source domains. To overcome these limitations, MARCO, a\nMulti-Agent Reinforcement Learning-based Cross-Domain recommendation framework,\nis proposed. It leverages cooperative multi-agent reinforcement learning, where\neach agent is dedicated to estimating the contribution from an individual\nsource domain, effectively managing credit assignment and mitigating negative\ntransfer. In addition, an entropy-based action diversity penalty is introduced\nto enhance policy expressiveness and stabilize training by encouraging diverse\nagents' joint actions. Extensive experiments across four benchmark datasets\ndemonstrate MARCO's superior performance over state-of-the-art methods,\nhighlighting its robustness and strong generalization capabilities. The code is\nat https://github.com/xiewilliams/MARCO.",
    "code_links": [
      "https://github.com/xiewilliams/MARCO"
    ],
    "comment": "SIGIR-AP 2025"
  },
  {
    "title": "GRACE: Generative Representation Learning via Contrastive Policy Optimization",
    "authors": "Jiashuo Sun, Shixuan Liu, Zhaochen Su, Xianrui Zhong, Pengcheng Jiang, Bowen Jin, Peiran Li, Weijia Shi, Jiawei Han",
    "published": "2025-10-06",
    "arxiv_id": "2510.04506v1",
    "url": "http://arxiv.org/abs/2510.04506v1",
    "pdf_url": "http://arxiv.org/pdf/2510.04506v1",
    "category": "information_retrieval",
    "primary_category": "cs.CL",
    "abstract": "Prevailing methods for training Large Language Models (LLMs) as text encoders\nrely on contrastive losses that treat the model as a black box function,\ndiscarding its generative and reasoning capabilities in favor of static\nembeddings. We introduce GRACE (Generative Representation Learning via\nContrastive Policy Optimization), a novel framework that reimagines contrastive\nsignals not as losses to be minimized, but as rewards that guide a generative\npolicy. In GRACE, the LLM acts as a policy that produces explicit,\nhuman-interpretable rationales--structured natural language explanations of its\nsemantic understanding. These rationales are then encoded into high-quality\nembeddings via mean pooling. Using policy gradient optimization, we train the\nmodel with a multi-component reward function that maximizes similarity between\nquery positive pairs and minimizes similarity with negatives. This transforms\nthe LLM from an opaque encoder into an interpretable agent whose reasoning\nprocess is transparent and inspectable. On MTEB benchmark, GRACE yields broad\ncross category gains: averaged over four backbones, the supervised setting\nimproves overall score by 11.5% over base models, and the unsupervised variant\nadds 6.9%, while preserving general capabilities. This work treats contrastive\nobjectives as rewards over rationales, unifying representation learning with\ngeneration to produce stronger embeddings and transparent rationales. The\nmodel, data and code are available at https://github.com/GasolSun36/GRACE.",
    "code_links": [
      "https://github.com/GasolSun36/GRACE"
    ],
    "comment": "23 pages, 7 figures, 7 tables"
  },
  {
    "title": "Causality-aware Graph Aggregation Weight Estimator for Popularity Debiasing in Top-K Recommendation",
    "authors": "Yue Que, Yingyi Zhang, Xiangyu Zhao, Chen Ma",
    "published": "2025-10-06",
    "arxiv_id": "2510.04502v1",
    "url": "http://arxiv.org/abs/2510.04502v1",
    "pdf_url": "http://arxiv.org/pdf/2510.04502v1",
    "category": "information_retrieval",
    "primary_category": "cs.IR",
    "abstract": "Graph-based recommender systems leverage neighborhood aggregation to generate\nnode representations, which is highly sensitive to popularity bias, resulting\nin an echo effect during information propagation. Existing graph-based\ndebiasing solutions refine the aggregation process with attempts such as edge\nreconstruction or weight adjustment. However, these methods remain inadequate\nin fully alleviating popularity bias. Specifically, this is because 1) they\nprovide no insights into graph aggregation rationality, thus lacking an\noptimality guarantee; 2) they fail to well balance the training and debiasing\nprocess, which undermines the effectiveness. In this paper, we propose a novel\napproach to mitigate popularity bias through rational modeling of the graph\naggregation process. We reveal that graph aggregation is a special form of\nbackdoor adjustment in causal inference, where the aggregation weight\ncorresponds to the historical interaction likelihood distribution. Based on\nthis insight, we devise an encoder-decoder architecture, namely Causality-aware\nGraph Aggregation Weight Estimator for Debiasing (CAGED), to approximate the\nunbiased aggregation weight by optimizing the evidence lower bound of the\ninteraction likelihood. In order to enhance the debiasing effectiveness during\nearly training stages, we further design a momentum update strategy that\nincrementally refines the aggregation weight matrix. Extensive experiments on\nthree datasets demonstrate that CAGED outperforms existing graph-based\ndebiasing methods. Our implementation is available at\nhttps://github.com/QueYork/CAGED.",
    "code_links": [
      "https://github.com/QueYork/CAGED"
    ],
    "comment": "Accepted by CIKM 2025"
  },
  {
    "title": "Which Programming Language and Model Work Best With LLM-as-a-Judge For Code Retrieval?",
    "authors": "Lucas Roberts, Denisa Roberts",
    "published": "2025-09-30",
    "arxiv_id": "2510.00324v1",
    "url": "http://arxiv.org/abs/2510.00324v1",
    "pdf_url": "http://arxiv.org/pdf/2510.00324v1",
    "category": "information_retrieval",
    "primary_category": "cs.SE",
    "abstract": "Code search is an important information retrieval application. Benefits of\nbetter code search include faster new developer on-boarding, reduced software\nmaintenance, and ease of understanding for large repositories. Despite\nimprovements in search algorithms and search benchmarks, the domain of code\nsearch has lagged behind. One reason is the high cost of human annotation for\ncode queries and answers. While humans may annotate search results in general\ntext QA systems, code annotations require specialized knowledge of a\nprogramming language (PL), as well as domain specific software engineering\nknowledge. In this work we study the use of Large Language Models (LLMs) to\nretrieve code at the level of functions and to generate annotations for code\nsearch results. We compare the impact of the retriever representation (sparse\nvs. semantic), programming language, and LLM by comparing human annotations\nacross several popular languages (C, Java, Javascript, Go, and Python). We\nfocus on repositories that implement common data structures likely to be\nimplemented in any PLs. For the same human annotations, we compare several\nLLM-as-a-Judge models to evaluate programming language and other affinities\nbetween LLMs. We find that the chosen retriever and PL exhibit affinities that\ncan be leveraged to improve alignment of human and AI relevance determinations,\nwith significant performance implications. We also find differences in\nrepresentation (sparse vs. semantic) across PLs that impact alignment of human\nand AI relevance determinations. We propose using transpilers to bootstrap\nscalable code search benchmark datasets in other PLs and in a case study\ndemonstrate that human-AI relevance agreement rates largely match the (worst\ncase) human-human agreement under study. The application code used in this work\nis available at \\href{https://github.com/rlucas7/code-searcher/}{this github\nrepo}.",
    "code_links": [
      "https://github.com/rlucas7/code-searcher"
    ],
    "comment": "Accepted as a full paper at SIGIR-AP 2025"
  },
  {
    "title": "MR$^2$-Bench: Going Beyond Matching to Reasoning in Multimodal Retrieval",
    "authors": "Junjie Zhou, Ze Liu, Lei Xiong, Jin-Ge Yao, Yueze Wang, Shitao Xiao, Fenfen Lin, Miguel Hu Chen, Zhicheng Dou, Siqi Bao, Defu Lian, Yongping Xiong, Zheng Liu",
    "published": "2025-09-30",
    "arxiv_id": "2509.26378v1",
    "url": "http://arxiv.org/abs/2509.26378v1",
    "pdf_url": "http://arxiv.org/pdf/2509.26378v1",
    "category": "information_retrieval",
    "primary_category": "cs.IR",
    "abstract": "Multimodal retrieval is becoming a crucial component of modern AI\napplications, yet its evaluation lags behind the demands of more realistic and\nchallenging scenarios. Existing benchmarks primarily probe surface-level\nsemantic correspondence (e.g., object-text matching) while failing to assess\nthe deeper reasoning required to capture complex relationships between visual\nand textual information. To address this gap, we introduce MR$^2$-Bench, a\nreasoning-intensive benchmark for multimodal retrieval. MR$^2$-Bench presents\nthe following critical values: 1) all tasks are reasoning-driven, going beyond\nshallow matching to effectively assess models' capacity for logical, spatial,\nand causal inference; 2) it features diverse multimodal data, such as natural\nimages, diagrams, and visual puzzles, enabling comprehensive evaluation across\ncontent types; 3) it supports complex queries and documents containing multiple\nimages and covers diverse retrieval scenarios, more accurately reflecting\nreal-world applications. Our benchmark contains 1,309 curated queries, derived\neither from manual collection and annotation or from selective consolidation of\npublic datasets. Despite achieving strong results on existing benchmarks,\ncurrent state-of-the-art models still struggle on MR$^2$-Bench: for example,\nthe leading Seed1.6-Embedding model attains a Recall@1 of 77.78 on MMEB, but\nonly 9.91 on MR$^2$-Bench. This substantial performance gap highlights both the\nincreased challenge posed by our benchmark and the pressing need for further\nadvances in reasoning-intensive multimodal retrieval. The dataset and\nevaluation code will be made publicly available at\nhttps://github.com/VectorSpaceLab/MR2-Bench.",
    "code_links": [
      "https://github.com/VectorSpaceLab/MR2-Bench"
    ],
    "comment": null
  },
  {
    "title": "Knowledge Graph-Guided Multi-Agent Distillation for Reliable Industrial Question Answering with Datasets",
    "authors": "Jiqun Pan, Zhenke Duan, Jiani Tu, Anzhi Cheng, Yanqing Wang",
    "published": "2025-10-03",
    "arxiv_id": "2510.06240v1",
    "url": "http://arxiv.org/abs/2510.06240v1",
    "pdf_url": "http://arxiv.org/pdf/2510.06240v1",
    "category": "databases",
    "primary_category": "cs.CL",
    "abstract": "Industrial question-answering (QA) systems require higher safety and\nreliability than general-purpose dialogue models, as errors in high-risk\nscenarios such as equipment fault diagnosis can have severe consequences.\nAlthough multi-agent large language models enhance reasoning depth, they suffer\nfrom uncontrolled iterations and unverifiable outputs, and conventional\ndistillation methods struggle to transfer collaborative reasoning capabilities\nto lightweight, deployable student models. To address these challenges, we\npropose Knowledge Graph-guided Multi-Agent System Distillation (KG-MASD). Our\napproach formulates distillation as a Markov Decision Process and incorporates\na knowledge graph as a verifiable structured prior to enrich state\nrepresentation and ensure convergence. By integrating collaborative reasoning\nwith knowledge grounding, KG-MASD generates high-confidence instruction-tuning\ndata and jointly distills reasoning depth and verifiability into compact\nstudent models suitable for edge deployment. Experiments on an industrial QA\ndataset show that KG-MASD improves accuracy by 2.4 per cent to 20.1 per cent\nover baselines and significantly enhances reliability, enabling trustworthy AI\ndeployment in safety-critical industrial scenarios. Code and data are available\nat https://github.com/erwinmsmith/KG-MAD/.",
    "code_links": [
      "https://github.com/erwinmsmith/KG-MAD"
    ],
    "comment": "41 pages, 12 figures, 6 tables"
  },
  {
    "title": "EMR-AGENT: Automating Cohort and Feature Extraction from EMR Databases",
    "authors": "Kwanhyung Lee, Sungsoo Hong, Joonhyung Park, Jeonghyeop Lim, Juhwan Choi, Donghwee Yoon, Eunho Yang",
    "published": "2025-10-01",
    "arxiv_id": "2510.00549v2",
    "url": "http://arxiv.org/abs/2510.00549v2",
    "pdf_url": "http://arxiv.org/pdf/2510.00549v2",
    "category": "databases",
    "primary_category": "cs.DB",
    "abstract": "Machine learning models for clinical prediction rely on structured data\nextracted from Electronic Medical Records (EMRs), yet this process remains\ndominated by hardcoded, database-specific pipelines for cohort definition,\nfeature selection, and code mapping. These manual efforts limit scalability,\nreproducibility, and cross-institutional generalization. To address this, we\nintroduce EMR-AGENT (Automated Generalized Extraction and Navigation Tool), an\nagent-based framework that replaces manual rule writing with dynamic, language\nmodel-driven interaction to extract and standardize structured clinical data.\nOur framework automates cohort selection, feature extraction, and code mapping\nthrough interactive querying of databases. Our modular agents iteratively\nobserve query results and reason over schema and documentation, using SQL not\njust for data retrieval but also as a tool for database observation and\ndecision making. This eliminates the need for hand-crafted, schema-specific\nlogic. To enable rigorous evaluation, we develop a benchmarking codebase for\nthree EMR databases (MIMIC-III, eICU, SICdb), including both seen and unseen\nschema settings. Our results demonstrate strong performance and generalization\nacross these databases, highlighting the feasibility of automating a process\npreviously thought to require expert-driven design. The code will be released\npublicly at https://github.com/AITRICS/EMR-AGENT/tree/main. For a\ndemonstration, please visit our anonymous demo page:\nhttps://anonymoususer-max600.github.io/EMR_AGENT/",
    "code_links": [
      "https://github.com/AITRICS/EMR-AGENT"
    ],
    "comment": "currently under submission to ICLR 2026"
  },
  {
    "title": "Graph2Region: Efficient Graph Similarity Learning with Structure and Scale Restoration",
    "authors": "Zhouyang Liu, Yixin Chen, Ning Liu, Jiezhong He, Dongsheng Li",
    "published": "2025-10-01",
    "arxiv_id": "2510.00394v1",
    "url": "http://arxiv.org/abs/2510.00394v1",
    "pdf_url": "http://arxiv.org/pdf/2510.00394v1",
    "category": "databases",
    "primary_category": "cs.LG",
    "abstract": "Graph similarity is critical in graph-related tasks such as graph retrieval,\nwhere metrics like maximum common subgraph (MCS) and graph edit distance (GED)\nare commonly used. However, exact computations of these metrics are known to be\nNP-Hard. Recent neural network-based approaches approximate the similarity\nscore in embedding spaces to alleviate the computational burden, but they\neither involve expensive pairwise node comparisons or fail to effectively\nutilize structural and scale information of graphs. To tackle these issues, we\npropose a novel geometric-based graph embedding method called Graph2Region\n(G2R). G2R represents nodes as closed regions and recovers their adjacency\npatterns within graphs in the embedding space. By incorporating the node\nfeatures and adjacency patterns of graphs, G2R summarizes graph regions, i.e.,\ngraph embeddings, where the shape captures the underlying graph structures and\nthe volume reflects the graph size. Consequently, the overlap between graph\nregions can serve as an approximation of MCS, signifying similar node regions\nand adjacency patterns. We further analyze the relationship between MCS and GED\nand propose using disjoint parts as a proxy for GED similarity. This analysis\nenables concurrent computation of MCS and GED, incorporating local and global\nstructural information. Experimental evaluation highlights G2R's competitive\nperformance in graph similarity computation. It achieves up to a 60.0\\%\nrelative accuracy improvement over state-of-the-art methods in MCS similarity\nlearning, while maintaining efficiency in both training and inference.\nMoreover, G2R showcases remarkable capability in predicting both MCS and GED\nsimilarities simultaneously, providing a holistic assessment of graph\nsimilarity. Code available at https://github.com/liuzhouyang/Graph2Region.",
    "code_links": [
      "https://github.com/liuzhouyang/Graph2Region"
    ],
    "comment": "Accepted by IEEE Transactions on Knowledge and Data Engineering"
  },
  {
    "title": "ActorDB: A Unified Database Model Integrating Single-Writer Actors, Incremental View Maintenance, and Zero-Trust Messaging",
    "authors": "Jun Kawasaki",
    "published": "2025-09-29",
    "arxiv_id": "2509.25285v1",
    "url": "http://arxiv.org/abs/2509.25285v1",
    "pdf_url": "http://arxiv.org/pdf/2509.25285v1",
    "category": "databases",
    "primary_category": "cs.DB",
    "abstract": "This paper presents ActorDB ( Dekigoto ) , a novel database architecture that\ntightly integrates a single-writer actor model for writes, Incremental View\nMaintenance (IVM), and a zero-trust security model as a core component. The\nprimary contribution of this work is the unification of these powerful but\ncomplex concepts into a single, cohesive system designed to reduce\narchitectural complexity for developers of modern, data-intensive applications.\nWe argue that by providing these capabilities out-of-the-box, ActorDB can offer\na more robust, secure, and developer-friendly platform compared to solutions\nthat require manual integration of separate systems for actor persistence,\nstream processing, and security. We present the core architecture, discuss the\ncritical trade-offs in its design, and define the performance criteria for a\nMinimum Viable Product (MVP) to validate our approach.",
    "code_links": [
      "https://github.com/com-junkawasaki/dekigoto"
    ],
    "comment": "7 pages, 1 table, 1 figures. Code and data available at\n  https://github.com/com-junkawasaki/dekigoto"
  },
  {
    "title": "Multilingual Text-to-SQL: Benchmarking the Limits of Language Models with Collaborative Language Agents",
    "authors": "Khanh Trinh Pham, Thu Huong Nguyen, Jun Jo, Quoc Viet Hung Nguyen, Thanh Tam Nguyen",
    "published": "2025-09-29",
    "arxiv_id": "2509.24405v1",
    "url": "http://arxiv.org/abs/2509.24405v1",
    "pdf_url": "http://arxiv.org/pdf/2509.24405v1",
    "category": "databases",
    "primary_category": "cs.CL",
    "abstract": "Text-to-SQL enables natural access to databases, yet most benchmarks are\nEnglish-only, limiting multilingual progress. We introduce MultiSpider 2.0,\nextending Spider 2.0 to eight languages (English, German, French, Spanish,\nPortuguese, Japanese, Chinese, Vietnamese). It preserves Spider 2.0's\nstructural difficulty while adding linguistic and dialectal variability,\ndemanding deeper reasoning for complex SQL. On this benchmark, state-of-the-art\nLLMs (such as DeepSeek-R1 and OpenAI o1) reach only 4\\% execution accuracy when\nrelying on intrinsic reasoning, versus 60\\% on MultiSpider 1.0. Therefore, we\nprovide a collaboration-driven language agents baseline that iteratively\nrefines queries, improving accuracy to 15\\%. These results reveal a substantial\nmultilingual gap and motivate methods that are robust across languages and\nready for real-world enterprise deployment. Our benchmark is available at\nhttps://github.com/phkhanhtrinh23/Multilingual_Text_to_SQL.",
    "code_links": [
      "https://github.com/phkhanhtrinh23/Multilingual_Text_to_SQL"
    ],
    "comment": null
  },
  {
    "title": "PARROT: A Benchmark for Evaluating LLMs in Cross-System SQL Translation",
    "authors": "Wei Zhou, Guoliang Li, Haoyu Wang, Yuxing Han, Xufei Wu, Fan Wu, Xuanhe Zhou",
    "published": "2025-09-27",
    "arxiv_id": "2509.23338v1",
    "url": "http://arxiv.org/abs/2509.23338v1",
    "pdf_url": "http://arxiv.org/pdf/2509.23338v1",
    "category": "databases",
    "primary_category": "cs.DB",
    "abstract": "Large language models (LLMS) have shown increasing effectiveness in\nText-to-SQL tasks. However, another closely related problem, Cross-System SQL\nTranslation (a.k.a., SQL-to-SQL), which adapts a query written for one database\nsystem (e.g., MySQL) into its equivalent one for another system (e.g.,\nClickHouse), is of great practical importance but remains underexplored.\nExisting SQL benchmarks are not well-suited for SQL-to-SQL evaluation, which\n(1) focus on a limited set of database systems (often just SQLite) and (2)\ncannot capture many system-specific SQL dialects (e.g., customized functions,\ndata types, and syntax rules). Thus, in this paper, we introduce PARROT, a\nPractical And Realistic BenchmaRk for CrOss-System SQL Translation. PARROT\ncomprises 598 translation pairs from 38 open-source benchmarks and real-world\nbusiness services, specifically prepared to challenge system-specific SQL\nunderstanding (e.g., LLMS achieve lower than 38.53% accuracy on average). We\nalso provide multiple benchmark variants, including PARROT-Diverse with 28,003\ntranslations (for extensive syntax testing) and PARROT-Simple with 5,306\nrepresentative samples (for focused stress testing), covering 22\nproduction-grade database systems. To promote future research, we release a\npublic leaderboard and source code at: https://code4db.github.io/parrot-bench/.",
    "code_links": [
      "https://github.com/weAIDB/PARROT"
    ],
    "comment": "To appear in NeurIPS 2025. Welcome your submission to challenge our\n  leaderboard at: https://code4db.github.io/parrot-bench/. Also visit our code\n  repository at: https://github.com/weAIDB/PARROT"
  },
  {
    "title": "AutoPK: Leveraging LLMs and a Hybrid Similarity Metric for Advanced Retrieval of Pharmacokinetic Data from Complex Tables and Documents",
    "authors": "Hossein Sholehrasa, Amirhossein Ghanaatian, Doina Caragea, Lisa A. Tell, Jim E. Riviere, Majid Jaberi-Douraki",
    "published": "2025-09-26",
    "arxiv_id": "2510.00039v1",
    "url": "http://arxiv.org/abs/2510.00039v1",
    "pdf_url": "http://arxiv.org/pdf/2510.00039v1",
    "category": "databases",
    "primary_category": "cs.DB",
    "abstract": "Pharmacokinetics (PK) plays a critical role in drug development and\nregulatory decision-making for human and veterinary medicine, directly\naffecting public health through drug safety and efficacy assessments. However,\nPK data are often embedded in complex, heterogeneous tables with variable\nstructures and inconsistent terminologies, posing significant challenges for\nautomated PK data retrieval and standardization. AutoPK, a novel two-stage\nframework for accurate and scalable extraction of PK data from complex\nscientific tables. In the first stage, AutoPK identifies and extracts PK\nparameter variants using large language models (LLMs), a hybrid similarity\nmetric, and LLM-based validation. The second stage filters relevant rows,\nconverts the table into a key-value text format, and uses an LLM to reconstruct\na standardized table. Evaluated on a real-world dataset of 605 PK tables,\nincluding captions and footnotes, AutoPK shows significant improvements in\nprecision and recall over direct LLM baselines. For instance, AutoPK with LLaMA\n3.1-70B achieved an F1-score of 0.92 on half-life and 0.91 on clearance\nparameters, outperforming direct use of LLaMA 3.1-70B by margins of 0.10 and\n0.21, respectively. Smaller models such as Gemma 3-27B and Phi 3-12B with\nAutoPK achieved 2-7 fold F1 gains over their direct use, with Gemma's\nhallucination rates reduced from 60-95% down to 8-14%. Notably, AutoPK enabled\nopen-source models like Gemma 3-27B to outperform commercial systems such as\nGPT-4o Mini on several PK parameters. AutoPK enables scalable and\nhigh-confidence PK data extraction, making it well-suited for critical\napplications in veterinary pharmacology, drug safety monitoring, and public\nhealth decision-making, while addressing heterogeneous table structures and\nterminology and demonstrating generalizability across key PK parameters. Code\nand data: https://github.com/hosseinsholehrasa/AutoPK",
    "code_links": [
      "https://github.com/hosseinsholehrasa/AutoPK"
    ],
    "comment": "Accepted at the 2025 IEEE 37th ICTAI"
  },
  {
    "title": "Play by the Type Rules: Inferring Constraints for LLM Functions in Declarative Programs",
    "authors": "Parker Glenn, Alfy Samuel, Daben Liu",
    "published": "2025-09-24",
    "arxiv_id": "2509.20208v1",
    "url": "http://arxiv.org/abs/2509.20208v1",
    "pdf_url": "http://arxiv.org/pdf/2509.20208v1",
    "category": "databases",
    "primary_category": "cs.CL",
    "abstract": "Integrating LLM powered operators in declarative query languages allows for\nthe combination of cheap and interpretable functions with powerful,\ngeneralizable language model reasoning. However, in order to benefit from the\noptimized execution of a database query language like SQL, generated outputs\nmust align with the rules enforced by both type checkers and database contents.\nCurrent approaches address this challenge with orchestrations consisting of\nmany LLM-based post-processing calls to ensure alignment between generated\noutputs and database values, introducing performance bottlenecks. We perform a\nstudy on the ability of various sized open-source language models to both parse\nand execute functions within a query language based on SQL, showing that small\nlanguage models can excel as function executors over hybrid data sources. Then,\nwe propose an efficient solution to enforce the well-typedness of LLM\nfunctions, demonstrating 7% accuracy improvement on a multi-hop question\nanswering dataset with 53% improvement in latency over comparable solutions. We\nmake our implementation available at https://github.com/parkervg/blendsql",
    "code_links": [
      "https://github.com/parkervg/blendsql"
    ],
    "comment": null
  },
  {
    "title": "ORQ: Complex Analytics on Private Data with Strong Security Guarantees",
    "authors": "Eli Baum, Sam Buxbaum, Nitin Mathai, Muhammad Faisal, Vasiliki Kalavri, Mayank Varia, John Liagouris",
    "published": "2025-09-13",
    "arxiv_id": "2509.10793v1",
    "url": "http://arxiv.org/abs/2509.10793v1",
    "pdf_url": "http://arxiv.org/pdf/2509.10793v1",
    "category": "databases",
    "primary_category": "cs.CR",
    "abstract": "We present ORQ, a system that enables collaborative analysis of large private\ndatasets using cryptographically secure multi-party computation (MPC). ORQ\nprotects data against semi-honest or malicious parties and can efficiently\nevaluate relational queries with multi-way joins and aggregations that have\nbeen considered notoriously expensive under MPC. To do so, ORQ eliminates the\nquadratic cost of secure joins by leveraging the fact that, in practice, the\nstructure of many real queries allows us to join records and apply the\naggregations \"on the fly\" while keeping the result size bounded. On the system\nside, ORQ contributes generic oblivious operators, a data-parallel vectorized\nquery engine, a communication layer that amortizes MPC network costs, and a\ndataflow API for expressing relational analytics -- all built from the ground\nup.\n  We evaluate ORQ in LAN and WAN deployments on a diverse set of workloads,\nincluding complex queries with multiple joins and custom aggregations. When\ncompared to state-of-the-art solutions, ORQ significantly reduces MPC execution\ntimes and can process one order of magnitude larger datasets. For our most\nchallenging workload, the full TPC-H benchmark, we report results entirely\nunder MPC with Scale Factor 10 -- a scale that had previously been achieved\nonly with information leakage or the use of trusted third parties.",
    "code_links": [
      "https://github.com/CASP-Systems-BU/orq"
    ],
    "comment": "14 pages, plus Appendix. To appear at SOSP 2025. Code published at\n  https://github.com/CASP-Systems-BU/orq"
  },
  {
    "title": "A Comparative Analysis of Identifier Schemes: UUIDv4, UUIDv7, and ULID for Distributed Systems",
    "authors": "Nima Karimian Kakolaki",
    "published": "2025-09-10",
    "arxiv_id": "2509.08969v1",
    "url": "http://arxiv.org/abs/2509.08969v1",
    "pdf_url": "http://arxiv.org/pdf/2509.08969v1",
    "category": "databases",
    "primary_category": "cs.DC",
    "abstract": "Distributed systems require robust, scalable identifier schemes to ensure\ndata uniqueness and efficient indexing across multiple nodes. This paper\npresents a comprehensive analysis of the evolution of distributed identifiers,\ncomparing traditional auto-increment keys with UUIDv4, UUIDv7, and ULIDs. We\ncombine mathematical calculation of collision probabilities with empirical\nexperiments measuring generation speed and network transmission overhead in a\nsimulated distributed environment. Results demonstrate that ULIDs significantly\noutperform UUIDv4 and UUIDv7, reducing network overhead by 83.7% and increasing\ngeneration speed by 97.32%. statistical analysis further shows ULIDs offer a\n98.42% lower collision risk compared to UUIDv7, while maintaining negligible\ncollision probabilities even at high generation rates. These findings highlight\nULIDs as an optimal choice for high-performance distributed systems, providing\nefficient, time-ordered, and lexicographically sortable identifiers suitable\nfor scalable applications. All source code, datasets, and analysis scripts\nutilized in this research are publicly available in our dedicated repository at\nhttps://github.com/nimakarimiank/uids-comparison. This repository contains\ncomprehensive documentation of the experimental setup, including configuration\nfiles for the distributed environment, producer and consumer implementations,\nand message broker integration. Additionally, it provides the data scripts and\ndatasets. Researchers and practitioners are encouraged to explore the\nrepository for full reproducibility of the experiments and to facilitate\nfurther investigation or extension of the presented work.",
    "code_links": [
      "https://github.com/nimakarimiank/uids-comparison"
    ],
    "comment": null
  }
]