title,authors,published,arxiv_id,url,pdf_url,category,primary_category,abstract,code_links,comment
Delta Activations: A Representation for Finetuned Large Language Models,"Zhiqiu Xu, Amish Sethi, Mayur Naik, Ser-Nam Lim",2025-09-04,2509.04442v1,http://arxiv.org/abs/2509.04442v1,http://arxiv.org/pdf/2509.04442v1,information_retrieval,cs.LG,"The success of powerful open source Large Language Models (LLMs) has enabled
the community to create a vast collection of post-trained models adapted to
specific tasks and domains. However, navigating and understanding these models
remains challenging due to inconsistent metadata and unstructured repositories.
We introduce Delta Activations, a method to represent finetuned models as
vector embeddings by measuring shifts in their internal activations relative to
a base model. This representation allows for effective clustering by domain and
task, revealing structure in the model landscape. Delta Activations also
demonstrate desirable properties: it is robust across finetuning settings and
exhibits an additive property when finetuning datasets are mixed. In addition,
we show that Delta Activations can embed tasks via few-shot finetuning, and
further explore its use for model selection and merging. We hope Delta
Activations can facilitate the practice of reusing publicly available models.
Code is available at https://github.com/OscarXZQ/delta_activations.",https://github.com/OscarXZQ/delta_activations,
NER Retriever: Zero-Shot Named Entity Retrieval with Type-Aware Embeddings,"Or Shachar, Uri Katz, Yoav Goldberg, Oren Glickman",2025-09-04,2509.04011v1,http://arxiv.org/abs/2509.04011v1,http://arxiv.org/pdf/2509.04011v1,information_retrieval,cs.IR,"We present NER Retriever, a zero-shot retrieval framework for ad-hoc Named
Entity Retrieval, a variant of Named Entity Recognition (NER), where the types
of interest are not provided in advance, and a user-defined type description is
used to retrieve documents mentioning entities of that type. Instead of relying
on fixed schemas or fine-tuned models, our method builds on internal
representations of large language models (LLMs) to embed both entity mentions
and user-provided open-ended type descriptions into a shared semantic space. We
show that internal representations, specifically the value vectors from
mid-layer transformer blocks, encode fine-grained type information more
effectively than commonly used top-layer embeddings. To refine these
representations, we train a lightweight contrastive projection network that
aligns type-compatible entities while separating unrelated types. The resulting
entity embeddings are compact, type-aware, and well-suited for nearest-neighbor
search. Evaluated on three benchmarks, NER Retriever significantly outperforms
both lexical and dense sentence-level retrieval baselines. Our findings provide
empirical support for representation selection within LLMs and demonstrate a
practical solution for scalable, schema-free entity retrieval. The NER
Retriever Codebase is publicly available at
https://github.com/ShacharOr100/ner_retriever",https://github.com/ShacharOr100/ner_retriever,Findings of EMNLP 2025
Evaluating the Robustness of Retrieval-Augmented Generation to Adversarial Evidence in the Health Domain,"Shakiba Amirshahi, Amin Bigdeli, Charles L. A. Clarke, Amira Ghenai",2025-09-04,2509.03787v1,http://arxiv.org/abs/2509.03787v1,http://arxiv.org/pdf/2509.03787v1,information_retrieval,cs.IR,"Retrieval augmented generation (RAG) systems provide a method for factually
grounding the responses of a Large Language Model (LLM) by providing retrieved
evidence, or context, as support. Guided by this context, RAG systems can
reduce hallucinations and expand the ability of LLMs to accurately answer
questions outside the scope of their training data. Unfortunately, this design
introduces a critical vulnerability: LLMs may absorb and reproduce
misinformation present in retrieved evidence. This problem is magnified if
retrieved evidence contains adversarial material explicitly intended to
promulgate misinformation. This paper presents a systematic evaluation of RAG
robustness in the health domain and examines alignment between model outputs
and ground-truth answers. We focus on the health domain due to the potential
for harm caused by incorrect responses, as well as the availability of
evidence-based ground truth for many common health-related questions. We
conduct controlled experiments using common health questions, varying both the
type and composition of the retrieved documents (helpful, harmful, and
adversarial) as well as the framing of the question by the user (consistent,
neutral, and inconsistent). Our findings reveal that adversarial documents
substantially degrade alignment, but robustness can be preserved when helpful
evidence is also present in the retrieval pool. These findings offer actionable
insights for designing safer RAG systems in high-stakes domains by highlighting
the need for retrieval safeguards. To enable reproducibility and facilitate
future research, all experimental results are publicly available in our github
repository.
  https://github.com/shakibaam/RAG_ROBUSTNESS_EVAL",https://github.com/shakibaam/RAG_ROBUSTNESS_EVAL,
Upcycling Candidate Tokens of Large Language Models for Query Expansion,"Jinseok Kim, Sukmin Cho, Soyeong Jeong, Sangyeop Kim, Sungzoon Cho",2025-09-02,2509.02377v1,http://arxiv.org/abs/2509.02377v1,http://arxiv.org/pdf/2509.02377v1,information_retrieval,cs.IR,"Query Expansion (QE) improves retrieval performance by enriching queries with
related terms. Recently, Large Language Models (LLMs) have been used for QE,
but existing methods face a trade-off: generating diverse terms boosts
performance but increases computational cost. To address this challenge, we
propose Candidate Token Query Expansion (CTQE), which extracts diverse and
relevant terms from a single LLM decoding pass by leveraging unselected
candidate tokens. These tokens, though not part of the final output, are
conditioned on the full query and capture useful information. By aggregating
them, CTQE achieves both relevance and diversity without extra inference,
reducing overhead and latency. Experiments show that CTQE delivers strong
retrieval performance with significantly lower cost, outperforming or
comparable to more expensive methods. Code is available at:
https://github.com/bluejeans8/CTQE",https://github.com/bluejeans8/CTQE,CIKM 2025
Abex-rat: Synergizing Abstractive Augmentation and Adversarial Training for Classification of Occupational Accident Reports,"Jian Chen, Jiabao Dou, Jinbao Tian, Yunqi Xu, Zhou Li",2025-09-02,2509.02072v2,http://arxiv.org/abs/2509.02072v2,http://arxiv.org/pdf/2509.02072v2,information_retrieval,cs.LG,"The automatic classification of occupational accident reports is a critical
research area for enhancing workplace safety and enabling large-scale risk
analysis. However, the severe class imbalance inherent in these real-world
datasets often compromises the performance of analytical models, particularly
for rare but severe incident types, hindering the development of reliable
automated systems. To address this challenge, we propose ABEX-RAT, a novel and
efficient framework that synergizes generative data augmentation with robust
adversarial training. Our approach first employs a twostep
abstractive-expansive (ABEX) pipeline, which leverages a large language model
to distill core incident semantics and then uses a generative model to create
diverse, highquality synthetic samples for underrepresented classes.
Subsequently, a lightweight classifier is trained on the augmented data using a
computationally efficient random adversarial training (RAT) protocol, which
stochastically applies perturbations to enhance model generalization and
robustness without significant overhead. Experimental results on the public
OSHA dataset demonstrate that our method achieves new state-of-the-art
performance, reaching a macro-F1 score of 90.32% and significantly
outperforming previous SOTA and fine-tuned large model baselines. Our work
validates that this synergistic strategy is a highly effective and efficient
alternative to brute-force fine-tuning for specialized, imbalanced
classification tasks. The code is publicly available
at:https://github.com/nxcc-lab/ABEX-RAT.",https://github.com/nxcc-lab/ABEX-RAT,
Empowering Large Language Model for Sequential Recommendation via Multimodal Embeddings and Semantic IDs,"Yuhao Wang, Junwei Pan, Xinhang Li, Maolin Wang, Yuan Wang, Yue Liu, Dapeng Liu, Jie Jiang, Xiangyu Zhao",2025-09-02,2509.02017v1,http://arxiv.org/abs/2509.02017v1,http://arxiv.org/pdf/2509.02017v1,information_retrieval,cs.IR,"Sequential recommendation (SR) aims to capture users' dynamic interests and
sequential patterns based on their historical interactions. Recently, the
powerful capabilities of large language models (LLMs) have driven their
adoption in SR. However, we identify two critical challenges in existing
LLM-based SR methods: 1) embedding collapse when incorporating pre-trained
collaborative embeddings and 2) catastrophic forgetting of quantized embeddings
when utilizing semantic IDs. These issues dampen the model scalability and lead
to suboptimal recommendation performance. Therefore, based on LLMs like
Llama3-8B-instruct, we introduce a novel SR framework named MME-SID, which
integrates multimodal embeddings and quantized embeddings to mitigate embedding
collapse. Additionally, we propose a Multimodal Residual Quantized Variational
Autoencoder (MM-RQ-VAE) with maximum mean discrepancy as the reconstruction
loss and contrastive learning for alignment, which effectively preserve
intra-modal distance information and capture inter-modal correlations,
respectively. To further alleviate catastrophic forgetting, we initialize the
model with the trained multimodal code embeddings. Finally, we fine-tune the
LLM efficiently using LoRA in a multimodal frequency-aware fusion manner.
Extensive experiments on three public datasets validate the superior
performance of MME-SID thanks to its capability to mitigate embedding collapse
and catastrophic forgetting. The implementation code and datasets are publicly
available for reproduction:
https://github.com/Applied-Machine-Learning-Lab/MME-SID.",https://github.com/Applied-Machine-Learning-Lab/MME-SID,CIKM 2025 Full Research Paper
MARS: Modality-Aligned Retrieval for Sequence Augmented CTR Prediction,"Yutian Xiao, Shukuan Wang, Binhao Wang, Zhao Zhang, Yanze Zhang, Shanqi Liu, Chao Feng, Xiang Li, Fuzhen Zhuang",2025-09-01,2509.01184v1,http://arxiv.org/abs/2509.01184v1,http://arxiv.org/pdf/2509.01184v1,information_retrieval,cs.IR,"Click-through rate (CTR) prediction serves as a cornerstone of recommender
systems. Despite the strong performance of current CTR models based on user
behavior modeling, they are still severely limited by interaction sparsity,
especially in low-active user scenarios. To address this issue, data
augmentation of user behavior is a promising research direction. However,
existing data augmentation methods heavily rely on collaborative signals while
overlooking the rich multimodal features of items, leading to insufficient
modeling of low-active users.
  To alleviate this problem, we propose a novel framework \textbf{MARS}
(\textbf{M}odality-\textbf{A}ligned \textbf{R}etrieval for \textbf{S}equence
Augmented CTR Prediction). MARS utilizes a Stein kernel-based approach to align
text and image features into a unified and unbiased semantic space to construct
multimodal user embeddings. Subsequently, each low-active user's behavior
sequence is augmented by retrieving, filtering, and concentrating the most
similar behavior sequence of high-active users via multimodal user embeddings.
Validated by extensive offline experiments and online A/B tests, our framework
MARS consistently outperforms state-of-the-art baselines and achieves
substantial growth on core business metrics within
Kuaishou~\footnote{https://www.kuaishou.com/}. Consequently, MARS has been
successfully deployed, serving the main traffic for hundreds of millions of
users. To ensure reproducibility, we provide anonymous access to the
implementation code~\footnote{https://github.com/wangshukuan/MARS}.",https://github.com/wangshukuan/MARS,
BALM-TSF: Balanced Multimodal Alignment for LLM-Based Time Series Forecasting,"Shiqiao Zhou, Holger Schöner, Huanbo Lyu, Edouard Fouché, Shuo Wang",2025-08-30,2509.00622v1,http://arxiv.org/abs/2509.00622v1,http://arxiv.org/pdf/2509.00622v1,information_retrieval,cs.AI,"Time series forecasting is a long-standing and highly challenging research
topic. Recently, driven by the rise of large language models (LLMs), research
has increasingly shifted from purely time series methods toward harnessing
textual modalities to enhance forecasting performance. However, the vast
discrepancy between text and temporal data often leads current multimodal
architectures to over-emphasise one modality while neglecting the other,
resulting in information loss that harms forecasting performance. To address
this modality imbalance, we introduce BALM-TSF (Balanced Multimodal Alignment
for LLM-Based Time Series Forecasting), a lightweight time series forecasting
framework that maintains balance between the two modalities. Specifically, raw
time series are processed by the time series encoder, while descriptive
statistics of raw time series are fed to an LLM with learnable prompt,
producing compact textual embeddings. To ensure balanced cross-modal context
alignment of time series and textual embeddings, a simple yet effective scaling
strategy combined with a contrastive objective then maps these textual
embeddings into the latent space of the time series embeddings. Finally, the
aligned textual semantic embeddings and time series embeddings are together
integrated for forecasting. Extensive experiments on standard benchmarks show
that, with minimal trainable parameters, BALM-TSF achieves state-of-the-art
performance in both long-term and few-shot forecasting, confirming its ability
to harness complementary information from text and time series. Code is
available at https://github.com/ShiqiaoZhou/BALM-TSF.",https://github.com/ShiqiaoZhou/BALM-TSF,
How to Make Museums More Interactive? Case Study of Artistic Chatbot,"Filip J. Kucia, Bartosz Grabek, Szymon D. Trochimiak, Anna Wróblewska",2025-08-30,2509.00572v1,http://arxiv.org/abs/2509.00572v1,http://arxiv.org/pdf/2509.00572v1,information_retrieval,cs.HC,"Conversational agents powered by Large Language Models (LLMs) are
increasingly utilized in educational settings, in particular in individual
closed digital environments, yet their potential adoption in the physical
learning environments like cultural heritage sites, museums, and art galleries
remains relatively unexplored. In this study, we present Artistic Chatbot, a
voice-to-voice RAG-powered chat system to support informal learning and enhance
visitor engagement during a live art exhibition celebrating the 15th
anniversary of the Faculty of Media Art at the Warsaw Academy of Fine Arts,
Poland. The question answering (QA) chatbot responded to free-form spoken
questions in Polish using the context retrieved from a curated, domain-specific
knowledge base consisting of 226 documents provided by the organizers,
including faculty information, art magazines, books, and journals. We describe
the key aspects of the system architecture and user interaction design, as well
as discuss the practical challenges associated with deploying chatbots at
public cultural sites. Our findings, based on interaction analysis, demonstrate
that chatbots such as Artistic Chatbot effectively maintain responses grounded
in exhibition content (60\% of responses directly relevant), even when faced
with unpredictable queries outside the target domain, showing their potential
for increasing interactivity in public cultural sites.
  GitHub project page: https://github.com/cinekucia/artistic-chatbot-cikm2025",https://github.com/cinekucia/artistic-chatbot-cikm2025,"7 pages, 3 figures"
CRouting: Reducing Expensive Distance Calls in Graph-Based Approximate Nearest Neighbor Search,"Zhenxin Li, Shuibing He, Jiahao Guo, Xuechen Zhang, Xian-He Sun, Gang Chen",2025-08-30,2509.00365v1,http://arxiv.org/abs/2509.00365v1,http://arxiv.org/pdf/2509.00365v1,information_retrieval,cs.DB,"Approximate nearest neighbor search (ANNS) is a crucial problem in
information retrieval and AI applications. Recently, there has been a surge of
interest in graph-based ANNS algorithms due to their superior efficiency and
accuracy. However, the repeated computation of distances in high-dimensional
spaces constitutes the primary time cost of graph-based methods. To accelerate
the search, we propose a novel routing strategy named CRouting, which bypasses
unnecessary distance computations by exploiting the angle distributions of
high-dimensional vectors. CRouting is designed as a plugin to optimize existing
graph-based search with minimal code modifications. Our experiments show that
CRouting reduces the number of distance computations by up to 41.5% and boosts
queries per second by up to 1.48$\times$ on two predominant graph indexes, HNSW
and NSG. Code is publicly available at https://github.com/ISCS-ZJU/CRouting.",https://github.com/ISCS-ZJU/CRouting,
NewsReX: A More Efficient Approach to News Recommendation with Keras 3 and JAX,"Igor L. R. Azevedo, Toyotaro Suzumura, Yuichiro Yasui",2025-08-29,2508.21572v1,http://arxiv.org/abs/2508.21572v1,http://arxiv.org/pdf/2508.21572v1,information_retrieval,cs.IR,"Reproducing and comparing results in news recommendation research has become
increasingly difficult. This is due to a fragmented ecosystem of diverse
codebases, varied configurations, and mainly due to resource-intensive models.
We introduce NewsReX, an open-source library designed to streamline this
process. Our key contribution is a modern implementation built on Keras 3 and
JAX, which provides an increase in computational efficiency. Experiments show
that NewsReX is faster than current implementations. To support broader
research, we provide a straightforward guide and scripts for training models on
custom datasets. We validated this functionality using a proprietary Japanese
news dataset from Nikkei News, a leading Japanese media corporation renowned
for its comprehensive coverage of business, economic, and financial news.
NewsReX makes reproducing complex experiments faster and more accessible to a
wider range of hardware making sure the speed up it also achieved for less
powerful GPUs, like an 8GB RTX 3060 Ti. Beyond the library, this paper offers
an analysis of key training parameters often overlooked in the literature,
including the effect of different negative sampling strategies, the varying
number of epochs, the impact of random batching, and more. This supplementary
analysis serves as a valuable reference for future research, aiming to reduce
redundant computation when comparing baselines and guide best practices. Code
available at https://github.com/igor17400/NewsReX.",https://github.com/igor17400/NewsReX,
Diffusion-based Multi-modal Synergy Interest Network for Click-through Rate Prediction,"Xiaoxi Cui, Weihai Lu, Yu Tong, Yiheng Li, Zhejun Zhao",2025-08-29,2508.21460v1,http://arxiv.org/abs/2508.21460v1,http://arxiv.org/pdf/2508.21460v1,information_retrieval,cs.IR,"In click-through rate prediction, click-through rate prediction is used to
model users' interests. However, most of the existing CTR prediction methods
are mainly based on the ID modality. As a result, they are unable to
comprehensively model users' multi-modal preferences. Therefore, it is
necessary to introduce multi-modal CTR prediction. Although it seems appealing
to directly apply the existing multi-modal fusion methods to click-through rate
prediction models, these methods (1) fail to effectively disentangle
commonalities and specificities across different modalities; (2) fail to
consider the synergistic effects between modalities and model the complex
interactions between modalities.
  To address the above issues, this paper proposes the Diffusion-based
Multi-modal Synergy Interest Network (Diff-MSIN) framework for click-through
prediction. This framework introduces three innovative modules: the Multi-modal
Feature Enhancement (MFE) Module Synergistic Relationship Capture (SRC) Module,
and the Feature Dynamic Adaptive Fusion (FDAF) Module. The MFE Module and SRC
Module extract synergistic, common, and special information among different
modalities. They effectively enhances the representation of the modalities,
improving the overall quality of the fusion. To encourage distinctiveness among
different features, we design a Knowledge Decoupling method. Additionally, the
FDAF Module focuses on capturing user preferences and reducing fusion noise. To
validate the effectiveness of the Diff-MSIN framework, we conducted extensive
experiments using the Rec-Tmall and three Amazon datasets. The results
demonstrate that our approach yields a significant improvement of at least
1.67% compared to the baseline, highlighting its potential for enhancing
multi-modal recommendation systems. Our code is available at the following
link: https://github.com/Cxx-0/Diff-MSIN.",https://github.com/Cxx-0/Diff-MSIN,SIGIR 2025
Stairway to Fairness: Connecting Group and Individual Fairness,"Theresia Veronika Rampisela, Maria Maistro, Tuukka Ruotsalo, Falk Scholer, Christina Lioma",2025-08-29,2508.21334v1,http://arxiv.org/abs/2508.21334v1,http://arxiv.org/pdf/2508.21334v1,information_retrieval,cs.IR,"Fairness in recommender systems (RSs) is commonly categorised into group
fairness and individual fairness. However, there is no established scientific
understanding of the relationship between the two fairness types, as prior work
on both types has used different evaluation measures or evaluation objectives
for each fairness type, thereby not allowing for a proper comparison of the
two. As a result, it is currently not known how increasing one type of fairness
may affect the other. To fill this gap, we study the relationship of group and
individual fairness through a comprehensive comparison of evaluation measures
that can be used for both fairness types. Our experiments with 8 runs across 3
datasets show that recommendations that are highly fair for groups can be very
unfair for individuals. Our finding is novel and useful for RS practitioners
aiming to improve the fairness of their systems. Our code is available at:
https://github.com/theresiavr/stairway-to-fairness.",https://github.com/theresiavr/stairway-to-fairness,Accepted to RecSys 2025 (short paper)
SEAL: Structure and Element Aware Learning to Improve Long Structured Document Retrieval,"Xinhao Huang, Zhibo Ren, Yipeng Yu, Ying Zhou, Zulong Chen, Zeyi Wen",2025-08-28,2508.20778v2,http://arxiv.org/abs/2508.20778v2,http://arxiv.org/pdf/2508.20778v2,information_retrieval,cs.IR,"In long structured document retrieval, existing methods typically fine-tune
pre-trained language models (PLMs) using contrastive learning on datasets
lacking explicit structural information. This practice suffers from two
critical issues: 1) current methods fail to leverage structural features and
element-level semantics effectively, and 2) the lack of datasets containing
structural metadata. To bridge these gaps, we propose \our, a novel contrastive
learning framework. It leverages structure-aware learning to preserve semantic
hierarchies and masked element alignment for fine-grained semantic
discrimination. Furthermore, we release \dataset, a long structured document
retrieval dataset with rich structural annotations. Extensive experiments on
both released and industrial datasets across various modern PLMs, along with
online A/B testing, demonstrate consistent performance improvements, boosting
NDCG@10 from 73.96\% to 77.84\% on BGE-M3. The resources are available at
https://github.com/xinhaoH/SEAL.",https://github.com/xinhaoH/SEAL,Accepted at EMNLP 2025 Main Conference
CRouting: Reducing Expensive Distance Calls in Graph-Based Approximate Nearest Neighbor Search,"Zhenxin Li, Shuibing He, Jiahao Guo, Xuechen Zhang, Xian-He Sun, Gang Chen",2025-08-30,2509.00365v1,http://arxiv.org/abs/2509.00365v1,http://arxiv.org/pdf/2509.00365v1,databases,cs.DB,"Approximate nearest neighbor search (ANNS) is a crucial problem in
information retrieval and AI applications. Recently, there has been a surge of
interest in graph-based ANNS algorithms due to their superior efficiency and
accuracy. However, the repeated computation of distances in high-dimensional
spaces constitutes the primary time cost of graph-based methods. To accelerate
the search, we propose a novel routing strategy named CRouting, which bypasses
unnecessary distance computations by exploiting the angle distributions of
high-dimensional vectors. CRouting is designed as a plugin to optimize existing
graph-based search with minimal code modifications. Our experiments show that
CRouting reduces the number of distance computations by up to 41.5% and boosts
queries per second by up to 1.48$\times$ on two predominant graph indexes, HNSW
and NSG. Code is publicly available at https://github.com/ISCS-ZJU/CRouting.",https://github.com/ISCS-ZJU/CRouting,
ST-Raptor: LLM-Powered Semi-Structured Table Question Answering,"Zirui Tang, Boyu Niu, Xuanhe Zhou, Boxiu Li, Wei Zhou, Jiannan Wang, Guoliang Li, Xinyi Zhang, Fan Wu",2025-08-25,2508.18190v3,http://arxiv.org/abs/2508.18190v3,http://arxiv.org/pdf/2508.18190v3,databases,cs.AI,"Semi-structured tables, widely used in real-world applications (e.g.,
financial reports, medical records, transactional orders), often involve
flexible and complex layouts (e.g., hierarchical headers and merged cells).
These tables generally rely on human analysts to interpret table layouts and
answer relevant natural language questions, which is costly and inefficient. To
automate the procedure, existing methods face significant challenges. First,
methods like NL2SQL require converting semi-structured tables into structured
ones, which often causes substantial information loss. Second, methods like
NL2Code and multi-modal LLM QA struggle to understand the complex layouts of
semi-structured tables and cannot accurately answer corresponding questions. To
this end, we propose ST-Raptor, a tree-based framework for semi-structured
table question answering using large language models. First, we introduce the
Hierarchical Orthogonal Tree (HO-Tree), a structural model that captures
complex semi-structured table layouts, along with an effective algorithm for
constructing the tree. Second, we define a set of basic tree operations to
guide LLMs in executing common QA tasks. Given a user question, ST-Raptor
decomposes it into simpler sub-questions, generates corresponding tree
operation pipelines, and conducts operation-table alignment for accurate
pipeline execution. Third, we incorporate a two-stage verification mechanism:
forward validation checks the correctness of execution steps, while backward
validation evaluates answer reliability by reconstructing queries from
predicted answers. To benchmark the performance, we present SSTQA, a dataset of
764 questions over 102 real-world semi-structured tables. Experiments show that
ST-Raptor outperforms nine baselines by up to 20% in answer accuracy. The code
is available at https://github.com/weAIDB/ST-Raptor.",https://github.com/weAIDB/ST-Raptor,"Extension of our SIGMOD 2026 paper. Please refer to source code
  available at: https://github.com/weAIDB/ST-Raptor"
PGTuner: An Efficient Framework for Automatic and Transferable Configuration Tuning of Proximity Graphs,"Hao Duan, Yitong Song, Bin Yao, Anqi Liang",2025-08-25,2508.17886v1,http://arxiv.org/abs/2508.17886v1,http://arxiv.org/pdf/2508.17886v1,databases,cs.DB,"Approximate Nearest Neighbor Search (ANNS) plays a crucial role in many key
areas. Proximity graphs (PGs) are the leading method for ANNS, offering the
best balance between query efficiency and accuracy. However, their performance
heavily depends on various construction and query parameters, which are
difficult to optimize due to their complex inter-dependencies. Given that users
often prioritize specific accuracy levels, efficiently identifying the optimal
PG configurations to meet these targets is essential. Although some studies
have explored automatic configuration tuning for PGs, they are limited by
inefficiencies and suboptimal results. These issues stem from the need to
construct numerous PGs for searching and re-tuning from scratch whenever the
dataset changes, as well as the failure to capture the complex dependencies
between configurations, query performance, and tuning objectives.
  To address these challenges, we propose PGTuner, an efficient framework for
automatic PG configuration tuning leveraging pre-training knowledge and model
transfer techniques. PGTuner improves efficiency through a pre-trained query
performance prediction (QPP) model, eliminating the need to build multiple PGs.
It also features a deep reinforcement learning-based parameter configuration
recommendation (PCR) model to recommend optimal configurations for specific
datasets and accuracy targets. Additionally, PGTuner incorporates
out-of-distribution detection and deep active learning for efficient tuning in
dynamic scenarios and transferring to new datasets. Extensive experiments
demonstrate that PGTuner can stably achieve the top-level tuning effect across
different datasets while significantly improving tuning efficiency by up to
14.69X, with a 14.64X boost in dynamic scenarios. The code and data for PGTuner
are available online at https://github.com/hao-duan/PGTuner.",https://github.com/hao-duan/PGTuner,
Attribute Filtering in Approximate Nearest Neighbor Search: An In-depth Experimental Study,"Mocheng Li, Xiao Yan, Baotong Lu, Yue Zhang, James Cheng, Chenhao Ma",2025-08-22,2508.16263v1,http://arxiv.org/abs/2508.16263v1,http://arxiv.org/pdf/2508.16263v1,databases,cs.DB,"With the growing integration of structured and unstructured data, new methods
have emerged for performing similarity searches on vectors while honoring
structured attribute constraints, i.e., a process known as Filtering
Approximate Nearest Neighbor (Filtering ANN) search. Since many of these
algorithms have only appeared in recent years and are designed to work with a
variety of base indexing methods and filtering strategies, there is a pressing
need for a unified analysis that identifies their core techniques and enables
meaningful comparisons.
  In this work, we present a unified Filtering ANN search interface that
encompasses the latest algorithms and evaluate them extensively from multiple
perspectives. First, we propose a comprehensive taxonomy of existing Filtering
ANN algorithms based on attribute types and filtering strategies. Next, we
analyze their key components, i.e., index structures, pruning strategies, and
entry point selection, to elucidate design differences and tradeoffs. We then
conduct a broad experimental evaluation on 10 algorithms and 12 methods across
4 datasets (each with up to 10 million items), incorporating both synthetic and
real attributes and covering selectivity levels from 0.1% to 100%. Finally, an
in-depth component analysis reveals the influence of pruning, entry point
selection, and edge filtering costs on overall performance. Based on our
findings, we summarize the strengths and limitations of each approach, provide
practical guidelines for selecting appropriate methods, and suggest promising
directions for future research. Our code is available at:
https://github.com/lmccccc/FANNBench.",https://github.com/lmccccc/FANNBench,"15 pages, 15 figures, Accepted at SIGMOD 2026"
AmbiSQL: Interactive Ambiguity Detection and Resolution for Text-to-SQL,"Zhongjun Ding, Yin Lin, Tianjing Zeng",2025-08-21,2508.15276v1,http://arxiv.org/abs/2508.15276v1,http://arxiv.org/pdf/2508.15276v1,databases,cs.DB,"Text-to-SQL systems translate natural language questions into SQL queries,
providing substantial value for non-expert users. While large language models
(LLMs) show promising results for this task, they remain error-prone. Query
ambiguity has been recognized as a major obstacle for LLM-based Text-to-SQL
systems, leading to misinterpretation of user intent and inaccurate SQL
generation. We demonstrate AmbiSQL, an interactive system that automatically
detects query ambiguities and guides users through intuitive multiple-choice
questions to clarify their intent. Our approach introduces a fine-grained
ambiguity taxonomy for identifying ambiguities that affect database element
mapping and LLM reasoning, then incorporates user feedback to rewrite ambiguous
questions. Evaluation on an ambiguous query dataset shows that AmbiSQL achieves
87.2% precision in ambiguity detection and improves SQL exact match accuracy by
50% when integrated with Text-to-SQL systems. Our demonstration showcases the
significant performance gains and highlights the system's practical usability.
Code repo and demonstration are available at:
https://github.com/JustinzjDing/AmbiSQL.",https://github.com/JustinzjDing/AmbiSQL,
Cold-RL: Learning Cache Eviction with Offline Reinforcement Learning for NGINX,"Aayush Gupta, Arpit Bhayani",2025-08-17,2508.12485v1,http://arxiv.org/abs/2508.12485v1,http://arxiv.org/pdf/2508.12485v1,databases,cs.LG,"Web proxies such as NGINX commonly rely on least-recently-used (LRU)
eviction, which is size agnostic and can thrash under periodic bursts and mixed
object sizes. We introduce Cold-RL, a learned eviction policy for NGINX that
replaces LRU's forced-expire path with a dueling Deep Q-Network served by an
ONNX sidecar within a strict microsecond budget. On each eviction, Cold-RL
samples the K least-recently-used objects, extracts six lightweight features
(age, size, hit count, inter-arrival time, remaining TTL, and last origin RTT),
and requests a bitmask of victims; a hard timeout of 500 microseconds triggers
immediate fallback to native LRU. Policies are trained offline by replaying
NGINX access logs through a cache simulator with a simple reward: a retained
object earns one point if it is hit again before TTL expiry. We compare against
LRU, LFU, size-based, adaptive LRU, and a hybrid baseline on two adversarial
workloads. With a 25 MB cache, Cold-RL raises hit ratio from 0.1436 to 0.3538,
a 146 percent improvement over the best classical baseline; at 100 MB, from
0.7530 to 0.8675, a 15 percent gain; and at 400 MB it matches classical methods
(about 0.918). Inference adds less than 2 percent CPU overhead and keeps 95th
percentile eviction latency within budget. To our knowledge, this is the first
reinforcement learning eviction policy integrated into NGINX with strict SLOs.",https://github.com/ayushgupta4897/DRL-Cache,"8 pages, 4 figures (system architecture, eviction path, training
  pipeline, and DQN algorithm), 2 tables. Code available at
  https://github.com/ayushgupta4897/DRL-Cache"
Chain-of-Query: Unleashing the Power of LLMs in SQL-Aided Table Understanding via Multi-Agent Collaboration,"Songyuan Sui, Hongyi Liu, Serena Liu, Li Li, Soo-Hyun Choi, Rui Chen, Xia Hu",2025-08-14,2508.15809v1,http://arxiv.org/abs/2508.15809v1,http://arxiv.org/pdf/2508.15809v1,databases,cs.CL,"Table understanding requires structured, multi-step reasoning. Large Language
Models (LLMs) struggle with it due to the structural complexity of tabular
data. Recently, multi-agent frameworks for SQL generation have shown promise in
tackling the challenges of understanding tabular data, but existing approaches
often suffer from limitations such as the inability to comprehend table
structure for reliable SQL generation, error propagation that results in
invalid queries, and over-reliance on execution correctness. To address these
issues, we propose Chain-of-Query (CoQ), a novel multi-agent framework for
SQL-aided table understanding. CoQ adopts natural-language-style
representations of table schemas to abstract away structural noise and enhance
understanding. It employs a clause-by-clause SQL generation strategy to improve
query quality and introduces a hybrid reasoning division that separates
SQL-based mechanical reasoning from LLM-based logical inference, thereby
reducing reliance on execution outcomes. Experiments with four models (both
closed- and open-source) across five widely used benchmarks show that
Chain-of-Query significantly improves accuracy from 61.11% to 74.77% and
reduces the invalid SQL rate from 9.48% to 3.34%, demonstrating its superior
effectiveness in table understanding. The code is available at
https://github.com/SongyuanSui/ChainofQuery.",https://github.com/SongyuanSui/ChainofQuery,"9 pages main content, 24 pages total including appendix, 6 figures"
