title,authors,published,arxiv_id,url,pdf_url,category,primary_category,abstract,code_links,comment
Confidence and Stability of Global and Pairwise Scores in NLP Evaluation,"Georgii Levtsov, Dmitry Ustalov",2025-07-02,2507.01633v1,http://arxiv.org/abs/2507.01633v1,http://arxiv.org/pdf/2507.01633v1,information_retrieval,cs.CL,"With the advent of highly capable instruction-tuned neural language models,
benchmarking in natural language processing (NLP) is increasingly shifting
towards pairwise comparison leaderboards, such as LMSYS Arena, from traditional
global pointwise scores (e.g., GLUE, BIG-bench, SWE-bench). This paper
empirically investigates the strengths and weaknesses of both global scores and
pairwise comparisons to aid decision-making in selecting appropriate model
evaluation strategies. Through computational experiments on synthetic and
real-world datasets using standard global metrics and the popular Bradley-Terry
model for pairwise comparisons, we found that while global scores provide more
reliable overall rankings, they can underestimate strong models with rare,
significant errors or low confidence. Conversely, pairwise comparisons are
particularly effective for identifying strong contenders among models with
lower global scores, especially where quality metrics are hard to define (e.g.,
text generation), though they require more comparisons to converge if ties are
frequent. Our code and data are available at
https://github.com/HSPyroblast/srw-ranking under a permissive license.",https://github.com/HSPyroblast/srw-ranking,"8 pages, accepted at ACL SRW 2025"
MassTool: A Multi-Task Search-Based Tool Retrieval Framework for Large Language Models,"Jianghao Lin, Xinyuan Wang, Xinyi Dai, Menghui Zhu, Bo Chen, Ruiming Tang, Yong Yu, Weinan Zhang",2025-07-01,2507.00487v2,http://arxiv.org/abs/2507.00487v2,http://arxiv.org/pdf/2507.00487v2,information_retrieval,cs.IR,"Tool retrieval is a critical component in enabling large language models
(LLMs) to interact effectively with external tools. It aims to precisely filter
the massive tools into a small set of candidates for the downstream
tool-augmented LLMs. However, most existing approaches primarily focus on
optimizing tool representations, often neglecting the importance of precise
query comprehension. To address this gap, we introduce MassTool, a multi-task
search-based framework designed to enhance both query representation and tool
retrieval accuracy. MassTool employs a two-tower architecture: a tool usage
detection tower that predicts the need for function calls, and a tool retrieval
tower that leverages a query-centric graph convolution network (QC-GCN) for
effective query-tool matching. It also incorporates search-based user intent
modeling (SUIM) to handle diverse and out-of-distribution queries, alongside an
adaptive knowledge transfer (AdaKT) module for efficient multi-task learning.
By jointly optimizing tool usage detection loss, list-wise retrieval loss, and
contrastive regularization loss, MassTool establishes a robust dual-step
sequential decision-making pipeline for precise query understanding. Extensive
experiments demonstrate its effectiveness in improving retrieval accuracy. Our
code is available at https://github.com/wxydada/MassTool.",https://github.com/wxydada/MassTool,
Thought-Augmented Planning for LLM-Powered Interactive Recommender Agent,"Haocheng Yu, Yaxiong Wu, Hao Wang, Wei Guo, Yong Liu, Yawen Li, Yuyang Ye, Junping Du, Enhong Chen",2025-06-30,2506.23485v1,http://arxiv.org/abs/2506.23485v1,http://arxiv.org/pdf/2506.23485v1,information_retrieval,cs.CL,"Interactive recommendation is a typical information-seeking task that allows
users to interactively express their needs through natural language and obtain
personalized recommendations. Large language model-powered (LLM-powered) agents
have become a new paradigm in interactive recommendations, effectively
capturing users' real-time needs and enhancing personalized experiences.
However, due to limited planning and generalization capabilities, existing
formulations of LLM-powered interactive recommender agents struggle to
effectively address diverse and complex user intents, such as intuitive,
unrefined, or occasionally ambiguous requests. To tackle this challenge, we
propose a novel thought-augmented interactive recommender agent system (TAIRA)
that addresses complex user intents through distilled thought patterns.
Specifically, TAIRA is designed as an LLM-powered multi-agent system featuring
a manager agent that orchestrates recommendation tasks by decomposing user
needs and planning subtasks, with its planning capacity strengthened through
Thought Pattern Distillation (TPD), a thought-augmentation method that extracts
high-level thoughts from the agent's and human experts' experiences. Moreover,
we designed a set of user simulation schemes to generate personalized queries
of different difficulties and evaluate the recommendations based on specific
datasets. Through comprehensive experiments conducted across multiple datasets,
TAIRA exhibits significantly enhanced performance compared to existing methods.
Notably, TAIRA shows a greater advantage on more challenging tasks while
generalizing effectively on novel tasks, further validating its superiority in
managing complex user intents within interactive recommendation systems. The
code is publicly available at:https://github.com/Alcein/TAIRA.",https://github.com/Alcein/TAIRA,
JointRank: Rank Large Set with Single Pass,Evgeny Dedov,2025-06-27,2506.22262v1,http://arxiv.org/abs/2506.22262v1,http://arxiv.org/pdf/2506.22262v1,information_retrieval,cs.IR,"Efficiently ranking relevant items from large candidate pools is a
cornerstone of modern information retrieval systems -- such as web search,
recommendation, and retrieval-augmented generation. Listwise rerankers, which
improve relevance by jointly considering multiple candidates, are often limited
in practice: either by model input size constraints, or by degraded quality
when processing large sets. We propose a model-agnostic method for fast
reranking large sets that exceed a model input limits. The method first
partitions candidate items into overlapping blocks, each of which is ranked
independently in parallel. Implicit pairwise comparisons are then derived from
these local rankings. Finally, these comparisons are aggregated to construct a
global ranking using algorithms such as Winrate or PageRank. Experiments on
TREC DL-2019 show that our method achieves an nDCG@10 of 70.88 compared to the
57.68 for full-context listwise approach using gpt-4.1-mini as long-context
model, while reducing latency from 21 to 8 seconds.
  The implementation of the algorithm and the experiments is available in the
repository: https://github.com/V3RGANz/jointrank",https://github.com/V3RGANz/jointrank,ICTIR'25 Accepted
skLEP: A Slovak General Language Understanding Benchmark,"Marek Šuppa, Andrej Ridzik, Daniel Hládek, Tomáš Javůrek, Viktória Ondrejová, Kristína Sásiková, Martin Tamajka, Marián Šimko",2025-06-26,2506.21508v1,http://arxiv.org/abs/2506.21508v1,http://arxiv.org/pdf/2506.21508v1,information_retrieval,cs.CL,"In this work, we introduce skLEP, the first comprehensive benchmark
specifically designed for evaluating Slovak natural language understanding
(NLU) models. We have compiled skLEP to encompass nine diverse tasks that span
token-level, sentence-pair, and document-level challenges, thereby offering a
thorough assessment of model capabilities. To create this benchmark, we curated
new, original datasets tailored for Slovak and meticulously translated
established English NLU resources. Within this paper, we also present the first
systematic and extensive evaluation of a wide array of Slovak-specific,
multilingual, and English pre-trained language models using the skLEP tasks.
Finally, we also release the complete benchmark data, an open-source toolkit
facilitating both fine-tuning and evaluation of models, and a public
leaderboard at https://github.com/slovak-nlp/sklep in the hopes of fostering
reproducibility and drive future research in Slovak NLU.",https://github.com/slovak-nlp/sklep,ACL 2025 Findings
Small Encoders Can Rival Large Decoders in Detecting Groundedness,"Istabrak Abbes, Gabriele Prato, Quentin Fournier, Fernando Rodriguez, Alaa Boukhary, Adam Elwood, Sarath Chandar",2025-06-26,2506.21288v1,http://arxiv.org/abs/2506.21288v1,http://arxiv.org/pdf/2506.21288v1,information_retrieval,cs.CL,"Augmenting large language models (LLMs) with external context significantly
improves their performance in natural language processing (NLP) tasks. However,
LLMs struggle to answer queries reliably when the provided context lacks
information, often resorting to ungrounded speculation or internal knowledge.
Groundedness - generating responses strictly supported by the context - is
essential for ensuring factual consistency and trustworthiness. This study
focuses on detecting whether a given query is grounded in a document provided
in context before the costly answer generation by LLMs. Such a detection
mechanism can significantly reduce both inference time and resource
consumption. We show that lightweight, task specific encoder models such as
RoBERTa and NomicBERT, fine-tuned on curated datasets, can achieve accuracy
comparable to state-of-the-art LLMs, such as Llama3 8B and GPT4o, in
groundedness detection while reducing inference latency by orders of magnitude.
The code is available at : https://github.com/chandarlab/Hallucinate-less",https://github.com/chandarlab/Hallucinate-less,
Response Quality Assessment for Retrieval-Augmented Generation via Conditional Conformal Factuality,"Naihe Feng, Yi Sui, Shiyi Hou, Jesse C. Cresswell, Ga Wu",2025-06-26,2506.20978v1,http://arxiv.org/abs/2506.20978v1,http://arxiv.org/pdf/2506.20978v1,information_retrieval,cs.IR,"Existing research on Retrieval-Augmented Generation (RAG) primarily focuses
on improving overall question-answering accuracy, often overlooking the quality
of sub-claims within generated responses. Recent methods that attempt to
improve RAG trustworthiness, such as through auto-evaluation metrics, lack
probabilistic guarantees or require ground truth answers. To address these
limitations, we propose Conformal-RAG, a novel framework inspired by recent
applications of conformal prediction (CP) on large language models (LLMs).
Conformal-RAG leverages CP and internal information from the RAG mechanism to
offer statistical guarantees on response quality. It ensures group-conditional
coverage spanning multiple sub-domains without requiring manual labelling of
conformal sets, making it suitable for complex RAG applications. Compared to
existing RAG auto-evaluation methods, Conformal-RAG offers statistical
guarantees on the quality of refined sub-claims, ensuring response reliability
without the need for ground truth answers. Additionally, our experiments
demonstrate that by leveraging information from the RAG system, Conformal-RAG
retains up to 60\% more high-quality sub-claims from the response compared to
direct applications of CP to LLMs, while maintaining the same reliability
guarantee.",https://github.com/n4feng/ResponseQualityAssessment,"Accepted by SIGIR 2025 short paper, 5 pages, Code is available at
  https://github.com/n4feng/ResponseQualityAssessment"
EraRAG: Efficient and Incremental Retrieval Augmented Generation for Growing Corpora,"Fangyuan Zhang, Zhengjun Huang, Yingli Zhou, Qintian Guo, Zhixun Li, Wensheng Luo, Di Jiang, Yixiang Fang, Xiaofang Zhou",2025-06-26,2506.20963v1,http://arxiv.org/abs/2506.20963v1,http://arxiv.org/pdf/2506.20963v1,information_retrieval,cs.IR,"Graph-based Retrieval-Augmented Generation (Graph-RAG) enhances large
language models (LLMs) by structuring retrieval over an external corpus.
However, existing approaches typically assume a static corpus, requiring
expensive full-graph reconstruction whenever new documents arrive, limiting
their scalability in dynamic, evolving environments. To address these
limitations, we introduce EraRAG, a novel multi-layered Graph-RAG framework
that supports efficient and scalable dynamic updates. Our method leverages
hyperplane-based Locality-Sensitive Hashing (LSH) to partition and organize the
original corpus into hierarchical graph structures, enabling efficient and
localized insertions of new data without disrupting the existing topology. The
design eliminates the need for retraining or costly recomputation while
preserving high retrieval accuracy and low latency. Experiments on large-scale
benchmarks demonstrate that EraRag achieves up to an order of magnitude
reduction in update time and token consumption compared to existing Graph-RAG
systems, while providing superior accuracy performance. This work offers a
practical path forward for RAG systems that must operate over continually
growing corpora, bridging the gap between retrieval efficiency and
adaptability. Our code and data are available at
https://github.com/EverM0re/EraRAG-Official.",https://github.com/EverM0re/EraRAG-Official,Under review
RAG-VisualRec: An Open Resource for Vision- and Text-Enhanced Retrieval-Augmented Generation in Recommendation,"Ali Tourani, Fatemeh Nazary, Yashar Deldjoo",2025-06-25,2506.20817v1,http://arxiv.org/abs/2506.20817v1,http://arxiv.org/pdf/2506.20817v1,information_retrieval,cs.IR,"This paper addresses the challenge of developing multimodal recommender
systems for the movie domain, where limited metadata (e.g., title, genre) often
hinders the generation of robust recommendations. We introduce a resource that
combines LLM-generated plot descriptions with trailer-derived visual embeddings
in a unified pipeline supporting both Retrieval-Augmented Generation (RAG) and
collaborative filtering. Central to our approach is a data augmentation step
that transforms sparse metadata into richer textual signals, alongside fusion
strategies (e.g., PCA, CCA) that integrate visual cues. Experimental
evaluations demonstrate that CCA-based fusion significantly boosts recall
compared to unimodal baselines, while an LLM-driven re-ranking step further
improves NDCG, particularly in scenarios with limited textual data. By
releasing this framework, we invite further exploration of multi-modal
recommendation techniques tailored to cold-start, novelty-focused, and
domain-specific settings. All code, data, and detailed documentation are
publicly available at: https://github.com/RecSys-lab/RAG-VisualRec",https://github.com/RecSys-lab/RAG-VisualRec,"20 pages, 6 figures, 5 tables"
ReCode: Updating Code API Knowledge with Reinforcement Learning,"Haoze Wu, Yunzhi Yao, Wenhao Yu, Huajun Chen, Ningyu Zhang",2025-06-25,2506.20495v1,http://arxiv.org/abs/2506.20495v1,http://arxiv.org/pdf/2506.20495v1,information_retrieval,cs.CL,"Large Language Models (LLMs) exhibit remarkable code generation capabilities
but falter when adapting to frequent updates in external library APIs. This
critical limitation, stemming from reliance on outdated API knowledge from
their training data, even with access to current documentation, impedes
reliable code generation in dynamic environments. To tackle this issue, we
propose ReCode (rule-based Reinforcement learning for Code Update), a novel
framework that mimics human programmer adaptation to API changes. Specifically,
we construct a dataset of approximately 2,000 data entries to train the LLMs to
perform version migration based on updated information. Then, we introduce a
modified string similarity metric for code evaluation as the reward for
reinforcement learning. Our experiments demonstrate that ReCode substantially
boosts LLMs' code generation performance in dynamic API scenarios, especially
on the unseen CodeUpdateArena task. Crucially, compared to supervised
fine-tuning, ReCode has less impact on LLMs' general code generation abilities.
We apply ReCode on various LLMs and reinforcement learning algorithms (GRPO and
DAPO), all achieving consistent improvements. Notably, after training,
Qwen2.5-Coder-7B outperforms that of the 32B parameter code instruction-tuned
model and the reasoning model with the same architecture. Code is available at
https://github.com/zjunlp/ReCode.",https://github.com/zjunlp/ReCode,Work in progress
CoVE: Compressed Vocabulary Expansion Makes Better LLM-based Recommender Systems,"Haochen Zhang, Tianyi Zhang, Junze Yin, Oren Gal, Anshumali Shrivastava, Vladimir Braverman",2025-06-24,2506.19993v1,http://arxiv.org/abs/2506.19993v1,http://arxiv.org/pdf/2506.19993v1,information_retrieval,cs.IR,"Recommender systems play a pivotal role in providing relevant content to
users. With the rapid development of large language models (LLMs), researchers
have begun utilizing LLMs to build more powerful recommender systems. However,
existing approaches that focus on aligning LLMs with recommendation tasks do
not fully leverage their sequential information processing capabilities,
leading to suboptimal performance.
  In this paper, we propose a novel system called compressed vocabulary
expansion (CoVE). In CoVE, each item is assigned a unique ID within the
expanded vocabulary. Our framework effectively capitalizes on sequence
understanding abilities of LLMs, significantly enhancing their performance on
recommendation tasks. Additionally, we compress the embedding layer, making
CoVE practical for large-scale industrial applications. The effectiveness and
performance of CoVE are demonstrated through comprehensive experiments on
multiple recommendation datasets and comparisons with prior works. Our code can
be found at https://github.com/HaochenZhang717/CoVE-official-Repo.",https://github.com/HaochenZhang717/CoVE-official-Repo,Accepted by ACL 2025 Findings
From Web Search towards Agentic Deep Research: Incentivizing Search with Reasoning Agents,"Weizhi Zhang, Yangning Li, Yuanchen Bei, Junyu Luo, Guancheng Wan, Liangwei Yang, Chenxuan Xie, Yuyao Yang, Wei-Chieh Huang, Chunyu Miao, Henry Peng Zou, Xiao Luo, Yusheng Zhao, Yankai Chen, Chunkit Chan, Peilin Zhou, Xinyang Zhang, Chenwei Zhang, Jingbo Shang, Ming Zhang, Yangqiu Song, Irwin King, Philip S. Yu",2025-06-23,2506.18959v3,http://arxiv.org/abs/2506.18959v3,http://arxiv.org/pdf/2506.18959v3,information_retrieval,cs.IR,"Information retrieval is a cornerstone of modern knowledge acquisition,
enabling billions of queries each day across diverse domains. However,
traditional keyword-based search engines are increasingly inadequate for
handling complex, multi-step information needs. Our position is that Large
Language Models (LLMs), endowed with reasoning and agentic capabilities, are
ushering in a new paradigm termed Agentic Deep Research. These systems
transcend conventional information search techniques by tightly integrating
autonomous reasoning, iterative retrieval, and information synthesis into a
dynamic feedback loop. We trace the evolution from static web search to
interactive, agent-based systems that plan, explore, and learn. We also
introduce a test-time scaling law to formalize the impact of computational
depth on reasoning and search. Supported by benchmark results and the rise of
open-source implementations, we demonstrate that Agentic Deep Research not only
significantly outperforms existing approaches, but is also poised to become the
dominant paradigm for future information seeking. All the related resources,
including industry products, research papers, benchmark datasets, and
open-source implementations, are collected for the community in
https://github.com/DavidZWZ/Awesome-Deep-Research.",https://github.com/DavidZWZ/Awesome-Deep-Research,
Harnessing the Power of Reinforcement Learning for Language-Model-Based Information Retriever via Query-Document Co-Augmentation,"Jingming Liu, Yumeng Li, Wei Shi, Yao-Xiang Ding, Hui Su, Kun Zhou",2025-06-23,2506.18670v1,http://arxiv.org/abs/2506.18670v1,http://arxiv.org/pdf/2506.18670v1,information_retrieval,cs.IR,"Recent studies have proposed leveraging Large Language Models (LLMs) as
information retrievers through query rewriting. However, for challenging
corpora, we argue that enhancing queries alone is insufficient for robust
semantic matching; the LLM should also have sufficient understanding of the
corpus by directly handling and augmenting the documents themselves. To this
end, we present an LLM-based retriever empowered to augment both user queries
and corpus documents, with its policy fully explored via reinforcement learning
(RL) and minimal human inductive bias. Notably, we find that simply allowing
the LLM to modify documents yields little benefit unless paired with our
carefully designed bidirectional RL framework, which enables the LLM to
simultaneously learn and collaborate on both query and document augmentation
policies. A key technical challenge in realizing such a framework lies in
jointly updating both policies during training, where the rewards for the two
directions depend on each other, making their entangled reward intractable. Our
approach addresses this by introducing a reward sampling strategy and a
specifically designed RL algorithm that enables effective training with these
sampled rewards. Experimental results demonstrate that our approach
significantly enhances LLM-based retrieval performance in both sparse and dense
settings, particularly in difficult retrieval domains, and achieves strong
cross-benchmark generalization. Our code is released at
https://github.com/liujm2001/CoAugRetriever.",https://github.com/liujm2001/CoAugRetriever,
Schema-R1: A reasoning training approach for schema linking in Text-to-SQL Task,"Wuzhenghong Wen, Su Pan, yuwei Sun",2025-06-13,2506.11986v1,http://arxiv.org/abs/2506.11986v1,http://arxiv.org/pdf/2506.11986v1,databases,cs.AI,"Schema linking is a critical step in Text-to-SQL task, aiming to accurately
predict the table names and column names required for the SQL query based on
the given question. However, current fine-tuning approaches for schema linking
models employ a rote-learning paradigm, excessively optimizing for ground truth
schema linking outcomes while compromising reasoning ability. This limitation
arises because of the difficulty in acquiring a high-quality reasoning sample
for downstream tasks. To address this, we propose Schema-R1, a reasoning schema
linking model trained using reinforcement learning. Specifically, Schema-R1
consists of three key steps: constructing small batches of high-quality
reasoning samples, supervised fine-tuning for cold-start initialization, and
rule-based reinforcement learning training. The final results demonstrate that
our method effectively enhances the reasoning ability of the schema linking
model, achieving a 10\% improvement in filter accuracy compared to the existing
method. Our code is available at https://github.com/hongWin/Schema-R1/.",https://github.com/hongWin/Schema-R1,"11 pages, 3 figures, conference"
Bridging RDF Knowledge Graphs with Graph Neural Networks for Semantically-Rich Recommender Systems,"Michael Färber, David Lamprecht, Yuni Susanti",2025-06-10,2506.08743v1,http://arxiv.org/abs/2506.08743v1,http://arxiv.org/pdf/2506.08743v1,databases,cs.IR,"Graph Neural Networks (GNNs) have substantially advanced the field of
recommender systems. However, despite the creation of more than a thousand
knowledge graphs (KGs) under the W3C standard RDF, their rich semantic
information has not yet been fully leveraged in GNN-based recommender systems.
To address this gap, we propose a comprehensive integration of RDF KGs with
GNNs that utilizes both the topological information from RDF object properties
and the content information from RDF datatype properties. Our main focus is an
in-depth evaluation of various GNNs, analyzing how different semantic feature
initializations and types of graph structure heterogeneity influence their
performance in recommendation tasks. Through experiments across multiple
recommendation scenarios involving multi-million-node RDF graphs, we
demonstrate that harnessing the semantic richness of RDF KGs significantly
improves recommender systems and lays the groundwork for GNN-based recommender
systems for the Linked Open Data cloud. The code and data are available on our
GitHub repository: https://github.com/davidlamprecht/rdf-gnn-recommendation",https://github.com/davidlamprecht/rdf-gnn-recommendation,Accepted at DASFAA 2025
KramaBench: A Benchmark for AI Systems on Data-to-Insight Pipelines over Data Lakes,"Eugenie Lai, Gerardo Vitagliano, Ziyu Zhang, Sivaprasad Sudhir, Om Chabra, Anna Zeng, Anton A. Zabreyko, Chenning Li, Ferdi Kossmann, Jialin Ding, Jun Chen, Markos Markakis, Matthew Russo, Weiyang Wang, Ziniu Wu, Michael J. Cafarella, Lei Cao, Samuel Madden, Tim Kraska",2025-06-06,2506.06541v1,http://arxiv.org/abs/2506.06541v1,http://arxiv.org/pdf/2506.06541v1,databases,cs.DB,"Constructing real-world data-to-insight pipelines often involves data
extraction from data lakes, data integration across heterogeneous data sources,
and diverse operations from data cleaning to analysis. The design and
implementation of data science pipelines require domain knowledge, technical
expertise, and even project-specific insights. AI systems have shown remarkable
reasoning, coding, and understanding capabilities. However, it remains unclear
to what extent these capabilities translate into successful design and
execution of such complex pipelines. We introduce KRAMABENCH: a benchmark
composed of 104 manually-curated real-world data science pipelines spanning
1700 data files from 24 data sources in 6 different domains. We show that these
pipelines test the end-to-end capabilities of AI systems on data processing,
requiring data discovery, wrangling and cleaning, efficient processing,
statistical reasoning, and orchestrating data processing steps given a
high-level task. Our evaluation tests 5 general models and 3 code generation
models using our reference framework, DS-GURU, which instructs the AI model to
decompose a question into a sequence of subtasks, reason through each step, and
synthesize Python code that implements the proposed design. Our results on
KRAMABENCH show that, although the models are sufficiently capable of solving
well-specified data science code generation tasks, when extensive data
processing and domain knowledge are required to construct real-world data
science pipelines, existing out-of-box models fall short. Progress on
KramaBench represents crucial steps towards developing autonomous data science
agents for real-world applications. Our code, reference framework, and data are
available at https://github.com/mitdbg/KramaBench.",https://github.com/mitdbg/KramaBench,
MMTU: A Massive Multi-Task Table Understanding and Reasoning Benchmark,"Junjie Xing, Yeye He, Mengyu Zhou, Haoyu Dong, Shi Han, Lingjiao Chen, Dongmei Zhang, Surajit Chaudhuri, H. V. Jagadish",2025-06-05,2506.05587v1,http://arxiv.org/abs/2506.05587v1,http://arxiv.org/pdf/2506.05587v1,databases,cs.AI,"Tables and table-based use cases play a crucial role in many important
real-world applications, such as spreadsheets, databases, and computational
notebooks, which traditionally require expert-level users like data engineers,
data analysts, and database administrators to operate. Although LLMs have shown
remarkable progress in working with tables (e.g., in spreadsheet and database
copilot scenarios), comprehensive benchmarking of such capabilities remains
limited. In contrast to an extensive and growing list of NLP benchmarks,
evaluations of table-related tasks are scarce, and narrowly focus on tasks like
NL-to-SQL and Table-QA, overlooking the broader spectrum of real-world tasks
that professional users face. This gap limits our understanding and model
progress in this important area.
  In this work, we introduce MMTU, a large-scale benchmark with over 30K
questions across 25 real-world table tasks, designed to comprehensively
evaluate models ability to understand, reason, and manipulate real tables at
the expert-level. These tasks are drawn from decades' worth of computer science
research on tabular data, with a focus on complex table tasks faced by
professional users. We show that MMTU require a combination of skills --
including table understanding, reasoning, and coding -- that remain challenging
for today's frontier models, where even frontier reasoning models like OpenAI
o4-mini and DeepSeek R1 score only around 60%, suggesting significant room for
improvement. We highlight key findings in our evaluation using MMTU and hope
that this benchmark drives further advances in understanding and developing
foundation models for structured data processing and analysis. Our code and
data are available at https://github.com/MMTU-Benchmark/MMTU and
https://huggingface.co/datasets/MMTU-benchmark/MMTU.",https://github.com/MMTU-Benchmark/MMTU,
