[
  {
    "title": "DS@GT at LongEval: Evaluating Temporal Performance in Web Search Systems and Topics with Two-Stage Retrieval",
    "authors": "Anthony Miyaguchi, Imran Afrulbasha, Aleksandar Pramov",
    "published": "2025-07-11",
    "arxiv_id": "2507.08360v1",
    "url": "http://arxiv.org/abs/2507.08360v1",
    "pdf_url": "http://arxiv.org/pdf/2507.08360v1",
    "category": "information_retrieval",
    "primary_category": "cs.IR",
    "abstract": "Information Retrieval (IR) models are often trained on static datasets,\nmaking them vulnerable to performance degradation as web content evolves. The\nDS@GT competition team participated in the Longitudinal Evaluation of Model\nPerformance (LongEval) lab at CLEF 2025, which evaluates IR systems across\ntemporally distributed web snapshots. Our analysis of the Qwant web dataset\nincludes exploratory data analysis with topic modeling over time. The two-phase\nretrieval system employs sparse keyword searches, utilizing query expansion and\ndocument reranking. Our best system achieves an average NDCG@10 of 0.296 across\nthe entire training and test dataset, with an overall best score of 0.395 on\n2023-05. The accompanying source code for this paper is at\nhttps://github.com/dsgt-arc/longeval-2025",
    "code_links": [
      "https://github.com/dsgt-arc/longeval-2025"
    ],
    "comment": null
  },
  {
    "title": "Transfer Learning and Mixup for Fine-Grained Few-Shot Fungi Classification",
    "authors": "Jason Kahei Tam, Murilo Gustineli, Anthony Miyaguchi",
    "published": "2025-07-11",
    "arxiv_id": "2507.08248v1",
    "url": "http://arxiv.org/abs/2507.08248v1",
    "pdf_url": "http://arxiv.org/pdf/2507.08248v1",
    "category": "information_retrieval",
    "primary_category": "cs.CV",
    "abstract": "Accurate identification of fungi species presents a unique challenge in\ncomputer vision due to fine-grained inter-species variation and high\nintra-species variation. This paper presents our approach for the FungiCLEF\n2025 competition, which focuses on few-shot fine-grained visual categorization\n(FGVC) using the FungiTastic Few-Shot dataset. Our team (DS@GT) experimented\nwith multiple vision transformer models, data augmentation, weighted sampling,\nand incorporating textual information. We also explored generative AI models\nfor zero-shot classification using structured prompting but found them to\nsignificantly underperform relative to vision-based models. Our final model\noutperformed both competition baselines and highlighted the effectiveness of\ndomain specific pretraining and balanced sampling strategies. Our approach\nranked 35/74 on the private test set in post-completion evaluation, this\nsuggests additional work can be done on metadata selection and domain-adapted\nmulti-modal learning. Our code is available at\nhttps://github.com/dsgt-arc/fungiclef-2025.",
    "code_links": [
      "https://github.com/dsgt-arc/fungiclef-2025"
    ],
    "comment": null
  },
  {
    "title": "DTECT: Dynamic Topic Explorer & Context Tracker",
    "authors": "Suman Adhya, Debarshi Kumar Sanyal",
    "published": "2025-07-10",
    "arxiv_id": "2507.07910v2",
    "url": "http://arxiv.org/abs/2507.07910v2",
    "pdf_url": "http://arxiv.org/pdf/2507.07910v2",
    "category": "information_retrieval",
    "primary_category": "cs.CL",
    "abstract": "The explosive growth of textual data over time presents a significant\nchallenge in uncovering evolving themes and trends. Existing dynamic topic\nmodeling techniques, while powerful, often exist in fragmented pipelines that\nlack robust support for interpretation and user-friendly exploration. We\nintroduce DTECT (Dynamic Topic Explorer & Context Tracker), an end-to-end\nsystem that bridges the gap between raw textual data and meaningful temporal\ninsights. DTECT provides a unified workflow that supports data preprocessing,\nmultiple model architectures, and dedicated evaluation metrics to analyze the\ntopic quality of temporal topic models. It significantly enhances\ninterpretability by introducing LLM-driven automatic topic labeling, trend\nanalysis via temporally salient words, interactive visualizations with\ndocument-level summarization, and a natural language chat interface for\nintuitive data querying. By integrating these features into a single, cohesive\nplatform, DTECT empowers users to more effectively track and understand\nthematic dynamics. DTECT is open-source and available at\nhttps://github.com/AdhyaSuman/DTECT.",
    "code_links": [
      "https://github.com/AdhyaSuman/DTECT"
    ],
    "comment": "Code: https://github.com/AdhyaSuman/DTECT | Demo:\n  https://huggingface.co/spaces/AdhyaSuman/DTECT | Video:\n  https://youtu.be/B8nNfxFoJAU"
  },
  {
    "title": "Boosting Parameter Efficiency in LLM-Based Recommendation through Sophisticated Pruning",
    "authors": "Shanle Zheng, Keqin Bao, Jizhi Zhang, Yang Zhang, Fuli Feng, Xiangnan He",
    "published": "2025-07-09",
    "arxiv_id": "2507.07064v1",
    "url": "http://arxiv.org/abs/2507.07064v1",
    "pdf_url": "http://arxiv.org/pdf/2507.07064v1",
    "category": "information_retrieval",
    "primary_category": "cs.IR",
    "abstract": "LLM-based recommender systems have made significant progress; however, the\ndeployment cost associated with the large parameter volume of LLMs still\nhinders their real-world applications. This work explores parameter pruning to\nimprove parameter efficiency while maintaining recommendation quality, thereby\nenabling easier deployment. Unlike existing approaches that focus primarily on\ninter-layer redundancy, we uncover intra-layer redundancy within components\nsuch as self-attention and MLP modules. Building on this analysis, we propose a\nmore fine-grained pruning approach that integrates both intra-layer and\nlayer-wise pruning. Specifically, we introduce a three-stage pruning strategy\nthat progressively prunes parameters at different levels and parts of the\nmodel, moving from intra-layer to layer-wise pruning, or from width to depth.\nEach stage also includes a performance restoration step using distillation\ntechniques, helping to strike a balance between performance and parameter\nefficiency. Empirical results demonstrate the effectiveness of our approach:\nacross three datasets, our models achieve an average of 88% of the original\nmodel's performance while pruning more than 95% of the non-embedding\nparameters. This underscores the potential of our method to significantly\nreduce resource requirements without greatly compromising recommendation\nquality. Our code will be available at: https://github.com/zheng-sl/PruneRec",
    "code_links": [
      "https://github.com/zheng-sl/PruneRec"
    ],
    "comment": null
  },
  {
    "title": "CDC: Causal Domain Clustering for Multi-Domain Recommendation",
    "authors": "Huishi Luo, Yiqing Wu, Yiwen Chen, Fuzhen Zhuang, Deqing Wang",
    "published": "2025-07-09",
    "arxiv_id": "2507.06877v1",
    "url": "http://arxiv.org/abs/2507.06877v1",
    "pdf_url": "http://arxiv.org/pdf/2507.06877v1",
    "category": "information_retrieval",
    "primary_category": "cs.IR",
    "abstract": "Multi-domain recommendation leverages domain-general knowledge to improve\nrecommendations across several domains. However, as platforms expand to dozens\nor hundreds of scenarios, training all domains in a unified model leads to\nperformance degradation due to significant inter-domain differences. Existing\ndomain grouping methods, based on business logic or data similarities, often\nfail to capture the true transfer relationships required for optimal grouping.\nTo effectively cluster domains, we propose Causal Domain Clustering (CDC). CDC\nmodels domain transfer patterns within a large number of domains using two\ndistinct effects: the Isolated Domain Affinity Matrix for modeling\nnon-interactive domain transfers, and the Hybrid Domain Affinity Matrix for\nconsidering dynamic domain synergy or interference under joint training. To\nintegrate these two transfer effects, we introduce causal discovery to\ncalculate a cohesion-based coefficient that adaptively balances their\ncontributions. A Co-Optimized Dynamic Clustering algorithm iteratively\noptimizes target domain clustering and source domain selection for training.\nCDC significantly enhances performance across over 50 domains on public\ndatasets and in industrial settings, achieving a 4.9% increase in online eCPM.\nCode is available at\nhttps://github.com/Chrissie-Law/Causal-Domain-Clustering-for-Multi-Domain-Recommendation",
    "code_links": [
      "https://github.com/Chrissie-Law/Causal-Domain-Clustering-for-Multi-Domain-Recommendation"
    ],
    "comment": "Accepted at SIGIR 2025"
  },
  {
    "title": "Shifting from Ranking to Set Selection for Retrieval Augmented Generation",
    "authors": "Dahyun Lee, Yongrae Jo, Haeju Park, Moontae Lee",
    "published": "2025-07-09",
    "arxiv_id": "2507.06838v2",
    "url": "http://arxiv.org/abs/2507.06838v2",
    "pdf_url": "http://arxiv.org/pdf/2507.06838v2",
    "category": "information_retrieval",
    "primary_category": "cs.CL",
    "abstract": "Retrieval in Retrieval-Augmented Generation(RAG) must ensure that retrieved\npassages are not only individually relevant but also collectively form a\ncomprehensive set. Existing approaches primarily rerank top-k passages based on\ntheir individual relevance, often failing to meet the information needs of\ncomplex queries in multi-hop question answering. In this work, we propose a\nset-wise passage selection approach and introduce SETR, which explicitly\nidentifies the information requirements of a query through Chain-of-Thought\nreasoning and selects an optimal set of passages that collectively satisfy\nthose requirements. Experiments on multi-hop RAG benchmarks show that SETR\noutperforms both proprietary LLM-based rerankers and open-source baselines in\nterms of answer correctness and retrieval quality, providing an effective and\nefficient alternative to traditional rerankers in RAG systems. The code is\navailable at https://github.com/LGAI-Research/SetR",
    "code_links": [
      "https://github.com/LGAI-Research/SetR"
    ],
    "comment": "Accepted to ACL 2025 main (Oral Presentation)"
  },
  {
    "title": "Temporal Information Retrieval via Time-Specifier Model Merging",
    "authors": "SeungYoon Han, Taeho Hwang, Sukmin Cho, Soyeong Jeong, Hoyun Song, Huije Lee, Jong C. Park",
    "published": "2025-07-09",
    "arxiv_id": "2507.06782v1",
    "url": "http://arxiv.org/abs/2507.06782v1",
    "pdf_url": "http://arxiv.org/pdf/2507.06782v1",
    "category": "information_retrieval",
    "primary_category": "cs.IR",
    "abstract": "The rapid expansion of digital information and knowledge across structured\nand unstructured sources has heightened the importance of Information Retrieval\n(IR). While dense retrieval methods have substantially improved semantic\nmatching for general queries, they consistently underperform on queries with\nexplicit temporal constraints--often those containing numerical expressions and\ntime specifiers such as ``in 2015.'' Existing approaches to Temporal\nInformation Retrieval (TIR) improve temporal reasoning but often suffer from\ncatastrophic forgetting, leading to reduced performance on non-temporal\nqueries. To address this, we propose Time-Specifier Model Merging (TSM), a\nnovel method that enhances temporal retrieval while preserving accuracy on\nnon-temporal queries. TSM trains specialized retrievers for individual time\nspecifiers and merges them in to a unified model, enabling precise handling of\ntemporal constraints without compromising non-temporal retrieval. Extensive\nexperiments on both temporal and non-temporal datasets demonstrate that TSM\nsignificantly improves performance on temporally constrained queries while\nmaintaining strong results on non-temporal queries, consistently outperforming\nother baseline methods. Our code is available at\nhttps://github.com/seungyoonee/TSM .",
    "code_links": [
      "https://github.com/seungyoonee/TSM"
    ],
    "comment": null
  },
  {
    "title": "MS-DPPs: Multi-Source Determinantal Point Processes for Contextual Diversity Refinement of Composite Attributes in Text to Image Retrieval",
    "authors": "Naoya Sogi, Takashi Shibata, Makoto Terao, Masanori Suganuma, Takayuki Okatani",
    "published": "2025-07-09",
    "arxiv_id": "2507.06654v1",
    "url": "http://arxiv.org/abs/2507.06654v1",
    "pdf_url": "http://arxiv.org/pdf/2507.06654v1",
    "category": "information_retrieval",
    "primary_category": "cs.CV",
    "abstract": "Result diversification (RD) is a crucial technique in Text-to-Image Retrieval\nfor enhancing the efficiency of a practical application. Conventional methods\nfocus solely on increasing the diversity metric of image appearances. However,\nthe diversity metric and its desired value vary depending on the application,\nwhich limits the applications of RD. This paper proposes a novel task called\nCDR-CA (Contextual Diversity Refinement of Composite Attributes). CDR-CA aims\nto refine the diversities of multiple attributes, according to the\napplication's context. To address this task, we propose Multi-Source DPPs, a\nsimple yet strong baseline that extends the Determinantal Point Process (DPP)\nto multi-sources. We model MS-DPP as a single DPP model with a unified\nsimilarity matrix based on a manifold representation. We also introduce Tangent\nNormalization to reflect contexts. Extensive experiments demonstrate the\neffectiveness of the proposed method. Our code is publicly available at\nhttps://github.com/NEC-N-SOGI/msdpp.",
    "code_links": [
      "https://github.com/NEC-N-SOGI/msdpp"
    ],
    "comment": "IJCAI 2025. Code: https://github.com/NEC-N-SOGI/msdpp"
  },
  {
    "title": "DS@GT at CheckThat! 2025: Exploring Retrieval and Reranking Pipelines for Scientific Claim Source Retrieval on Social Media Discourse",
    "authors": "Jeanette Schofield, Shuyu Tian, Hoang Thanh Thanh Truong, Maximilian Heil",
    "published": "2025-07-09",
    "arxiv_id": "2507.06563v1",
    "url": "http://arxiv.org/abs/2507.06563v1",
    "pdf_url": "http://arxiv.org/pdf/2507.06563v1",
    "category": "information_retrieval",
    "primary_category": "cs.IR",
    "abstract": "Social media users often make scientific claims without citing where these\nclaims come from, generating a need to verify these claims. This paper details\nwork done by the DS@GT team for CLEF 2025 CheckThat! Lab Task 4b Scientific\nClaim Source Retrieval which seeks to find relevant scientific papers based on\nimplicit references in tweets. Our team explored 6 different data augmentation\ntechniques, 7 different retrieval and reranking pipelines, and finetuned a\nbi-encoder. Achieving an MRR@5 of 0.58, our team ranked 16th out of 30 teams\nfor the CLEF 2025 CheckThat! Lab Task 4b, and improvement of 0.15 over the BM25\nbaseline of 0.43. Our code is available on Github at\nhttps://github.com/dsgt-arc/checkthat-2025-swd/tree/main/subtask-4b.",
    "code_links": [
      "https://github.com/dsgt-arc/checkthat-2025-swd"
    ],
    "comment": null
  },
  {
    "title": "Unconditional Diffusion for Generative Sequential Recommendation",
    "authors": "Yimeng Bai, Yang Zhang, Sihao Ding, Shaohui Ruan, Han Yao, Danhui Guan, Fuli Feng, Tat-Seng Chua",
    "published": "2025-07-08",
    "arxiv_id": "2507.06121v1",
    "url": "http://arxiv.org/abs/2507.06121v1",
    "pdf_url": "http://arxiv.org/pdf/2507.06121v1",
    "category": "information_retrieval",
    "primary_category": "cs.IR",
    "abstract": "Diffusion models, known for their generative ability to simulate data\ncreation through noise-adding and denoising processes, have emerged as a\npromising approach for building generative recommenders. To incorporate user\nhistory for personalization, existing methods typically adopt a conditional\ndiffusion framework, where the reverse denoising process of reconstructing\nitems from noise is modified to be conditioned on the user history. However,\nthis design may fail to fully utilize historical information, as it gets\ndistracted by the need to model the \"item $\\leftrightarrow$ noise\" translation.\nThis motivates us to reformulate the diffusion process for sequential\nrecommendation in an unconditional manner, treating user history (instead of\nnoise) as the endpoint of the forward diffusion process (i.e., the starting\npoint of the reverse process), rather than as a conditional input. This\nformulation allows for exclusive focus on modeling the \"item $\\leftrightarrow$\nhistory\" translation. To this end, we introduce Brownian Bridge Diffusion\nRecommendation (BBDRec). By leveraging a Brownian bridge process, BBDRec\nenforces a structured noise addition and denoising mechanism, ensuring that the\ntrajectories are constrained towards a specific endpoint -- user history,\nrather than noise. Extensive experiments demonstrate BBDRec's effectiveness in\nenhancing sequential recommendation performance. The source code is available\nat https://github.com/baiyimeng/BBDRec.",
    "code_links": [
      "https://github.com/baiyimeng/BBDRec"
    ],
    "comment": null
  },
  {
    "title": "Tile-Based ViT Inference with Visual-Cluster Priors for Zero-Shot Multi-Species Plant Identification",
    "authors": "Murilo Gustineli, Anthony Miyaguchi, Adrian Cheung, Divyansh Khattak",
    "published": "2025-07-08",
    "arxiv_id": "2507.06093v1",
    "url": "http://arxiv.org/abs/2507.06093v1",
    "pdf_url": "http://arxiv.org/pdf/2507.06093v1",
    "category": "information_retrieval",
    "primary_category": "cs.CV",
    "abstract": "We describe DS@GT's second-place solution to the PlantCLEF 2025 challenge on\nmulti-species plant identification in vegetation quadrat images. Our pipeline\ncombines (i) a fine-tuned Vision Transformer ViTD2PC24All for patch-level\ninference, (ii) a 4x4 tiling strategy that aligns patch size with the network's\n518x518 receptive field, and (iii) domain-prior adaptation through PaCMAP +\nK-Means visual clustering and geolocation filtering. Tile predictions are\naggregated by majority vote and re-weighted with cluster-specific Bayesian\npriors, yielding a macro-averaged F1 of 0.348 (private leaderboard) while\nrequiring no additional training. All code, configuration files, and\nreproducibility scripts are publicly available at\nhttps://github.com/dsgt-arc/plantclef-2025.",
    "code_links": [
      "https://github.com/dsgt-arc/plantclef-2025"
    ],
    "comment": null
  },
  {
    "title": "When Transformers Meet Recommenders: Integrating Self-Attentive Sequential Recommendation with Fine-Tuned LLMs",
    "authors": "Kechen Liu",
    "published": "2025-07-08",
    "arxiv_id": "2507.05733v1",
    "url": "http://arxiv.org/abs/2507.05733v1",
    "pdf_url": "http://arxiv.org/pdf/2507.05733v1",
    "category": "information_retrieval",
    "primary_category": "cs.IR",
    "abstract": "Self-Attentive Sequential Recommendation (SASRec) effectively captures\nlong-term user preferences by applying attention mechanisms to historical\ninteractions. Concurrently, the rise of Large Language Models (LLMs) has\nmotivated research into LLM-based recommendation, which leverages their\npowerful generalization and language understanding capabilities. However, LLMs\noften lack the domain-specific knowledge and collaborative signals essential\nfor high-quality recommendations when relying solely on textual prompts. To\naddress this limitation, this study proposes SASRecLLM, a novel framework that\nintegrates SASRec as a collaborative encoder with an LLM fine-tuned using\nLow-Rank Adaptation (LoRA). The components are connected via a mapping layer to\nalign their dimensional spaces, and three targeted training strategies are\ndesigned to optimize the hybrid architecture. Extensive experiments on multiple\ndatasets demonstrate that SASRecLLM achieves robust and consistent improvements\nover strong baselines in both cold-start and warm-start scenarios. This work\nadvances the field of LLM-based recommendation by presenting a modular and\neffective paradigm for fusing structured collaborative filtering with the\nsemantic power of fine-tuned LLMs. The implementation is available on GitHub:\nhttps://github.com/kechenkristin/RecLLM",
    "code_links": [
      "https://github.com/kechenkristin/RecLLM"
    ],
    "comment": null
  },
  {
    "title": "From ID-based to ID-free: Rethinking ID Effectiveness in Multimodal Collaborative Filtering Recommendation",
    "authors": "Guohao Li, Li Jing, Jia Wu, Xuefei Li, Kai Zhu, Yue He",
    "published": "2025-07-08",
    "arxiv_id": "2507.05715v1",
    "url": "http://arxiv.org/abs/2507.05715v1",
    "pdf_url": "http://arxiv.org/pdf/2507.05715v1",
    "category": "information_retrieval",
    "primary_category": "cs.IR",
    "abstract": "Most existing multimodal collaborative filtering recommendation (MCFRec)\nmethods rely heavily on ID features and multimodal content to enhance\nrecommendation performance. However, this paper reveals that ID features are\neffective but have limited benefits in multimodal collaborative filtering\nrecommendation. Therefore, this paper systematically deconstruct the pros and\ncons of ID features: (i) they provide initial embedding but lack semantic\nrichness, (ii) they provide a unique identifier for each user and item but\nhinder generalization to untrained data, and (iii) they assist in aligning and\nfusing multimodal features but may lead to representation shift. Based on these\ninsights, this paper proposes IDFREE, an ID-free multimodal collaborative\nFiltering REcommEndation baseline. IDFREE replaces ID features with multimodal\nfeatures and positional encodings to generate semantically meaningful ID-free\nembeddings. For ID-free multimodal collaborative filtering, it further proposes\nan adaptive similarity graph module to construct dynamic user-user and\nitem-item graphs based on multimodal features. Then, an augmented user-item\ngraph encoder is proposed to construct more effective user and item encoding.\nFinally, IDFREE achieves inter-multimodal alignment based on the contrastive\nlearning and uses Softmax loss as recommendation loss. Basic experiments on\nthree public datasets demonstrate that IDFREE outperforms existing ID-based\nMCFRec methods, achieving an average performance gain of 72.24% across standard\nmetrics (Recall@5, 10, 20, 50 and NDCG@5, 10, 20, 50). Exploratory and extended\nexperiments further validate our findings on the limitations of ID features in\nMCFRec. The code is released at https://github.com/G-H-Li/IDFREE.",
    "code_links": [
      "https://github.com/G-H-Li/IDFREE"
    ],
    "comment": "ACM MM'25 (Experimental supplementary version)"
  },
  {
    "title": "FindRec: Stein-Guided Entropic Flow for Multi-Modal Sequential Recommendation",
    "authors": "Maolin Wang, Yutian Xiao, Binhao Wang, Sheng Zhang, Shanshan Ye, Wanyu Wang, Hongzhi Yin, Ruocheng Guo, Zenglin Xu",
    "published": "2025-07-07",
    "arxiv_id": "2507.04651v1",
    "url": "http://arxiv.org/abs/2507.04651v1",
    "pdf_url": "http://arxiv.org/pdf/2507.04651v1",
    "category": "information_retrieval",
    "primary_category": "cs.IR",
    "abstract": "Modern recommendation systems face significant challenges in processing\nmultimodal sequential data, particularly in temporal dynamics modeling and\ninformation flow coordination. Traditional approaches struggle with\ndistribution discrepancies between heterogeneous features and noise\ninterference in multimodal signals. We propose \\textbf{FindRec}~\n(\\textbf{F}lexible unified \\textbf{in}formation \\textbf{d}isentanglement for\nmulti-modal sequential \\textbf{Rec}ommendation), introducing a novel\n\"information flow-control-output\" paradigm. The framework features two key\ninnovations: (1) A Stein kernel-based Integrated Information Coordination\nModule (IICM) that theoretically guarantees distribution consistency between\nmultimodal features and ID streams, and (2) A cross-modal expert routing\nmechanism that adaptively filters and combines multimodal features based on\ntheir contextual relevance. Our approach leverages multi-head subspace\ndecomposition for routing stability and RBF-Stein gradient for unbiased\ndistribution alignment, enhanced by linear-complexity Mamba layers for\nefficient temporal modeling. Extensive experiments on three real-world datasets\ndemonstrate FindRec's superior performance over state-of-the-art baselines,\nparticularly in handling long sequences and noisy multimodal inputs. Our\nframework achieves both improved recommendation accuracy and enhanced model\ninterpretability through its modular design. The implementation code is\navailable anonymously online for easy\nreproducibility~\\footnote{https://github.com/Applied-Machine-Learning-Lab/FindRec}.",
    "code_links": [
      "https://github.com/Applied-Machine-Learning-Lab/FindRec"
    ],
    "comment": "Accepted by KDD 2025"
  },
  {
    "title": "Hierarchical Intent-guided Optimization with Pluggable LLM-Driven Semantics for Session-based Recommendation",
    "authors": "Jinpeng Chen, Jianxiang He, Huan Li, Senzhang Wang, Yuan Cao, Kaimin Wei, Zhenye Yang, Ye Ji",
    "published": "2025-07-07",
    "arxiv_id": "2507.04623v1",
    "url": "http://arxiv.org/abs/2507.04623v1",
    "pdf_url": "http://arxiv.org/pdf/2507.04623v1",
    "category": "information_retrieval",
    "primary_category": "cs.IR",
    "abstract": "Session-based Recommendation (SBR) aims to predict the next item a user will\nlikely engage with, using their interaction sequence within an anonymous\nsession. Existing SBR models often focus only on single-session information,\nignoring inter-session relationships and valuable cross-session insights. Some\nmethods try to include inter-session data but struggle with noise and\nirrelevant information, reducing performance. Additionally, most models rely on\nitem ID co-occurrence and overlook rich semantic details, limiting their\nability to capture fine-grained item features. To address these challenges, we\npropose a novel hierarchical intent-guided optimization approach with pluggable\nLLM-driven semantic learning for session-based recommendations, called HIPHOP.\nFirst, we introduce a pluggable embedding module based on large language models\n(LLMs) to generate high-quality semantic representations, enhancing item\nembeddings. Second, HIPHOP utilizes graph neural networks (GNNs) to model item\ntransition relationships and incorporates a dynamic multi-intent capturing\nmodule to address users' diverse interests within a session. Additionally, we\ndesign a hierarchical inter-session similarity learning module, guided by user\nintent, to capture global and local session relationships, effectively\nexploring users' long-term and short-term interests. To mitigate noise, an\nintent-guided denoising strategy is applied during inter-session learning.\nFinally, we enhance the model's discriminative capability by using contrastive\nlearning to optimize session representations. Experiments on multiple datasets\nshow that HIPHOP significantly outperforms existing methods, demonstrating its\neffectiveness in improving recommendation quality. Our code is available:\nhttps://github.com/hjx159/HIPHOP.",
    "code_links": [
      "https://github.com/hjx159/HIPHOP"
    ],
    "comment": null
  },
  {
    "title": "Decoupled Planning and Execution: A Hierarchical Reasoning Framework for Deep Search",
    "authors": "Jiajie Jin, Xiaoxi Li, Guanting Dong, Yuyao Zhang, Yutao Zhu, Yang Zhao, Hongjin Qian, Zhicheng Dou",
    "published": "2025-07-03",
    "arxiv_id": "2507.02652v1",
    "url": "http://arxiv.org/abs/2507.02652v1",
    "pdf_url": "http://arxiv.org/pdf/2507.02652v1",
    "category": "information_retrieval",
    "primary_category": "cs.AI",
    "abstract": "Complex information needs in real-world search scenarios demand deep\nreasoning and knowledge synthesis across diverse sources, which traditional\nretrieval-augmented generation (RAG) pipelines struggle to address effectively.\nCurrent reasoning-based approaches suffer from a fundamental limitation: they\nuse a single model to handle both high-level planning and detailed execution,\nleading to inefficient reasoning and limited scalability. In this paper, we\nintroduce HiRA, a hierarchical framework that separates strategic planning from\nspecialized execution. Our approach decomposes complex search tasks into\nfocused subtasks, assigns each subtask to domain-specific agents equipped with\nexternal tools and reasoning capabilities, and coordinates the results through\na structured integration mechanism. This separation prevents execution details\nfrom disrupting high-level reasoning while enabling the system to leverage\nspecialized expertise for different types of information processing.\nExperiments on four complex, cross-modal deep search benchmarks demonstrate\nthat HiRA significantly outperforms state-of-the-art RAG and agent-based\nsystems. Our results show improvements in both answer quality and system\nefficiency, highlighting the effectiveness of decoupled planning and execution\nfor multi-step information seeking tasks. Our code is available at\nhttps://github.com/ignorejjj/HiRA.",
    "code_links": [
      "https://github.com/ignorejjj/HiRA"
    ],
    "comment": "9 pages"
  },
  {
    "title": "Listwise Preference Alignment Optimization for Tail Item Recommendation",
    "authors": "Zihao Li, Chao Yang, Tong Zhang, Yakun Chen, Xianzhi Wang, Guandong Xu, Daoyi Dong",
    "published": "2025-07-03",
    "arxiv_id": "2507.02255v1",
    "url": "http://arxiv.org/abs/2507.02255v1",
    "pdf_url": "http://arxiv.org/pdf/2507.02255v1",
    "category": "information_retrieval",
    "primary_category": "cs.IR",
    "abstract": "Preference alignment has achieved greater success on Large Language Models\n(LLMs) and drawn broad interest in recommendation research. Existing preference\nalignment methods for recommendation either require explicit reward modeling or\nonly support pairwise preference comparison. The former directly increases\nsubstantial computational costs, while the latter hinders training efficiency\non negative samples. Moreover, no existing effort has explored preference\nalignment solutions for tail-item recommendation. To bridge the above gaps, we\npropose LPO4Rec, which extends the Bradley-Terry model from pairwise comparison\nto listwise comparison, to improve the efficiency of model training.\nSpecifically, we derive a closed form optimal policy to enable more efficient\nand effective training without explicit reward modeling. We also present an\nadaptive negative sampling and reweighting strategy to prioritize tail items\nduring optimization and enhance performance in tail-item recommendations.\nBesides, we theoretically prove that optimizing the listwise preference\noptimization (LPO) loss is equivalent to maximizing the upper bound of the\noptimal reward. Our experiments on three public datasets show that our method\noutperforms 10 baselines by a large margin, achieving up to 50% performance\nimprovement while reducing 17.9% GPU memory usage when compared with direct\npreference optimization (DPO) in tail-item recommendation. Our code is\navailable at https://github.com/Yuhanleeee/LPO4Rec.",
    "code_links": [
      "https://github.com/Yuhanleeee/LPO4Rec"
    ],
    "comment": null
  },
  {
    "title": "Confidence and Stability of Global and Pairwise Scores in NLP Evaluation",
    "authors": "Georgii Levtsov, Dmitry Ustalov",
    "published": "2025-07-02",
    "arxiv_id": "2507.01633v1",
    "url": "http://arxiv.org/abs/2507.01633v1",
    "pdf_url": "http://arxiv.org/pdf/2507.01633v1",
    "category": "information_retrieval",
    "primary_category": "cs.CL",
    "abstract": "With the advent of highly capable instruction-tuned neural language models,\nbenchmarking in natural language processing (NLP) is increasingly shifting\ntowards pairwise comparison leaderboards, such as LMSYS Arena, from traditional\nglobal pointwise scores (e.g., GLUE, BIG-bench, SWE-bench). This paper\nempirically investigates the strengths and weaknesses of both global scores and\npairwise comparisons to aid decision-making in selecting appropriate model\nevaluation strategies. Through computational experiments on synthetic and\nreal-world datasets using standard global metrics and the popular Bradley-Terry\nmodel for pairwise comparisons, we found that while global scores provide more\nreliable overall rankings, they can underestimate strong models with rare,\nsignificant errors or low confidence. Conversely, pairwise comparisons are\nparticularly effective for identifying strong contenders among models with\nlower global scores, especially where quality metrics are hard to define (e.g.,\ntext generation), though they require more comparisons to converge if ties are\nfrequent. Our code and data are available at\nhttps://github.com/HSPyroblast/srw-ranking under a permissive license.",
    "code_links": [
      "https://github.com/HSPyroblast/srw-ranking"
    ],
    "comment": "8 pages, accepted at ACL SRW 2025"
  },
  {
    "title": "PDFMathTranslate: Scientific Document Translation Preserving Layouts",
    "authors": "Rongxin Ouyang, Chang Chu, Zhikuang Xin, Xiangyao Ma",
    "published": "2025-07-02",
    "arxiv_id": "2507.03009v2",
    "url": "http://arxiv.org/abs/2507.03009v2",
    "pdf_url": "http://arxiv.org/pdf/2507.03009v2",
    "category": "information_retrieval",
    "primary_category": "cs.CL",
    "abstract": "Language barriers in scientific documents hinder the diffusion and\ndevelopment of science and technologies. However, prior efforts in translating\nsuch documents largely overlooked the information in layouts. To bridge the\ngap, we introduce PDFMathTranslate, the world's first open-source software for\ntranslating scientific documents while preserving layouts. Leveraging the most\nrecent advances in large language models and precise layout detection, we\ncontribute to the community with key improvements in precision, flexibility,\nand efficiency. The work has been open-sourced at\nhttps://github.com/byaidu/pdfmathtranslate with more than 222k downloads.",
    "code_links": [
      "https://github.com/byaidu/pdfmathtranslate"
    ],
    "comment": "7 pages, 4 figures"
  }
]