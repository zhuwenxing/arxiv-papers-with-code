[
  {
    "title": "Retrieval-Augmented Generation for Reliable Interpretation of Radio Regulations",
    "authors": "Zakaria El Kassimi, Fares Fourati, Mohamed-Slim Alouini",
    "published": "2025-09-11",
    "arxiv_id": "2509.09651v1",
    "url": "http://arxiv.org/abs/2509.09651v1",
    "pdf_url": "http://arxiv.org/pdf/2509.09651v1",
    "category": "information_retrieval",
    "primary_category": "cs.IR",
    "abstract": "We study question answering in the domain of radio regulations, a legally\nsensitive and high-stakes area. We propose a telecom-specific\nRetrieval-Augmented Generation (RAG) pipeline and introduce, to our knowledge,\nthe first multiple-choice evaluation set for this domain, constructed from\nauthoritative sources using automated filtering and human validation. To assess\nretrieval quality, we define a domain-specific retrieval metric, under which\nour retriever achieves approximately 97% accuracy. Beyond retrieval, our\napproach consistently improves generation accuracy across all tested models. In\nparticular, while naively inserting documents without structured retrieval\nyields only marginal gains for GPT-4o (less than 1%), applying our pipeline\nresults in nearly a 12% relative improvement. These findings demonstrate that\ncarefully targeted grounding provides a simple yet strong baseline and an\neffective domain-specific solution for regulatory question answering. All code\nand evaluation scripts, along with our derived question-answer dataset, are\navailable at https://github.com/Zakaria010/Radio-RAG.",
    "code_links": [
      "https://github.com/Zakaria010/Radio-RAG"
    ],
    "comment": null
  },
  {
    "title": "Modality Alignment with Multi-scale Bilateral Attention for Multimodal Recommendation",
    "authors": "Kelin Ren, Chan-Yang Ju, Dong-Ho Lee",
    "published": "2025-09-11",
    "arxiv_id": "2509.09114v1",
    "url": "http://arxiv.org/abs/2509.09114v1",
    "pdf_url": "http://arxiv.org/pdf/2509.09114v1",
    "category": "information_retrieval",
    "primary_category": "cs.IR",
    "abstract": "Multimodal recommendation systems are increasingly becoming foundational\ntechnologies for e-commerce and content platforms, enabling personalized\nservices by jointly modeling users' historical behaviors and the multimodal\nfeatures of items (e.g., visual and textual). However, most existing methods\nrely on either static fusion strategies or graph-based local interaction\nmodeling, facing two critical limitations: (1) insufficient ability to model\nfine-grained cross-modal associations, leading to suboptimal fusion quality;\nand (2) a lack of global distribution-level consistency, causing\nrepresentational bias. To address these, we propose MambaRec, a novel framework\nthat integrates local feature alignment and global distribution regularization\nvia attention-guided learning. At its core, we introduce the Dilated Refinement\nAttention Module (DREAM), which uses multi-scale dilated convolutions with\nchannel-wise and spatial attention to align fine-grained semantic patterns\nbetween visual and textual modalities. This module captures hierarchical\nrelationships and context-aware associations, improving cross-modal semantic\nmodeling. Additionally, we apply Maximum Mean Discrepancy (MMD) and contrastive\nloss functions to constrain global modality alignment, enhancing semantic\nconsistency. This dual regularization reduces mode-specific deviations and\nboosts robustness. To improve scalability, MambaRec employs a dimensionality\nreduction strategy to lower the computational cost of high-dimensional\nmultimodal features. Extensive experiments on real-world e-commerce datasets\nshow that MambaRec outperforms existing methods in fusion quality,\ngeneralization, and efficiency. Our code has been made publicly available at\nhttps://github.com/rkl71/MambaRec.",
    "code_links": [
      "https://github.com/rkl71/MambaRec"
    ],
    "comment": "Accepted by CIKM 2025"
  },
  {
    "title": "SciNLP: A Domain-Specific Benchmark for Full-Text Scientific Entity and Relation Extraction in NLP",
    "authors": "Decheng Duan, Yingyi Zhang, Jitong Peng, Chengzhi Zhang",
    "published": "2025-09-09",
    "arxiv_id": "2509.07801v2",
    "url": "http://arxiv.org/abs/2509.07801v2",
    "pdf_url": "http://arxiv.org/pdf/2509.07801v2",
    "category": "information_retrieval",
    "primary_category": "cs.CL",
    "abstract": "Structured information extraction from scientific literature is crucial for\ncapturing core concepts and emerging trends in specialized fields. While\nexisting datasets aid model development, most focus on specific publication\nsections due to domain complexity and the high cost of annotating scientific\ntexts. To address this limitation, we introduce SciNLP - a specialized\nbenchmark for full-text entity and relation extraction in the Natural Language\nProcessing (NLP) domain. The dataset comprises 60 manually annotated full-text\nNLP publications, covering 7,072 entities and 1,826 relations. Compared to\nexisting research, SciNLP is the first dataset providing full-text annotations\nof entities and their relationships in the NLP domain. To validate the\neffectiveness of SciNLP, we conducted comparative experiments with similar\ndatasets and evaluated the performance of state-of-the-art supervised models on\nthis dataset. Results reveal varying extraction capabilities of existing models\nacross academic texts of different lengths. Cross-comparisons with existing\ndatasets show that SciNLP achieves significant performance improvements on\ncertain baseline models. Using models trained on SciNLP, we implemented\nautomatic construction of a fine-grained knowledge graph for the NLP domain.\nOur KG has an average node degree of 3.2 per entity, indicating rich semantic\ntopological information that enhances downstream applications. The dataset is\npublicly available at https://github.com/AKADDC/SciNLP.",
    "code_links": [
      "https://github.com/AKADDC/SciNLP"
    ],
    "comment": "EMNLP 2025 Main"
  },
  {
    "title": "Multi-view-guided Passage Reranking with Large Language Models",
    "authors": "Jeongwoo Na, Jun Kwon, Eunseong Choi, Jongwuk Lee",
    "published": "2025-09-09",
    "arxiv_id": "2509.07485v1",
    "url": "http://arxiv.org/abs/2509.07485v1",
    "pdf_url": "http://arxiv.org/pdf/2509.07485v1",
    "category": "information_retrieval",
    "primary_category": "cs.IR",
    "abstract": "Recent advances in large language models (LLMs) have shown impressive\nperformance in passage reranking tasks. Despite their success, LLM-based\nmethods still face challenges in efficiency and sensitivity to external biases.\n(1) Existing models rely mostly on autoregressive generation and sliding window\nstrategies to rank passages, which incur heavy computational overhead as the\nnumber of passages increases. (2) External biases, such as position or\nselection bias, hinder the model's ability to accurately represent passages and\nincrease input-order sensitivity. To address these limitations, we introduce a\nnovel passage reranking model, called Multi-View-guided Passage Reranking\n(MVP). MVP is a non-generative LLM-based reranking method that encodes\nquery-passage information into diverse view embeddings without being influenced\nby external biases. For each view, it combines query-aware passage embeddings\nto produce a distinct anchor vector, which is then used to directly compute\nrelevance scores in a single decoding step. In addition, it employs an\northogonal loss to make the views more distinctive. Extensive experiments\ndemonstrate that MVP, with just 220M parameters, matches the performance of\nmuch larger 7B-scale fine-tuned models while achieving a 100x reduction in\ninference latency. Notably, the 3B-parameter variant of MVP achieves\nstate-of-the-art performance on both in-domain and out-of-domain benchmarks.\nThe source code is available at: https://github.com/bulbna/MVP",
    "code_links": [
      "https://github.com/bulbna/MVP"
    ],
    "comment": null
  },
  {
    "title": "MoLoRAG: Bootstrapping Document Understanding via Multi-modal Logic-aware Retrieval",
    "authors": "Xixi Wu, Yanchao Tan, Nan Hou, Ruiyang Zhang, Hong Cheng",
    "published": "2025-09-06",
    "arxiv_id": "2509.07666v1",
    "url": "http://arxiv.org/abs/2509.07666v1",
    "pdf_url": "http://arxiv.org/pdf/2509.07666v1",
    "category": "information_retrieval",
    "primary_category": "cs.CL",
    "abstract": "Document Understanding is a foundational AI capability with broad\napplications, and Document Question Answering (DocQA) is a key evaluation task.\nTraditional methods convert the document into text for processing by Large\nLanguage Models (LLMs), but this process strips away critical multi-modal\ninformation like figures. While Large Vision-Language Models (LVLMs) address\nthis limitation, their constrained input size makes multi-page document\ncomprehension infeasible. Retrieval-augmented generation (RAG) methods mitigate\nthis by selecting relevant pages, but they rely solely on semantic relevance,\nignoring logical connections between pages and the query, which is essential\nfor reasoning.\n  To this end, we propose MoLoRAG, a logic-aware retrieval framework for\nmulti-modal, multi-page document understanding. By constructing a page graph\nthat captures contextual relationships between pages, a lightweight VLM\nperforms graph traversal to retrieve relevant pages, including those with\nlogical connections often overlooked. This approach combines semantic and\nlogical relevance to deliver more accurate retrieval. After retrieval, the\ntop-$K$ pages are fed into arbitrary LVLMs for question answering. To enhance\nflexibility, MoLoRAG offers two variants: a training-free solution for easy\ndeployment and a fine-tuned version to improve logical relevance checking.\nExperiments on four DocQA datasets demonstrate average improvements of 9.68% in\naccuracy over LVLM direct inference and 7.44% in retrieval precision over\nbaselines. Codes and datasets are released at\nhttps://github.com/WxxShirley/MoLoRAG.",
    "code_links": [
      "https://github.com/WxxShirley/MoLoRAG"
    ],
    "comment": "EMNLP Main 2025"
  },
  {
    "title": "REMOTE: A Unified Multimodal Relation Extraction Framework with Multilevel Optimal Transport and Mixture-of-Experts",
    "authors": "Xinkui Lin, Yongxiu Xu, Minghao Tang, Shilong Zhang, Hongbo Xu, Hao Xu, Yubin Wang",
    "published": "2025-09-05",
    "arxiv_id": "2509.04844v1",
    "url": "http://arxiv.org/abs/2509.04844v1",
    "pdf_url": "http://arxiv.org/pdf/2509.04844v1",
    "category": "information_retrieval",
    "primary_category": "cs.MM",
    "abstract": "Multimodal relation extraction (MRE) is a crucial task in the fields of\nKnowledge Graph and Multimedia, playing a pivotal role in multimodal knowledge\ngraph construction. However, existing methods are typically limited to\nextracting a single type of relational triplet, which restricts their ability\nto extract triplets beyond the specified types. Directly combining these\nmethods fails to capture dynamic cross-modal interactions and introduces\nsignificant computational redundancy. Therefore, we propose a novel\n\\textit{unified multimodal Relation Extraction framework with Multilevel\nOptimal Transport and mixture-of-Experts}, termed REMOTE, which can\nsimultaneously extract intra-modal and inter-modal relations between textual\nentities and visual objects. To dynamically select optimal interaction features\nfor different types of relational triplets, we introduce mixture-of-experts\nmechanism, ensuring the most relevant modality information is utilized.\nAdditionally, considering that the inherent property of multilayer sequential\nencoding in existing encoders often leads to the loss of low-level information,\nwe adopt a multilevel optimal transport fusion module to preserve low-level\nfeatures while maintaining multilayer encoding, yielding more expressive\nrepresentations. Correspondingly, we also create a Unified Multimodal Relation\nExtraction (UMRE) dataset to evaluate the effectiveness of our framework,\nencompassing diverse cases where the head and tail entities can originate from\neither text or image. Extensive experiments show that REMOTE effectively\nextracts various types of relational triplets and achieves state-of-the-art\nperformanc on almost all metrics across two other public MRE datasets. We\nrelease our resources at https://github.com/Nikol-coder/REMOTE.",
    "code_links": [
      "https://github.com/Nikol-coder/REMOTE"
    ],
    "comment": "ACM MM 2025"
  },
  {
    "title": "Delta Activations: A Representation for Finetuned Large Language Models",
    "authors": "Zhiqiu Xu, Amish Sethi, Mayur Naik, Ser-Nam Lim",
    "published": "2025-09-04",
    "arxiv_id": "2509.04442v1",
    "url": "http://arxiv.org/abs/2509.04442v1",
    "pdf_url": "http://arxiv.org/pdf/2509.04442v1",
    "category": "information_retrieval",
    "primary_category": "cs.LG",
    "abstract": "The success of powerful open source Large Language Models (LLMs) has enabled\nthe community to create a vast collection of post-trained models adapted to\nspecific tasks and domains. However, navigating and understanding these models\nremains challenging due to inconsistent metadata and unstructured repositories.\nWe introduce Delta Activations, a method to represent finetuned models as\nvector embeddings by measuring shifts in their internal activations relative to\na base model. This representation allows for effective clustering by domain and\ntask, revealing structure in the model landscape. Delta Activations also\ndemonstrate desirable properties: it is robust across finetuning settings and\nexhibits an additive property when finetuning datasets are mixed. In addition,\nwe show that Delta Activations can embed tasks via few-shot finetuning, and\nfurther explore its use for model selection and merging. We hope Delta\nActivations can facilitate the practice of reusing publicly available models.\nCode is available at https://github.com/OscarXZQ/delta_activations.",
    "code_links": [
      "https://github.com/OscarXZQ/delta_activations"
    ],
    "comment": null
  },
  {
    "title": "NER Retriever: Zero-Shot Named Entity Retrieval with Type-Aware Embeddings",
    "authors": "Or Shachar, Uri Katz, Yoav Goldberg, Oren Glickman",
    "published": "2025-09-04",
    "arxiv_id": "2509.04011v1",
    "url": "http://arxiv.org/abs/2509.04011v1",
    "pdf_url": "http://arxiv.org/pdf/2509.04011v1",
    "category": "information_retrieval",
    "primary_category": "cs.IR",
    "abstract": "We present NER Retriever, a zero-shot retrieval framework for ad-hoc Named\nEntity Retrieval, a variant of Named Entity Recognition (NER), where the types\nof interest are not provided in advance, and a user-defined type description is\nused to retrieve documents mentioning entities of that type. Instead of relying\non fixed schemas or fine-tuned models, our method builds on internal\nrepresentations of large language models (LLMs) to embed both entity mentions\nand user-provided open-ended type descriptions into a shared semantic space. We\nshow that internal representations, specifically the value vectors from\nmid-layer transformer blocks, encode fine-grained type information more\neffectively than commonly used top-layer embeddings. To refine these\nrepresentations, we train a lightweight contrastive projection network that\naligns type-compatible entities while separating unrelated types. The resulting\nentity embeddings are compact, type-aware, and well-suited for nearest-neighbor\nsearch. Evaluated on three benchmarks, NER Retriever significantly outperforms\nboth lexical and dense sentence-level retrieval baselines. Our findings provide\nempirical support for representation selection within LLMs and demonstrate a\npractical solution for scalable, schema-free entity retrieval. The NER\nRetriever Codebase is publicly available at\nhttps://github.com/ShacharOr100/ner_retriever",
    "code_links": [
      "https://github.com/ShacharOr100/ner_retriever"
    ],
    "comment": "Findings of EMNLP 2025"
  },
  {
    "title": "Evaluating the Robustness of Retrieval-Augmented Generation to Adversarial Evidence in the Health Domain",
    "authors": "Shakiba Amirshahi, Amin Bigdeli, Charles L. A. Clarke, Amira Ghenai",
    "published": "2025-09-04",
    "arxiv_id": "2509.03787v1",
    "url": "http://arxiv.org/abs/2509.03787v1",
    "pdf_url": "http://arxiv.org/pdf/2509.03787v1",
    "category": "information_retrieval",
    "primary_category": "cs.IR",
    "abstract": "Retrieval augmented generation (RAG) systems provide a method for factually\ngrounding the responses of a Large Language Model (LLM) by providing retrieved\nevidence, or context, as support. Guided by this context, RAG systems can\nreduce hallucinations and expand the ability of LLMs to accurately answer\nquestions outside the scope of their training data. Unfortunately, this design\nintroduces a critical vulnerability: LLMs may absorb and reproduce\nmisinformation present in retrieved evidence. This problem is magnified if\nretrieved evidence contains adversarial material explicitly intended to\npromulgate misinformation. This paper presents a systematic evaluation of RAG\nrobustness in the health domain and examines alignment between model outputs\nand ground-truth answers. We focus on the health domain due to the potential\nfor harm caused by incorrect responses, as well as the availability of\nevidence-based ground truth for many common health-related questions. We\nconduct controlled experiments using common health questions, varying both the\ntype and composition of the retrieved documents (helpful, harmful, and\nadversarial) as well as the framing of the question by the user (consistent,\nneutral, and inconsistent). Our findings reveal that adversarial documents\nsubstantially degrade alignment, but robustness can be preserved when helpful\nevidence is also present in the retrieval pool. These findings offer actionable\ninsights for designing safer RAG systems in high-stakes domains by highlighting\nthe need for retrieval safeguards. To enable reproducibility and facilitate\nfuture research, all experimental results are publicly available in our github\nrepository.\n  https://github.com/shakibaam/RAG_ROBUSTNESS_EVAL",
    "code_links": [
      "https://github.com/shakibaam/RAG_ROBUSTNESS_EVAL"
    ],
    "comment": null
  },
  {
    "title": "Upcycling Candidate Tokens of Large Language Models for Query Expansion",
    "authors": "Jinseok Kim, Sukmin Cho, Soyeong Jeong, Sangyeop Kim, Sungzoon Cho",
    "published": "2025-09-02",
    "arxiv_id": "2509.02377v1",
    "url": "http://arxiv.org/abs/2509.02377v1",
    "pdf_url": "http://arxiv.org/pdf/2509.02377v1",
    "category": "information_retrieval",
    "primary_category": "cs.IR",
    "abstract": "Query Expansion (QE) improves retrieval performance by enriching queries with\nrelated terms. Recently, Large Language Models (LLMs) have been used for QE,\nbut existing methods face a trade-off: generating diverse terms boosts\nperformance but increases computational cost. To address this challenge, we\npropose Candidate Token Query Expansion (CTQE), which extracts diverse and\nrelevant terms from a single LLM decoding pass by leveraging unselected\ncandidate tokens. These tokens, though not part of the final output, are\nconditioned on the full query and capture useful information. By aggregating\nthem, CTQE achieves both relevance and diversity without extra inference,\nreducing overhead and latency. Experiments show that CTQE delivers strong\nretrieval performance with significantly lower cost, outperforming or\ncomparable to more expensive methods. Code is available at:\nhttps://github.com/bluejeans8/CTQE",
    "code_links": [
      "https://github.com/bluejeans8/CTQE"
    ],
    "comment": "CIKM 2025"
  },
  {
    "title": "Abex-rat: Synergizing Abstractive Augmentation and Adversarial Training for Classification of Occupational Accident Reports",
    "authors": "Jian Chen, Jiabao Dou, Jinbao Tian, Yunqi Xu, Zhou Li",
    "published": "2025-09-02",
    "arxiv_id": "2509.02072v2",
    "url": "http://arxiv.org/abs/2509.02072v2",
    "pdf_url": "http://arxiv.org/pdf/2509.02072v2",
    "category": "information_retrieval",
    "primary_category": "cs.LG",
    "abstract": "The automatic classification of occupational accident reports is a critical\nresearch area for enhancing workplace safety and enabling large-scale risk\nanalysis. However, the severe class imbalance inherent in these real-world\ndatasets often compromises the performance of analytical models, particularly\nfor rare but severe incident types, hindering the development of reliable\nautomated systems. To address this challenge, we propose ABEX-RAT, a novel and\nefficient framework that synergizes generative data augmentation with robust\nadversarial training. Our approach first employs a twostep\nabstractive-expansive (ABEX) pipeline, which leverages a large language model\nto distill core incident semantics and then uses a generative model to create\ndiverse, highquality synthetic samples for underrepresented classes.\nSubsequently, a lightweight classifier is trained on the augmented data using a\ncomputationally efficient random adversarial training (RAT) protocol, which\nstochastically applies perturbations to enhance model generalization and\nrobustness without significant overhead. Experimental results on the public\nOSHA dataset demonstrate that our method achieves new state-of-the-art\nperformance, reaching a macro-F1 score of 90.32% and significantly\noutperforming previous SOTA and fine-tuned large model baselines. Our work\nvalidates that this synergistic strategy is a highly effective and efficient\nalternative to brute-force fine-tuning for specialized, imbalanced\nclassification tasks. The code is publicly available\nat:https://github.com/nxcc-lab/ABEX-RAT.",
    "code_links": [
      "https://github.com/nxcc-lab/ABEX-RAT"
    ],
    "comment": null
  },
  {
    "title": "Empowering Large Language Model for Sequential Recommendation via Multimodal Embeddings and Semantic IDs",
    "authors": "Yuhao Wang, Junwei Pan, Xinhang Li, Maolin Wang, Yuan Wang, Yue Liu, Dapeng Liu, Jie Jiang, Xiangyu Zhao",
    "published": "2025-09-02",
    "arxiv_id": "2509.02017v1",
    "url": "http://arxiv.org/abs/2509.02017v1",
    "pdf_url": "http://arxiv.org/pdf/2509.02017v1",
    "category": "information_retrieval",
    "primary_category": "cs.IR",
    "abstract": "Sequential recommendation (SR) aims to capture users' dynamic interests and\nsequential patterns based on their historical interactions. Recently, the\npowerful capabilities of large language models (LLMs) have driven their\nadoption in SR. However, we identify two critical challenges in existing\nLLM-based SR methods: 1) embedding collapse when incorporating pre-trained\ncollaborative embeddings and 2) catastrophic forgetting of quantized embeddings\nwhen utilizing semantic IDs. These issues dampen the model scalability and lead\nto suboptimal recommendation performance. Therefore, based on LLMs like\nLlama3-8B-instruct, we introduce a novel SR framework named MME-SID, which\nintegrates multimodal embeddings and quantized embeddings to mitigate embedding\ncollapse. Additionally, we propose a Multimodal Residual Quantized Variational\nAutoencoder (MM-RQ-VAE) with maximum mean discrepancy as the reconstruction\nloss and contrastive learning for alignment, which effectively preserve\nintra-modal distance information and capture inter-modal correlations,\nrespectively. To further alleviate catastrophic forgetting, we initialize the\nmodel with the trained multimodal code embeddings. Finally, we fine-tune the\nLLM efficiently using LoRA in a multimodal frequency-aware fusion manner.\nExtensive experiments on three public datasets validate the superior\nperformance of MME-SID thanks to its capability to mitigate embedding collapse\nand catastrophic forgetting. The implementation code and datasets are publicly\navailable for reproduction:\nhttps://github.com/Applied-Machine-Learning-Lab/MME-SID.",
    "code_links": [
      "https://github.com/Applied-Machine-Learning-Lab/MME-SID"
    ],
    "comment": "CIKM 2025 Full Research Paper"
  },
  {
    "title": "A Comparative Analysis of Identifier Schemes: UUIDv4, UUIDv7, and ULID for Distributed Systems",
    "authors": "Nima Karimian Kakolaki",
    "published": "2025-09-10",
    "arxiv_id": "2509.08969v1",
    "url": "http://arxiv.org/abs/2509.08969v1",
    "pdf_url": "http://arxiv.org/pdf/2509.08969v1",
    "category": "databases",
    "primary_category": "cs.DC",
    "abstract": "Distributed systems require robust, scalable identifier schemes to ensure\ndata uniqueness and efficient indexing across multiple nodes. This paper\npresents a comprehensive analysis of the evolution of distributed identifiers,\ncomparing traditional auto-increment keys with UUIDv4, UUIDv7, and ULIDs. We\ncombine mathematical calculation of collision probabilities with empirical\nexperiments measuring generation speed and network transmission overhead in a\nsimulated distributed environment. Results demonstrate that ULIDs significantly\noutperform UUIDv4 and UUIDv7, reducing network overhead by 83.7% and increasing\ngeneration speed by 97.32%. statistical analysis further shows ULIDs offer a\n98.42% lower collision risk compared to UUIDv7, while maintaining negligible\ncollision probabilities even at high generation rates. These findings highlight\nULIDs as an optimal choice for high-performance distributed systems, providing\nefficient, time-ordered, and lexicographically sortable identifiers suitable\nfor scalable applications. All source code, datasets, and analysis scripts\nutilized in this research are publicly available in our dedicated repository at\nhttps://github.com/nimakarimiank/uids-comparison. This repository contains\ncomprehensive documentation of the experimental setup, including configuration\nfiles for the distributed environment, producer and consumer implementations,\nand message broker integration. Additionally, it provides the data scripts and\ndatasets. Researchers and practitioners are encouraged to explore the\nrepository for full reproducibility of the experiments and to facilitate\nfurther investigation or extension of the presented work.",
    "code_links": [
      "https://github.com/nimakarimiank/uids-comparison"
    ],
    "comment": null
  },
  {
    "title": "Tiga: Accelerating Geo-Distributed Transactions with Synchronized Clocks [Technical Report]",
    "authors": "Jinkun Geng, Shuai Mu, Anirudh Sivaraman, Balaji Prabhakar",
    "published": "2025-09-06",
    "arxiv_id": "2509.05759v1",
    "url": "http://arxiv.org/abs/2509.05759v1",
    "pdf_url": "http://arxiv.org/pdf/2509.05759v1",
    "category": "databases",
    "primary_category": "cs.NI",
    "abstract": "This paper presents Tiga, a new design for geo-replicated and scalable\ntransactional databases such as Google Spanner. Tiga aims to commit\ntransactions within 1 wide-area roundtrip time, or 1 WRTT, for a wide range of\nscenarios, while maintaining high throughput with minimal computational\noverhead. Tiga consolidates concurrency control and consensus, completing both\nstrictly serializable execution and consistent replication in a single round.\nIt uses synchronized clocks to proactively order transactions by assigning each\na future timestamp at submission. In most cases, transactions arrive at servers\nbefore their future timestamps and are serialized according to the designated\ntimestamp, requiring 1 WRTT to commit. In rare cases, transactions are delayed\nand proactive ordering fails, in which case Tiga falls back to a slow path,\ncommitting in 1.5--2 WRTTs. Compared to state-of-the-art solutions, Tiga can\ncommit more transactions at 1-WRTT latency, and incurs much less throughput\noverhead. Evaluation results show that Tiga outperforms all baselines,\nachieving 1.3--7.2$\\times$ higher throughput and 1.4--4.6$\\times$ lower\nlatency. Tiga is open-sourced at\nhttps://github.com/New-Consensus-Concurrency-Control/Tiga.",
    "code_links": [
      "https://github.com/New-Consensus-Concurrency-Control/Tiga"
    ],
    "comment": "This is the technical report for our paper accepted by The 31st\n  Symposium on Operating Systems Principles (SOSP'25)"
  },
  {
    "title": "Schema Inference for Tabular Data Repositories Using Large Language Models",
    "authors": "Zhenyu Wu, Jiaoyan Chen, Norman W. Paton",
    "published": "2025-09-04",
    "arxiv_id": "2509.04632v1",
    "url": "http://arxiv.org/abs/2509.04632v1",
    "pdf_url": "http://arxiv.org/pdf/2509.04632v1",
    "category": "databases",
    "primary_category": "cs.DB",
    "abstract": "Minimally curated tabular data often contain representational inconsistencies\nacross heterogeneous sources, and are accompanied by sparse metadata. Working\nwith such data is intimidating. While prior work has advanced dataset discovery\nand exploration, schema inference remains difficult when metadata are limited.\nWe present SI-LLM (Schema Inference using Large Language Models), which infers\na concise conceptual schema for tabular data using only column headers and cell\nvalues. The inferred schema comprises hierarchical entity types, attributes,\nand inter-type relationships. In extensive evaluation on two datasets from web\ntables and open data, SI-LLM achieves promising end-to-end results, as well as\nbetter or comparable results to state-of-the-art methods at each step. All\nsource code, full prompts, and datasets of SI-LLM are available at\nhttps://github.com/PierreWoL/SILLM.",
    "code_links": [
      "https://github.com/PierreWoL/SILLM"
    ],
    "comment": null
  },
  {
    "title": "CRouting: Reducing Expensive Distance Calls in Graph-Based Approximate Nearest Neighbor Search",
    "authors": "Zhenxin Li, Shuibing He, Jiahao Guo, Xuechen Zhang, Xian-He Sun, Gang Chen",
    "published": "2025-08-30",
    "arxiv_id": "2509.00365v1",
    "url": "http://arxiv.org/abs/2509.00365v1",
    "pdf_url": "http://arxiv.org/pdf/2509.00365v1",
    "category": "databases",
    "primary_category": "cs.DB",
    "abstract": "Approximate nearest neighbor search (ANNS) is a crucial problem in\ninformation retrieval and AI applications. Recently, there has been a surge of\ninterest in graph-based ANNS algorithms due to their superior efficiency and\naccuracy. However, the repeated computation of distances in high-dimensional\nspaces constitutes the primary time cost of graph-based methods. To accelerate\nthe search, we propose a novel routing strategy named CRouting, which bypasses\nunnecessary distance computations by exploiting the angle distributions of\nhigh-dimensional vectors. CRouting is designed as a plugin to optimize existing\ngraph-based search with minimal code modifications. Our experiments show that\nCRouting reduces the number of distance computations by up to 41.5% and boosts\nqueries per second by up to 1.48$\\times$ on two predominant graph indexes, HNSW\nand NSG. Code is publicly available at https://github.com/ISCS-ZJU/CRouting.",
    "code_links": [
      "https://github.com/ISCS-ZJU/CRouting"
    ],
    "comment": null
  },
  {
    "title": "ST-Raptor: LLM-Powered Semi-Structured Table Question Answering",
    "authors": "Zirui Tang, Boyu Niu, Xuanhe Zhou, Boxiu Li, Wei Zhou, Jiannan Wang, Guoliang Li, Xinyi Zhang, Fan Wu",
    "published": "2025-08-25",
    "arxiv_id": "2508.18190v3",
    "url": "http://arxiv.org/abs/2508.18190v3",
    "pdf_url": "http://arxiv.org/pdf/2508.18190v3",
    "category": "databases",
    "primary_category": "cs.AI",
    "abstract": "Semi-structured tables, widely used in real-world applications (e.g.,\nfinancial reports, medical records, transactional orders), often involve\nflexible and complex layouts (e.g., hierarchical headers and merged cells).\nThese tables generally rely on human analysts to interpret table layouts and\nanswer relevant natural language questions, which is costly and inefficient. To\nautomate the procedure, existing methods face significant challenges. First,\nmethods like NL2SQL require converting semi-structured tables into structured\nones, which often causes substantial information loss. Second, methods like\nNL2Code and multi-modal LLM QA struggle to understand the complex layouts of\nsemi-structured tables and cannot accurately answer corresponding questions. To\nthis end, we propose ST-Raptor, a tree-based framework for semi-structured\ntable question answering using large language models. First, we introduce the\nHierarchical Orthogonal Tree (HO-Tree), a structural model that captures\ncomplex semi-structured table layouts, along with an effective algorithm for\nconstructing the tree. Second, we define a set of basic tree operations to\nguide LLMs in executing common QA tasks. Given a user question, ST-Raptor\ndecomposes it into simpler sub-questions, generates corresponding tree\noperation pipelines, and conducts operation-table alignment for accurate\npipeline execution. Third, we incorporate a two-stage verification mechanism:\nforward validation checks the correctness of execution steps, while backward\nvalidation evaluates answer reliability by reconstructing queries from\npredicted answers. To benchmark the performance, we present SSTQA, a dataset of\n764 questions over 102 real-world semi-structured tables. Experiments show that\nST-Raptor outperforms nine baselines by up to 20% in answer accuracy. The code\nis available at https://github.com/weAIDB/ST-Raptor.",
    "code_links": [
      "https://github.com/weAIDB/ST-Raptor"
    ],
    "comment": "Extension of our SIGMOD 2026 paper. Please refer to source code\n  available at: https://github.com/weAIDB/ST-Raptor"
  },
  {
    "title": "PGTuner: An Efficient Framework for Automatic and Transferable Configuration Tuning of Proximity Graphs",
    "authors": "Hao Duan, Yitong Song, Bin Yao, Anqi Liang",
    "published": "2025-08-25",
    "arxiv_id": "2508.17886v1",
    "url": "http://arxiv.org/abs/2508.17886v1",
    "pdf_url": "http://arxiv.org/pdf/2508.17886v1",
    "category": "databases",
    "primary_category": "cs.DB",
    "abstract": "Approximate Nearest Neighbor Search (ANNS) plays a crucial role in many key\nareas. Proximity graphs (PGs) are the leading method for ANNS, offering the\nbest balance between query efficiency and accuracy. However, their performance\nheavily depends on various construction and query parameters, which are\ndifficult to optimize due to their complex inter-dependencies. Given that users\noften prioritize specific accuracy levels, efficiently identifying the optimal\nPG configurations to meet these targets is essential. Although some studies\nhave explored automatic configuration tuning for PGs, they are limited by\ninefficiencies and suboptimal results. These issues stem from the need to\nconstruct numerous PGs for searching and re-tuning from scratch whenever the\ndataset changes, as well as the failure to capture the complex dependencies\nbetween configurations, query performance, and tuning objectives.\n  To address these challenges, we propose PGTuner, an efficient framework for\nautomatic PG configuration tuning leveraging pre-training knowledge and model\ntransfer techniques. PGTuner improves efficiency through a pre-trained query\nperformance prediction (QPP) model, eliminating the need to build multiple PGs.\nIt also features a deep reinforcement learning-based parameter configuration\nrecommendation (PCR) model to recommend optimal configurations for specific\ndatasets and accuracy targets. Additionally, PGTuner incorporates\nout-of-distribution detection and deep active learning for efficient tuning in\ndynamic scenarios and transferring to new datasets. Extensive experiments\ndemonstrate that PGTuner can stably achieve the top-level tuning effect across\ndifferent datasets while significantly improving tuning efficiency by up to\n14.69X, with a 14.64X boost in dynamic scenarios. The code and data for PGTuner\nare available online at https://github.com/hao-duan/PGTuner.",
    "code_links": [
      "https://github.com/hao-duan/PGTuner"
    ],
    "comment": null
  },
  {
    "title": "Attribute Filtering in Approximate Nearest Neighbor Search: An In-depth Experimental Study",
    "authors": "Mocheng Li, Xiao Yan, Baotong Lu, Yue Zhang, James Cheng, Chenhao Ma",
    "published": "2025-08-22",
    "arxiv_id": "2508.16263v1",
    "url": "http://arxiv.org/abs/2508.16263v1",
    "pdf_url": "http://arxiv.org/pdf/2508.16263v1",
    "category": "databases",
    "primary_category": "cs.DB",
    "abstract": "With the growing integration of structured and unstructured data, new methods\nhave emerged for performing similarity searches on vectors while honoring\nstructured attribute constraints, i.e., a process known as Filtering\nApproximate Nearest Neighbor (Filtering ANN) search. Since many of these\nalgorithms have only appeared in recent years and are designed to work with a\nvariety of base indexing methods and filtering strategies, there is a pressing\nneed for a unified analysis that identifies their core techniques and enables\nmeaningful comparisons.\n  In this work, we present a unified Filtering ANN search interface that\nencompasses the latest algorithms and evaluate them extensively from multiple\nperspectives. First, we propose a comprehensive taxonomy of existing Filtering\nANN algorithms based on attribute types and filtering strategies. Next, we\nanalyze their key components, i.e., index structures, pruning strategies, and\nentry point selection, to elucidate design differences and tradeoffs. We then\nconduct a broad experimental evaluation on 10 algorithms and 12 methods across\n4 datasets (each with up to 10 million items), incorporating both synthetic and\nreal attributes and covering selectivity levels from 0.1% to 100%. Finally, an\nin-depth component analysis reveals the influence of pruning, entry point\nselection, and edge filtering costs on overall performance. Based on our\nfindings, we summarize the strengths and limitations of each approach, provide\npractical guidelines for selecting appropriate methods, and suggest promising\ndirections for future research. Our code is available at:\nhttps://github.com/lmccccc/FANNBench.",
    "code_links": [
      "https://github.com/lmccccc/FANNBench"
    ],
    "comment": "15 pages, 15 figures, Accepted at SIGMOD 2026"
  }
]