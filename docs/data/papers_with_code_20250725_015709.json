[
  {
    "title": "Leave No One Behind: Fairness-Aware Cross-Domain Recommender Systems for Non-Overlapping Users",
    "authors": "Weixin Chen, Yuhan Zhao, Li Chen, Weike Pan",
    "published": "2025-07-23",
    "arxiv_id": "2507.17749v1",
    "url": "http://arxiv.org/abs/2507.17749v1",
    "pdf_url": "http://arxiv.org/pdf/2507.17749v1",
    "category": "information_retrieval",
    "primary_category": "cs.IR",
    "abstract": "Cross-domain recommendation (CDR) methods predominantly leverage overlapping\nusers to transfer knowledge from a source domain to a target domain. However,\nthrough empirical studies, we uncover a critical bias inherent in these\napproaches: while overlapping users experience significant enhancements in\nrecommendation quality, non-overlapping users benefit minimally and even face\nperformance degradation. This unfairness may erode user trust, and,\nconsequently, negatively impact business engagement and revenue. To address\nthis issue, we propose a novel solution that generates virtual source-domain\nusers for non-overlapping target-domain users. Our method utilizes a dual\nattention mechanism to discern similarities between overlapping and\nnon-overlapping users, thereby synthesizing realistic virtual user embeddings.\nWe further introduce a limiter component that ensures the generated virtual\nusers align with real-data distributions while preserving each user's unique\ncharacteristics. Notably, our method is model-agnostic and can be seamlessly\nintegrated into any CDR model. Comprehensive experiments conducted on three\npublic datasets with five CDR baselines demonstrate that our method effectively\nmitigates the CDR non-overlapping user bias, without loss of overall accuracy.\nOur code is publicly available at https://github.com/WeixinChen98/VUG.",
    "code_links": [
      "https://github.com/WeixinChen98/VUG"
    ],
    "comment": "Accepted by RecSys 2025"
  },
  {
    "title": "BGM-HAN: A Hierarchical Attention Network for Accurate and Fair Decision Assessment on Semi-Structured Profiles",
    "authors": "Junhua Liu, Roy Ka-Wei Lee, Kwan Hui Lim",
    "published": "2025-07-23",
    "arxiv_id": "2507.17472v1",
    "url": "http://arxiv.org/abs/2507.17472v1",
    "pdf_url": "http://arxiv.org/pdf/2507.17472v1",
    "category": "information_retrieval",
    "primary_category": "cs.LG",
    "abstract": "Human decision-making in high-stakes domains often relies on expertise and\nheuristics, but is vulnerable to hard-to-detect cognitive biases that threaten\nfairness and long-term outcomes. This work presents a novel approach to\nenhancing complex decision-making workflows through the integration of\nhierarchical learning alongside various enhancements. Focusing on university\nadmissions as a representative high-stakes domain, we propose BGM-HAN, an\nenhanced Byte-Pair Encoded, Gated Multi-head Hierarchical Attention Network,\ndesigned to effectively model semi-structured applicant data. BGM-HAN captures\nmulti-level representations that are crucial for nuanced assessment, improving\nboth interpretability and predictive performance. Experimental results on real\nadmissions data demonstrate that our proposed model significantly outperforms\nboth state-of-the-art baselines from traditional machine learning to large\nlanguage models, offering a promising framework for augmenting decision-making\nin domains where structure, context, and fairness matter. Source code is\navailable at: https://github.com/junhua/bgm-han.",
    "code_links": [
      "https://github.com/junhua/bgm-han"
    ],
    "comment": "Accepted at ASONAM 2025"
  },
  {
    "title": "HLFormer: Enhancing Partially Relevant Video Retrieval with Hyperbolic Learning",
    "authors": "Li Jun, Wang Jinpeng, Tan Chaolei, Lian Niu, Chen Long, Zhang Min, Wang Yaowei, Xia Shu-Tao, Chen Bin",
    "published": "2025-07-23",
    "arxiv_id": "2507.17402v1",
    "url": "http://arxiv.org/abs/2507.17402v1",
    "pdf_url": "http://arxiv.org/pdf/2507.17402v1",
    "category": "information_retrieval",
    "primary_category": "cs.CV",
    "abstract": "Partially Relevant Video Retrieval (PRVR) addresses the critical challenge of\nmatching untrimmed videos with text queries describing only partial content.\nExisting methods suffer from geometric distortion in Euclidean space that\nsometimes misrepresents the intrinsic hierarchical structure of videos and\noverlooks certain hierarchical semantics, ultimately leading to suboptimal\ntemporal modeling. To address this issue, we propose the first hyperbolic\nmodeling framework for PRVR, namely HLFormer, which leverages hyperbolic space\nlearning to compensate for the suboptimal hierarchical modeling capabilities of\nEuclidean space. Specifically, HLFormer integrates the Lorentz Attention Block\nand Euclidean Attention Block to encode video embeddings in hybrid spaces,\nusing the Mean-Guided Adaptive Interaction Module to dynamically fuse features.\nAdditionally, we introduce a Partial Order Preservation Loss to enforce \"text <\nvideo\" hierarchy through Lorentzian cone constraints. This approach further\nenhances cross-modal matching by reinforcing partial relevance between video\ncontent and text queries. Extensive experiments show that HLFormer outperforms\nstate-of-the-art methods. Code is released at\nhttps://github.com/lijun2005/ICCV25-HLFormer.",
    "code_links": [
      "https://github.com/lijun2005/ICCV25-HLFormer"
    ],
    "comment": "Accepted by ICCV'25. 13 pages, 6 figures, 4 tables"
  },
  {
    "title": "EndoFinder: Online Lesion Retrieval for Explainable Colorectal Polyp Diagnosis Leveraging Latent Scene Representations",
    "authors": "Ruijie Yang, Yan Zhu, Peiyao Fu, Yizhe Zhang, Zhihua Wang, Quanlin Li, Pinghong Zhou, Xian Yang, Shuo Wang",
    "published": "2025-07-23",
    "arxiv_id": "2507.17323v1",
    "url": "http://arxiv.org/abs/2507.17323v1",
    "pdf_url": "http://arxiv.org/pdf/2507.17323v1",
    "category": "information_retrieval",
    "primary_category": "cs.IR",
    "abstract": "Colorectal cancer (CRC) remains a leading cause of cancer-related mortality,\nunderscoring the importance of timely polyp detection and diagnosis. While deep\nlearning models have improved optical-assisted diagnostics, they often demand\nextensive labeled datasets and yield \"black-box\" outputs with limited\ninterpretability. In this paper, we propose EndoFinder, an online polyp\nretrieval framework that leverages multi-view scene representations for\nexplainable and scalable CRC diagnosis. First, we develop a Polyp-aware Image\nEncoder by combining contrastive learning and a reconstruction task, guided by\npolyp segmentation masks. This self-supervised approach captures robust\nfeatures without relying on large-scale annotated data. Next, we treat each\npolyp as a three-dimensional \"scene\" and introduce a Scene Representation\nTransformer, which fuses multiple views of the polyp into a single latent\nrepresentation. By discretizing this representation through a hashing layer,\nEndoFinder enables real-time retrieval from a compiled database of historical\npolyp cases, where diagnostic information serves as interpretable references\nfor new queries. We evaluate EndoFinder on both public and newly collected\npolyp datasets for re-identification and pathology classification. Results show\nthat EndoFinder outperforms existing methods in accuracy while providing\ntransparent, retrieval-based insights for clinical decision-making. By\ncontributing a novel dataset and a scalable, explainable framework, our work\naddresses key challenges in polyp diagnosis and offers a promising direction\nfor more efficient AI-driven colonoscopy workflows. The source code is\navailable at https://github.com/ku262/EndoFinder-Scene.",
    "code_links": [
      "https://github.com/ku262/EndoFinder-Scene"
    ],
    "comment": null
  },
  {
    "title": "RAVine: Reality-Aligned Evaluation for Agentic Search",
    "authors": "Yilong Xu, Xiang Long, Zhi Zheng, Jinhua Gao",
    "published": "2025-07-22",
    "arxiv_id": "2507.16725v1",
    "url": "http://arxiv.org/abs/2507.16725v1",
    "pdf_url": "http://arxiv.org/pdf/2507.16725v1",
    "category": "information_retrieval",
    "primary_category": "cs.CL",
    "abstract": "Agentic search, as a more autonomous and adaptive paradigm of retrieval\naugmentation, is driving the evolution of intelligent search systems. However,\nexisting evaluation frameworks fail to align well with the goals of agentic\nsearch. First, the complex queries commonly used in current benchmarks often\ndeviate from realistic user search scenarios. Second, prior approaches tend to\nintroduce noise when extracting ground truth for end-to-end evaluations,\nleading to distorted assessments at a fine-grained level. Third, most current\nframeworks focus solely on the quality of final answers, neglecting the\nevaluation of the iterative process inherent to agentic search. To address\nthese limitations, we propose RAVine -- a Reality-Aligned eValuation framework\nfor agentic LLMs with search. RAVine targets multi-point queries and long-form\nanswers that better reflect user intents, and introduces an attributable ground\ntruth construction strategy to enhance the accuracy of fine-grained evaluation.\nMoreover, RAVine examines model's interaction with search tools throughout the\niterative process, and accounts for factors of efficiency. We benchmark a\nseries of models using RAVine and derive several insights, which we hope will\ncontribute to advancing the development of agentic search systems. The code and\ndatasets are available at https://github.com/SwordFaith/RAVine.",
    "code_links": [
      "https://github.com/SwordFaith/RAVine"
    ],
    "comment": null
  },
  {
    "title": "Knowledge-aware Diffusion-Enhanced Multimedia Recommendation",
    "authors": "Xian Mo, Fei Liu, Rui Tang, Jintao, Gao, Hao Liu",
    "published": "2025-07-22",
    "arxiv_id": "2507.16396v1",
    "url": "http://arxiv.org/abs/2507.16396v1",
    "pdf_url": "http://arxiv.org/pdf/2507.16396v1",
    "category": "information_retrieval",
    "primary_category": "cs.MM",
    "abstract": "Multimedia recommendations aim to use rich multimedia content to enhance\nhistorical user-item interaction information, which can not only indicate the\ncontent relatedness among items but also reveal finer-grained preferences of\nusers. In this paper, we propose a Knowledge-aware Diffusion-Enhanced\narchitecture using contrastive learning paradigms (KDiffE) for multimedia\nrecommendations. Specifically, we first utilize original user-item graphs to\nbuild an attention-aware matrix into graph neural networks, which can learn the\nimportance between users and items for main view construction. The\nattention-aware matrix is constructed by adopting a random walk with a restart\nstrategy, which can preserve the importance between users and items to generate\naggregation of attention-aware node features. Then, we propose a guided\ndiffusion model to generate strongly task-relevant knowledge graphs with less\nnoise for constructing a knowledge-aware contrastive view, which utilizes user\nembeddings with an edge connected to an item to guide the generation of\nstrongly task-relevant knowledge graphs for enhancing the item's semantic\ninformation. We perform comprehensive experiments on three multimedia datasets\nthat reveal the effectiveness of our KDiffE and its components on various\nstate-of-the-art methods. Our source codes are available\nhttps://github.com/1453216158/KDiffE.",
    "code_links": [
      "https://github.com/1453216158/KDiffE"
    ],
    "comment": null
  },
  {
    "title": "Time to Split: Exploring Data Splitting Strategies for Offline Evaluation of Sequential Recommenders",
    "authors": "Danil Gusak, Anna Volodkevich, Anton Klenitskiy, Alexey Vasilev, Evgeny Frolov",
    "published": "2025-07-22",
    "arxiv_id": "2507.16289v1",
    "url": "http://arxiv.org/abs/2507.16289v1",
    "pdf_url": "http://arxiv.org/pdf/2507.16289v1",
    "category": "information_retrieval",
    "primary_category": "cs.IR",
    "abstract": "Modern sequential recommender systems, ranging from lightweight\ntransformer-based variants to large language models, have become increasingly\nprominent in academia and industry due to their strong performance in the\nnext-item prediction task. Yet common evaluation protocols for sequential\nrecommendations remain insufficiently developed: they often fail to reflect the\ncorresponding recommendation task accurately, or are not aligned with\nreal-world scenarios.\n  Although the widely used leave-one-out split matches next-item prediction, it\npermits the overlap between training and test periods, which leads to temporal\nleakage and unrealistically long test horizon, limiting real-world relevance.\nGlobal temporal splitting addresses these issues by evaluating on distinct\nfuture periods. However, its applications to sequential recommendations remain\nloosely defined, particularly in terms of selecting target interactions and\nconstructing a validation subset that provides necessary consistency between\nvalidation and test metrics.\n  In this paper, we demonstrate that evaluation outcomes can vary significantly\nacross splitting strategies, influencing model rankings and practical\ndeployment decisions. To improve reproducibility in both academic and\nindustrial settings, we systematically compare different splitting strategies\nfor sequential recommendations across multiple datasets and established\nbaselines. Our findings show that prevalent splits, such as leave-one-out, may\nbe insufficiently aligned with more realistic evaluation strategies. Code:\nhttps://github.com/monkey0head/time-to-split",
    "code_links": [
      "https://github.com/monkey0head/time-to-split"
    ],
    "comment": "Accepted for ACM RecSys 2025. Author's version. The final published\n  version will be available at the ACM Digital Library"
  },
  {
    "title": "Hierarchical Graph Information Bottleneck for Multi-Behavior Recommendation",
    "authors": "Hengyu Zhang, Chunxu Shen, Xiangguo Sun, Jie Tan, Yanchao Tan, Yu Rong, Hong Cheng, Lingling Yi",
    "published": "2025-07-21",
    "arxiv_id": "2507.15395v1",
    "url": "http://arxiv.org/abs/2507.15395v1",
    "pdf_url": "http://arxiv.org/pdf/2507.15395v1",
    "category": "information_retrieval",
    "primary_category": "cs.IR",
    "abstract": "In real-world recommendation scenarios, users typically engage with platforms\nthrough multiple types of behavioral interactions. Multi-behavior\nrecommendation algorithms aim to leverage various auxiliary user behaviors to\nenhance prediction for target behaviors of primary interest (e.g., buy),\nthereby overcoming performance limitations caused by data sparsity in target\nbehavior records. Current state-of-the-art approaches typically employ\nhierarchical design following either cascading (e.g.,\nview$\\rightarrow$cart$\\rightarrow$buy) or parallel\n(unified$\\rightarrow$behavior$\\rightarrow$specific components) paradigms, to\ncapture behavioral relationships. However, these methods still face two\ncritical challenges: (1) severe distribution disparities across behaviors, and\n(2) negative transfer effects caused by noise in auxiliary behaviors. In this\npaper, we propose a novel model-agnostic Hierarchical Graph Information\nBottleneck (HGIB) framework for multi-behavior recommendation to effectively\naddress these challenges. Following information bottleneck principles, our\nframework optimizes the learning of compact yet sufficient representations that\npreserve essential information for target behavior prediction while eliminating\ntask-irrelevant redundancies. To further mitigate interaction noise, we\nintroduce a Graph Refinement Encoder (GRE) that dynamically prunes redundant\nedges through learnable edge dropout mechanisms. We conduct comprehensive\nexperiments on three real-world public datasets, which demonstrate the superior\neffectiveness of our framework. Beyond these widely used datasets in the\nacademic community, we further expand our evaluation on several real industrial\nscenarios and conduct an online A/B testing, showing again a significant\nimprovement in multi-behavior recommendations. The source code of our proposed\nHGIB is available at https://github.com/zhy99426/HGIB.",
    "code_links": [
      "https://github.com/zhy99426/HGIB"
    ],
    "comment": "Accepted by RecSys2025"
  },
  {
    "title": "SPAR: Scholar Paper Retrieval with LLM-based Agents for Enhanced Academic Search",
    "authors": "Xiaofeng Shi, Yuduo Li, Qian Kou, Longbin Yu, Jinxin Xie, Hua Zhou",
    "published": "2025-07-21",
    "arxiv_id": "2507.15245v1",
    "url": "http://arxiv.org/abs/2507.15245v1",
    "pdf_url": "http://arxiv.org/pdf/2507.15245v1",
    "category": "information_retrieval",
    "primary_category": "cs.IR",
    "abstract": "Recent advances in large language models (LLMs) have opened new opportunities\nfor academic literature retrieval. However, existing systems often rely on\nrigid pipelines and exhibit limited reasoning capabilities. We introduce SPAR,\na multi-agent framework that incorporates RefChain-based query decomposition\nand query evolution to enable more flexible and effective search. To facilitate\nsystematic evaluation, we also construct SPARBench, a challenging benchmark\nwith expert-annotated relevance labels. Experimental results demonstrate that\nSPAR substantially outperforms strong baselines, achieving up to +56% F1 on\nAutoScholar and +23% F1 on SPARBench over the best-performing baseline.\nTogether, SPAR and SPARBench provide a scalable, interpretable, and\nhigh-performing foundation for advancing research in scholarly retrieval. Code\nand data will be available at: https://github.com/xiaofengShi/SPAR",
    "code_links": [
      "https://github.com/xiaofengShi/SPAR"
    ],
    "comment": null
  },
  {
    "title": "U-MARVEL: Unveiling Key Factors for Universal Multimodal Retrieval via Embedding Learning with MLLMs",
    "authors": "Xiaojie Li, Chu Li, Shi-Zhe Chen, Xi Chen",
    "published": "2025-07-20",
    "arxiv_id": "2507.14902v1",
    "url": "http://arxiv.org/abs/2507.14902v1",
    "pdf_url": "http://arxiv.org/pdf/2507.14902v1",
    "category": "information_retrieval",
    "primary_category": "cs.IR",
    "abstract": "Universal multimodal retrieval (UMR), which aims to address complex retrieval\ntasks where both queries and candidates span diverse modalities, has been\nsignificantly advanced by the emergence of MLLMs. While state-of-the-art\nMLLM-based methods in the literature predominantly adopt contrastive learning\nprinciples, they often differ in their specific training recipes. Despite their\nsuccess, the mechanisms underlying their retrieval capabilities remain largely\nunexplored, potentially resulting in suboptimal performance and limited\ngeneralization ability. To address these issues, we present a comprehensive\nstudy aimed at uncovering the key factors that drive effective embedding\nlearning for UMR using MLLMs. We begin by implementing a general MLLM-based\nembedding learning pipeline, and systematically analyze the primary\ncontributors to high-performing universal retrieval systems. Based on this, we\nexplore various aspects of the details in embedding generation and training\nstrategies, including progressive transition, hard negative mining and\nre-ranker distillation. Notably, our findings reveal that often-overlooked\nfactors can have a substantial impact on model performance. Building on these\ndiscoveries, we introduce a unified framework termed U-MARVEL\n(\\textbf{U}niversal \\textbf{M}ultimod\\textbf{A}l \\textbf{R}etrie\\textbf{V}al\nvia \\textbf{E}mbedding \\textbf{L}earning), which outperforms state-of-the-art\ncompetitors on the M-BEIR benchmark by a large margin in supervised settings,\nand also exihibits strong zero-shot performance on several tasks such as\ncomposed image retrieval and text-to-video retrieval. These results underscore\nthe generalization potential of our framework across various embedding-based\nretrieval tasks. Code is available at https://github.com/chaxjli/U-MARVEL",
    "code_links": [
      "https://github.com/chaxjli/U-MARVEL"
    ],
    "comment": "Technical Report (in progress)"
  },
  {
    "title": "Advancing Retrieval-Augmented Generation for Structured Enterprise and Internal Data",
    "authors": "Chandana Cheerla",
    "published": "2025-07-16",
    "arxiv_id": "2507.12425v1",
    "url": "http://arxiv.org/abs/2507.12425v1",
    "pdf_url": "http://arxiv.org/pdf/2507.12425v1",
    "category": "information_retrieval",
    "primary_category": "cs.CL",
    "abstract": "Organizations increasingly rely on proprietary enterprise data, including HR\nrecords, structured reports, and tabular documents, for critical\ndecision-making. While Large Language Models (LLMs) have strong generative\ncapabilities, they are limited by static pretraining, short context windows,\nand challenges in processing heterogeneous data formats. Conventional\nRetrieval-Augmented Generation (RAG) frameworks address some of these gaps but\noften struggle with structured and semi-structured data.\n  This work proposes an advanced RAG framework that combines hybrid retrieval\nstrategies using dense embeddings (all-mpnet-base-v2) and BM25, enhanced by\nmetadata-aware filtering with SpaCy NER and cross-encoder reranking. The\nframework applies semantic chunking to maintain textual coherence and retains\ntabular data structures to preserve row-column integrity. Quantized indexing\noptimizes retrieval efficiency, while human-in-the-loop feedback and\nconversation memory improve adaptability.\n  Experiments on enterprise datasets show notable improvements: Precision@5\nincreased by 15 percent (90 versus 75), Recall@5 by 13 percent (87 versus 74),\nand Mean Reciprocal Rank by 16 percent (0.85 versus 0.69). Qualitative\nevaluations show higher scores in Faithfulness (4.6 versus 3.0), Completeness\n(4.2 versus 2.5), and Relevance (4.5 versus 3.2) on a 5-point Likert scale.\nThese results demonstrate the framework's effectiveness in delivering accurate,\ncomprehensive, and contextually relevant responses for enterprise tasks. Future\nwork includes extending to multimodal data and integrating agent-based\nretrieval. The source code will be released at\nhttps://github.com/CheerlaChandana/Enterprise-Chatbot",
    "code_links": [
      "https://github.com/CheerlaChandana/Enterprise-Chatbot"
    ],
    "comment": null
  },
  {
    "title": "DyG-RAG: Dynamic Graph Retrieval-Augmented Generation with Event-Centric Reasoning",
    "authors": "Qingyun Sun, Jiaqi Yuan, Shan He, Xiao Guan, Haonan Yuan, Xingcheng Fu, Jianxin Li, Philip S. Yu",
    "published": "2025-07-16",
    "arxiv_id": "2507.13396v1",
    "url": "http://arxiv.org/abs/2507.13396v1",
    "pdf_url": "http://arxiv.org/pdf/2507.13396v1",
    "category": "information_retrieval",
    "primary_category": "cs.IR",
    "abstract": "Graph Retrieval-Augmented Generation has emerged as a powerful paradigm for\ngrounding large language models with external structured knowledge. However,\nexisting Graph RAG methods struggle with temporal reasoning, due to their\ninability to model the evolving structure and order of real-world events. In\nthis work, we introduce DyG-RAG, a novel event-centric dynamic graph\nretrieval-augmented generation framework designed to capture and reason over\ntemporal knowledge embedded in unstructured text. To eliminate temporal\nambiguity in traditional retrieval units, DyG-RAG proposes Dynamic Event Units\n(DEUs) that explicitly encode both semantic content and precise temporal\nanchors, enabling accurate and interpretable time-aware retrieval. To capture\ntemporal and causal dependencies across events, DyG-RAG constructs an event\ngraph by linking DEUs that share entities and occur close in time, supporting\nefficient and meaningful multi-hop reasoning. To ensure temporally consistent\ngeneration, DyG-RAG introduces an event timeline retrieval pipeline that\nretrieves event sequences via time-aware traversal, and proposes a Time\nChain-of-Thought strategy for temporally grounded answer generation. This\nunified pipeline enables DyG-RAG to retrieve coherent, temporally ordered event\nsequences and to answer complex, time-sensitive queries that standard RAG\nsystems cannot resolve. Extensive experiments on temporal QA benchmarks\ndemonstrate that DyG-RAG significantly improves the accuracy and recall of\nthree typical types of temporal reasoning questions, paving the way for more\nfaithful and temporal-aware generation. DyG-RAG is available at\nhttps://github.com/RingBDStack/DyG-RAG.",
    "code_links": [
      "https://github.com/RingBDStack/DyG-RAG"
    ],
    "comment": null
  },
  {
    "title": "Graph-Structured Data Analysis of Component Failure in Autonomous Cargo Ships Based on Feature Fusion",
    "authors": "Zizhao Zhang, Tianxiang Zhao, Yu Sun, Liping Sun, Jichuan Kang",
    "published": "2025-07-18",
    "arxiv_id": "2507.13721v1",
    "url": "http://arxiv.org/abs/2507.13721v1",
    "pdf_url": "http://arxiv.org/pdf/2507.13721v1",
    "category": "databases",
    "primary_category": "cs.LG",
    "abstract": "To address the challenges posed by cascading reactions caused by component\nfailures in autonomous cargo ships (ACS) and the uncertainties in emergency\ndecision-making, this paper proposes a novel hybrid feature fusion framework\nfor constructing a graph-structured dataset of failure modes. By employing an\nimproved cuckoo search algorithm (HN-CSA), the literature retrieval efficiency\nis significantly enhanced, achieving improvements of 7.1% and 3.4% compared to\nthe NSGA-II and CSA search algorithms, respectively. A hierarchical feature\nfusion framework is constructed, using Word2Vec encoding to encode\nsubsystem/component features, BERT-KPCA to process failure modes/reasons, and\nSentence-BERT to quantify the semantic association between failure impact and\nemergency decision-making. The dataset covers 12 systems, 1,262 failure modes,\nand 6,150 propagation paths. Validation results show that the GATE-GNN model\nachieves a classification accuracy of 0.735, comparable to existing benchmarks.\nAdditionally, a silhouette coefficient of 0.641 indicates that the features are\nhighly distinguishable. In the label prediction results, the Shore-based\nMeteorological Service System achieved an F1 score of 0.93, demonstrating high\nprediction accuracy. This paper not only provides a solid foundation for\nfailure analysis in autonomous cargo ships but also offers reliable support for\nfault diagnosis, risk assessment, and intelligent decision-making systems. The\nlink to the dataset is\nhttps://github.com/wojiufukele/Graph-Structured-about-CSA.",
    "code_links": [
      "https://github.com/wojiufukele/Graph-Structured-about-CSA"
    ],
    "comment": null
  },
  {
    "title": "TOPJoin: A Context-Aware Multi-Criteria Approach for Joinable Column Search",
    "authors": "Harsha Kokel, Aamod Khatiwada, Tejaswini Pedapati, Haritha Ananthakrishnan, Oktie Hassanzadeh, Horst Samulowitz, Kavitha Srinivas",
    "published": "2025-07-15",
    "arxiv_id": "2507.11505v1",
    "url": "http://arxiv.org/abs/2507.11505v1",
    "pdf_url": "http://arxiv.org/pdf/2507.11505v1",
    "category": "databases",
    "primary_category": "cs.DB",
    "abstract": "One of the major challenges in enterprise data analysis is the task of\nfinding joinable tables that are conceptually related and provide meaningful\ninsights. Traditionally, joinable tables have been discovered through a search\nfor similar columns, where two columns are considered similar syntactically if\nthere is a set overlap or they are considered similar semantically if either\nthe column embeddings or value embeddings are closer in the embedding space.\nHowever, for enterprise data lakes, column similarity is not sufficient to\nidentify joinable columns and tables. The context of the query column is\nimportant. Hence, in this work, we first define context-aware column\njoinability. Then we propose a multi-criteria approach, called TOPJoin, for\njoinable column search. We evaluate TOPJoin against existing join search\nbaselines over one academic and one real-world join search benchmark. Through\nexperiments, we find that TOPJoin performs better on both benchmarks than the\nbaselines.",
    "code_links": [
      "https://github.com/IBM/ContextAwareJoin"
    ],
    "comment": "VLDB 2025 Workshop: Tabular Data Analysis (TaDA); The source code,\n  data, and/or other artifacts have been made available at\n  https://github.com/IBM/ContextAwareJoin"
  }
]