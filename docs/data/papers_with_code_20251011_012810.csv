title,authors,published,arxiv_id,url,pdf_url,category,primary_category,abstract,code_links,comment
Haystack Engineering: Context Engineering for Heterogeneous and Agentic Long-Context Evaluation,"Mufei Li, Dongqi Fu, Limei Wang, Si Zhang, Hanqing Zeng, Kaan Sancak, Ruizhong Qiu, Haoyu Wang, Xiaoxin He, Xavier Bresson, Yinglong Xia, Chonglin Sun, Pan Li",2025-10-08,2510.07414v1,http://arxiv.org/abs/2510.07414v1,http://arxiv.org/pdf/2510.07414v1,information_retrieval,cs.CL,"Modern long-context large language models (LLMs) perform well on synthetic
""needle-in-a-haystack"" (NIAH) benchmarks, but such tests overlook how noisy
contexts arise from biased retrieval and agentic workflows. We argue that
haystack engineering is necessary to construct noisy long contexts that
faithfully capture key real-world factors -- distraction from heterogeneous
biased retrievers and cascading errors in agentic workflows -- to test models'
long-context robustness. We instantiate it through HaystackCraft, a new NIAH
benchmark built on the full English Wikipedia hyperlink network with multi-hop
questions. HaystackCraft evaluates how heterogeneous retrieval strategies
(e.g., sparse, dense, hybrid, and graph-based) affect distractor composition,
haystack ordering, and downstream LLM performance. HaystackCraft further
extends NIAH to dynamic, LLM-dependent settings that simulate agentic
operations, where models refine queries, reflect on their past reasonings, and
decide when to stop. Experiments with 15 long-context models show that (1)
while stronger dense retrievers can introduce more challenging distractors,
graph-based reranking simultaneously improves retrieval effectiveness and
mitigates more harmful distractors; (2) in agentic tests, even advanced models
like Gemini 2.5 Pro and GPT-5 suffer cascading failures from self-generated
distractors or struggle to perform early stops. These results highlight
persistent challenges in agentic long-context reasoning and establish
HaystackCraft as a valuable testbed for future progress.",https://github.com/Graph-COM/HaystackCraft,Code available at https://github.com/Graph-COM/HaystackCraft
M3Retrieve: Benchmarking Multimodal Retrieval for Medicine,"Arkadeep Acharya, Akash Ghosh, Pradeepika Verma, Kitsuchart Pasupa, Sriparna Saha, Priti Singh",2025-10-08,2510.06888v1,http://arxiv.org/abs/2510.06888v1,http://arxiv.org/pdf/2510.06888v1,information_retrieval,cs.IR,"With the increasing use of RetrievalAugmented Generation (RAG), strong
retrieval models have become more important than ever. In healthcare,
multimodal retrieval models that combine information from both text and images
offer major advantages for many downstream tasks such as question answering,
cross-modal retrieval, and multimodal summarization, since medical data often
includes both formats. However, there is currently no standard benchmark to
evaluate how well these models perform in medical settings. To address this
gap, we introduce M3Retrieve, a Multimodal Medical Retrieval Benchmark.
M3Retrieve, spans 5 domains,16 medical fields, and 4 distinct tasks, with over
1.2 Million text documents and 164K multimodal queries, all collected under
approved licenses. We evaluate leading multimodal retrieval models on this
benchmark to explore the challenges specific to different medical specialities
and to understand their impact on retrieval performance. By releasing
M3Retrieve, we aim to enable systematic evaluation, foster model innovation,
and accelerate research toward building more capable and reliable multimodal
retrieval systems for medical applications. The dataset and the baselines code
are available in this github page https://github.com/AkashGhosh/M3Retrieve.",https://github.com/AkashGhosh/M3Retrieve,EMNLP Mains 2025
Are LLMs Reliable Rankers? Rank Manipulation via Two-Stage Token Optimization,"Tiancheng Xing, Jerry Li, Yixuan Du, Xiyang Hu",2025-10-08,2510.06732v1,http://arxiv.org/abs/2510.06732v1,http://arxiv.org/pdf/2510.06732v1,information_retrieval,cs.CL,"Large language models (LLMs) are increasingly used as rerankers in
information retrieval, yet their ranking behavior can be steered by small,
natural-sounding prompts. To expose this vulnerability, we present Rank
Anything First (RAF), a two-stage token optimization method that crafts concise
textual perturbations to consistently promote a target item in LLM-generated
rankings while remaining hard to detect. Stage 1 uses Greedy Coordinate
Gradient to shortlist candidate tokens at the current position by combining the
gradient of the rank-target with a readability score; Stage 2 evaluates those
candidates under exact ranking and readability losses using an entropy-based
dynamic weighting scheme, and selects a token via temperature-controlled
sampling. RAF generates ranking-promoting prompts token-by-token, guided by
dual objectives: maximizing ranking effectiveness and preserving linguistic
naturalness. Experiments across multiple LLMs show that RAF significantly
boosts the rank of target items using naturalistic language, with greater
robustness than existing methods in both promoting target items and maintaining
naturalness. These findings underscore a critical security implication:
LLM-based reranking is inherently susceptible to adversarial manipulation,
raising new challenges for the trustworthiness and robustness of modern
retrieval systems. Our code is available at: https://github.com/glad-lab/RAF.",https://github.com/glad-lab/RAF,"10 pages, 3 figures"
MARCO: A Cooperative Knowledge Transfer Framework for Personalized Cross-domain Recommendations,"Lili Xie, Yi Zhang, Ruihong Qiu, Jiajun Liu, Sen Wang",2025-10-06,2510.04508v1,http://arxiv.org/abs/2510.04508v1,http://arxiv.org/pdf/2510.04508v1,information_retrieval,cs.IR,"Recommender systems frequently encounter data sparsity issues, particularly
when addressing cold-start scenarios involving new users or items. Multi-source
cross-domain recommendation (CDR) addresses these challenges by transferring
valuable knowledge from multiple source domains to enhance recommendations in a
target domain. However, existing reinforcement learning (RL)-based CDR methods
typically rely on a single-agent framework, leading to negative transfer issues
caused by inconsistent domain contributions and inherent distributional
discrepancies among source domains. To overcome these limitations, MARCO, a
Multi-Agent Reinforcement Learning-based Cross-Domain recommendation framework,
is proposed. It leverages cooperative multi-agent reinforcement learning, where
each agent is dedicated to estimating the contribution from an individual
source domain, effectively managing credit assignment and mitigating negative
transfer. In addition, an entropy-based action diversity penalty is introduced
to enhance policy expressiveness and stabilize training by encouraging diverse
agents' joint actions. Extensive experiments across four benchmark datasets
demonstrate MARCO's superior performance over state-of-the-art methods,
highlighting its robustness and strong generalization capabilities. The code is
at https://github.com/xiewilliams/MARCO.",https://github.com/xiewilliams/MARCO,SIGIR-AP 2025
GRACE: Generative Representation Learning via Contrastive Policy Optimization,"Jiashuo Sun, Shixuan Liu, Zhaochen Su, Xianrui Zhong, Pengcheng Jiang, Bowen Jin, Peiran Li, Weijia Shi, Jiawei Han",2025-10-06,2510.04506v1,http://arxiv.org/abs/2510.04506v1,http://arxiv.org/pdf/2510.04506v1,information_retrieval,cs.CL,"Prevailing methods for training Large Language Models (LLMs) as text encoders
rely on contrastive losses that treat the model as a black box function,
discarding its generative and reasoning capabilities in favor of static
embeddings. We introduce GRACE (Generative Representation Learning via
Contrastive Policy Optimization), a novel framework that reimagines contrastive
signals not as losses to be minimized, but as rewards that guide a generative
policy. In GRACE, the LLM acts as a policy that produces explicit,
human-interpretable rationales--structured natural language explanations of its
semantic understanding. These rationales are then encoded into high-quality
embeddings via mean pooling. Using policy gradient optimization, we train the
model with a multi-component reward function that maximizes similarity between
query positive pairs and minimizes similarity with negatives. This transforms
the LLM from an opaque encoder into an interpretable agent whose reasoning
process is transparent and inspectable. On MTEB benchmark, GRACE yields broad
cross category gains: averaged over four backbones, the supervised setting
improves overall score by 11.5% over base models, and the unsupervised variant
adds 6.9%, while preserving general capabilities. This work treats contrastive
objectives as rewards over rationales, unifying representation learning with
generation to produce stronger embeddings and transparent rationales. The
model, data and code are available at https://github.com/GasolSun36/GRACE.",https://github.com/GasolSun36/GRACE,"23 pages, 7 figures, 7 tables"
Causality-aware Graph Aggregation Weight Estimator for Popularity Debiasing in Top-K Recommendation,"Yue Que, Yingyi Zhang, Xiangyu Zhao, Chen Ma",2025-10-06,2510.04502v1,http://arxiv.org/abs/2510.04502v1,http://arxiv.org/pdf/2510.04502v1,information_retrieval,cs.IR,"Graph-based recommender systems leverage neighborhood aggregation to generate
node representations, which is highly sensitive to popularity bias, resulting
in an echo effect during information propagation. Existing graph-based
debiasing solutions refine the aggregation process with attempts such as edge
reconstruction or weight adjustment. However, these methods remain inadequate
in fully alleviating popularity bias. Specifically, this is because 1) they
provide no insights into graph aggregation rationality, thus lacking an
optimality guarantee; 2) they fail to well balance the training and debiasing
process, which undermines the effectiveness. In this paper, we propose a novel
approach to mitigate popularity bias through rational modeling of the graph
aggregation process. We reveal that graph aggregation is a special form of
backdoor adjustment in causal inference, where the aggregation weight
corresponds to the historical interaction likelihood distribution. Based on
this insight, we devise an encoder-decoder architecture, namely Causality-aware
Graph Aggregation Weight Estimator for Debiasing (CAGED), to approximate the
unbiased aggregation weight by optimizing the evidence lower bound of the
interaction likelihood. In order to enhance the debiasing effectiveness during
early training stages, we further design a momentum update strategy that
incrementally refines the aggregation weight matrix. Extensive experiments on
three datasets demonstrate that CAGED outperforms existing graph-based
debiasing methods. Our implementation is available at
https://github.com/QueYork/CAGED.",https://github.com/QueYork/CAGED,Accepted by CIKM 2025
Which Programming Language and Model Work Best With LLM-as-a-Judge For Code Retrieval?,"Lucas Roberts, Denisa Roberts",2025-09-30,2510.00324v1,http://arxiv.org/abs/2510.00324v1,http://arxiv.org/pdf/2510.00324v1,information_retrieval,cs.SE,"Code search is an important information retrieval application. Benefits of
better code search include faster new developer on-boarding, reduced software
maintenance, and ease of understanding for large repositories. Despite
improvements in search algorithms and search benchmarks, the domain of code
search has lagged behind. One reason is the high cost of human annotation for
code queries and answers. While humans may annotate search results in general
text QA systems, code annotations require specialized knowledge of a
programming language (PL), as well as domain specific software engineering
knowledge. In this work we study the use of Large Language Models (LLMs) to
retrieve code at the level of functions and to generate annotations for code
search results. We compare the impact of the retriever representation (sparse
vs. semantic), programming language, and LLM by comparing human annotations
across several popular languages (C, Java, Javascript, Go, and Python). We
focus on repositories that implement common data structures likely to be
implemented in any PLs. For the same human annotations, we compare several
LLM-as-a-Judge models to evaluate programming language and other affinities
between LLMs. We find that the chosen retriever and PL exhibit affinities that
can be leveraged to improve alignment of human and AI relevance determinations,
with significant performance implications. We also find differences in
representation (sparse vs. semantic) across PLs that impact alignment of human
and AI relevance determinations. We propose using transpilers to bootstrap
scalable code search benchmark datasets in other PLs and in a case study
demonstrate that human-AI relevance agreement rates largely match the (worst
case) human-human agreement under study. The application code used in this work
is available at \href{https://github.com/rlucas7/code-searcher/}{this github
repo}.",https://github.com/rlucas7/code-searcher,Accepted as a full paper at SIGIR-AP 2025
MR$^2$-Bench: Going Beyond Matching to Reasoning in Multimodal Retrieval,"Junjie Zhou, Ze Liu, Lei Xiong, Jin-Ge Yao, Yueze Wang, Shitao Xiao, Fenfen Lin, Miguel Hu Chen, Zhicheng Dou, Siqi Bao, Defu Lian, Yongping Xiong, Zheng Liu",2025-09-30,2509.26378v1,http://arxiv.org/abs/2509.26378v1,http://arxiv.org/pdf/2509.26378v1,information_retrieval,cs.IR,"Multimodal retrieval is becoming a crucial component of modern AI
applications, yet its evaluation lags behind the demands of more realistic and
challenging scenarios. Existing benchmarks primarily probe surface-level
semantic correspondence (e.g., object-text matching) while failing to assess
the deeper reasoning required to capture complex relationships between visual
and textual information. To address this gap, we introduce MR$^2$-Bench, a
reasoning-intensive benchmark for multimodal retrieval. MR$^2$-Bench presents
the following critical values: 1) all tasks are reasoning-driven, going beyond
shallow matching to effectively assess models' capacity for logical, spatial,
and causal inference; 2) it features diverse multimodal data, such as natural
images, diagrams, and visual puzzles, enabling comprehensive evaluation across
content types; 3) it supports complex queries and documents containing multiple
images and covers diverse retrieval scenarios, more accurately reflecting
real-world applications. Our benchmark contains 1,309 curated queries, derived
either from manual collection and annotation or from selective consolidation of
public datasets. Despite achieving strong results on existing benchmarks,
current state-of-the-art models still struggle on MR$^2$-Bench: for example,
the leading Seed1.6-Embedding model attains a Recall@1 of 77.78 on MMEB, but
only 9.91 on MR$^2$-Bench. This substantial performance gap highlights both the
increased challenge posed by our benchmark and the pressing need for further
advances in reasoning-intensive multimodal retrieval. The dataset and
evaluation code will be made publicly available at
https://github.com/VectorSpaceLab/MR2-Bench.",https://github.com/VectorSpaceLab/MR2-Bench,
Knowledge Graph-Guided Multi-Agent Distillation for Reliable Industrial Question Answering with Datasets,"Jiqun Pan, Zhenke Duan, Jiani Tu, Anzhi Cheng, Yanqing Wang",2025-10-03,2510.06240v1,http://arxiv.org/abs/2510.06240v1,http://arxiv.org/pdf/2510.06240v1,databases,cs.CL,"Industrial question-answering (QA) systems require higher safety and
reliability than general-purpose dialogue models, as errors in high-risk
scenarios such as equipment fault diagnosis can have severe consequences.
Although multi-agent large language models enhance reasoning depth, they suffer
from uncontrolled iterations and unverifiable outputs, and conventional
distillation methods struggle to transfer collaborative reasoning capabilities
to lightweight, deployable student models. To address these challenges, we
propose Knowledge Graph-guided Multi-Agent System Distillation (KG-MASD). Our
approach formulates distillation as a Markov Decision Process and incorporates
a knowledge graph as a verifiable structured prior to enrich state
representation and ensure convergence. By integrating collaborative reasoning
with knowledge grounding, KG-MASD generates high-confidence instruction-tuning
data and jointly distills reasoning depth and verifiability into compact
student models suitable for edge deployment. Experiments on an industrial QA
dataset show that KG-MASD improves accuracy by 2.4 per cent to 20.1 per cent
over baselines and significantly enhances reliability, enabling trustworthy AI
deployment in safety-critical industrial scenarios. Code and data are available
at https://github.com/erwinmsmith/KG-MAD/.",https://github.com/erwinmsmith/KG-MAD,"41 pages, 12 figures, 6 tables"
EMR-AGENT: Automating Cohort and Feature Extraction from EMR Databases,"Kwanhyung Lee, Sungsoo Hong, Joonhyung Park, Jeonghyeop Lim, Juhwan Choi, Donghwee Yoon, Eunho Yang",2025-10-01,2510.00549v2,http://arxiv.org/abs/2510.00549v2,http://arxiv.org/pdf/2510.00549v2,databases,cs.DB,"Machine learning models for clinical prediction rely on structured data
extracted from Electronic Medical Records (EMRs), yet this process remains
dominated by hardcoded, database-specific pipelines for cohort definition,
feature selection, and code mapping. These manual efforts limit scalability,
reproducibility, and cross-institutional generalization. To address this, we
introduce EMR-AGENT (Automated Generalized Extraction and Navigation Tool), an
agent-based framework that replaces manual rule writing with dynamic, language
model-driven interaction to extract and standardize structured clinical data.
Our framework automates cohort selection, feature extraction, and code mapping
through interactive querying of databases. Our modular agents iteratively
observe query results and reason over schema and documentation, using SQL not
just for data retrieval but also as a tool for database observation and
decision making. This eliminates the need for hand-crafted, schema-specific
logic. To enable rigorous evaluation, we develop a benchmarking codebase for
three EMR databases (MIMIC-III, eICU, SICdb), including both seen and unseen
schema settings. Our results demonstrate strong performance and generalization
across these databases, highlighting the feasibility of automating a process
previously thought to require expert-driven design. The code will be released
publicly at https://github.com/AITRICS/EMR-AGENT/tree/main. For a
demonstration, please visit our anonymous demo page:
https://anonymoususer-max600.github.io/EMR_AGENT/",https://github.com/AITRICS/EMR-AGENT,currently under submission to ICLR 2026
Graph2Region: Efficient Graph Similarity Learning with Structure and Scale Restoration,"Zhouyang Liu, Yixin Chen, Ning Liu, Jiezhong He, Dongsheng Li",2025-10-01,2510.00394v1,http://arxiv.org/abs/2510.00394v1,http://arxiv.org/pdf/2510.00394v1,databases,cs.LG,"Graph similarity is critical in graph-related tasks such as graph retrieval,
where metrics like maximum common subgraph (MCS) and graph edit distance (GED)
are commonly used. However, exact computations of these metrics are known to be
NP-Hard. Recent neural network-based approaches approximate the similarity
score in embedding spaces to alleviate the computational burden, but they
either involve expensive pairwise node comparisons or fail to effectively
utilize structural and scale information of graphs. To tackle these issues, we
propose a novel geometric-based graph embedding method called Graph2Region
(G2R). G2R represents nodes as closed regions and recovers their adjacency
patterns within graphs in the embedding space. By incorporating the node
features and adjacency patterns of graphs, G2R summarizes graph regions, i.e.,
graph embeddings, where the shape captures the underlying graph structures and
the volume reflects the graph size. Consequently, the overlap between graph
regions can serve as an approximation of MCS, signifying similar node regions
and adjacency patterns. We further analyze the relationship between MCS and GED
and propose using disjoint parts as a proxy for GED similarity. This analysis
enables concurrent computation of MCS and GED, incorporating local and global
structural information. Experimental evaluation highlights G2R's competitive
performance in graph similarity computation. It achieves up to a 60.0\%
relative accuracy improvement over state-of-the-art methods in MCS similarity
learning, while maintaining efficiency in both training and inference.
Moreover, G2R showcases remarkable capability in predicting both MCS and GED
similarities simultaneously, providing a holistic assessment of graph
similarity. Code available at https://github.com/liuzhouyang/Graph2Region.",https://github.com/liuzhouyang/Graph2Region,Accepted by IEEE Transactions on Knowledge and Data Engineering
"ActorDB: A Unified Database Model Integrating Single-Writer Actors, Incremental View Maintenance, and Zero-Trust Messaging",Jun Kawasaki,2025-09-29,2509.25285v1,http://arxiv.org/abs/2509.25285v1,http://arxiv.org/pdf/2509.25285v1,databases,cs.DB,"This paper presents ActorDB ( Dekigoto ) , a novel database architecture that
tightly integrates a single-writer actor model for writes, Incremental View
Maintenance (IVM), and a zero-trust security model as a core component. The
primary contribution of this work is the unification of these powerful but
complex concepts into a single, cohesive system designed to reduce
architectural complexity for developers of modern, data-intensive applications.
We argue that by providing these capabilities out-of-the-box, ActorDB can offer
a more robust, secure, and developer-friendly platform compared to solutions
that require manual integration of separate systems for actor persistence,
stream processing, and security. We present the core architecture, discuss the
critical trade-offs in its design, and define the performance criteria for a
Minimum Viable Product (MVP) to validate our approach.",https://github.com/com-junkawasaki/dekigoto,"7 pages, 1 table, 1 figures. Code and data available at
  https://github.com/com-junkawasaki/dekigoto"
Multilingual Text-to-SQL: Benchmarking the Limits of Language Models with Collaborative Language Agents,"Khanh Trinh Pham, Thu Huong Nguyen, Jun Jo, Quoc Viet Hung Nguyen, Thanh Tam Nguyen",2025-09-29,2509.24405v1,http://arxiv.org/abs/2509.24405v1,http://arxiv.org/pdf/2509.24405v1,databases,cs.CL,"Text-to-SQL enables natural access to databases, yet most benchmarks are
English-only, limiting multilingual progress. We introduce MultiSpider 2.0,
extending Spider 2.0 to eight languages (English, German, French, Spanish,
Portuguese, Japanese, Chinese, Vietnamese). It preserves Spider 2.0's
structural difficulty while adding linguistic and dialectal variability,
demanding deeper reasoning for complex SQL. On this benchmark, state-of-the-art
LLMs (such as DeepSeek-R1 and OpenAI o1) reach only 4\% execution accuracy when
relying on intrinsic reasoning, versus 60\% on MultiSpider 1.0. Therefore, we
provide a collaboration-driven language agents baseline that iteratively
refines queries, improving accuracy to 15\%. These results reveal a substantial
multilingual gap and motivate methods that are robust across languages and
ready for real-world enterprise deployment. Our benchmark is available at
https://github.com/phkhanhtrinh23/Multilingual_Text_to_SQL.",https://github.com/phkhanhtrinh23/Multilingual_Text_to_SQL,
PARROT: A Benchmark for Evaluating LLMs in Cross-System SQL Translation,"Wei Zhou, Guoliang Li, Haoyu Wang, Yuxing Han, Xufei Wu, Fan Wu, Xuanhe Zhou",2025-09-27,2509.23338v1,http://arxiv.org/abs/2509.23338v1,http://arxiv.org/pdf/2509.23338v1,databases,cs.DB,"Large language models (LLMS) have shown increasing effectiveness in
Text-to-SQL tasks. However, another closely related problem, Cross-System SQL
Translation (a.k.a., SQL-to-SQL), which adapts a query written for one database
system (e.g., MySQL) into its equivalent one for another system (e.g.,
ClickHouse), is of great practical importance but remains underexplored.
Existing SQL benchmarks are not well-suited for SQL-to-SQL evaluation, which
(1) focus on a limited set of database systems (often just SQLite) and (2)
cannot capture many system-specific SQL dialects (e.g., customized functions,
data types, and syntax rules). Thus, in this paper, we introduce PARROT, a
Practical And Realistic BenchmaRk for CrOss-System SQL Translation. PARROT
comprises 598 translation pairs from 38 open-source benchmarks and real-world
business services, specifically prepared to challenge system-specific SQL
understanding (e.g., LLMS achieve lower than 38.53% accuracy on average). We
also provide multiple benchmark variants, including PARROT-Diverse with 28,003
translations (for extensive syntax testing) and PARROT-Simple with 5,306
representative samples (for focused stress testing), covering 22
production-grade database systems. To promote future research, we release a
public leaderboard and source code at: https://code4db.github.io/parrot-bench/.",https://github.com/weAIDB/PARROT,"To appear in NeurIPS 2025. Welcome your submission to challenge our
  leaderboard at: https://code4db.github.io/parrot-bench/. Also visit our code
  repository at: https://github.com/weAIDB/PARROT"
AutoPK: Leveraging LLMs and a Hybrid Similarity Metric for Advanced Retrieval of Pharmacokinetic Data from Complex Tables and Documents,"Hossein Sholehrasa, Amirhossein Ghanaatian, Doina Caragea, Lisa A. Tell, Jim E. Riviere, Majid Jaberi-Douraki",2025-09-26,2510.00039v1,http://arxiv.org/abs/2510.00039v1,http://arxiv.org/pdf/2510.00039v1,databases,cs.DB,"Pharmacokinetics (PK) plays a critical role in drug development and
regulatory decision-making for human and veterinary medicine, directly
affecting public health through drug safety and efficacy assessments. However,
PK data are often embedded in complex, heterogeneous tables with variable
structures and inconsistent terminologies, posing significant challenges for
automated PK data retrieval and standardization. AutoPK, a novel two-stage
framework for accurate and scalable extraction of PK data from complex
scientific tables. In the first stage, AutoPK identifies and extracts PK
parameter variants using large language models (LLMs), a hybrid similarity
metric, and LLM-based validation. The second stage filters relevant rows,
converts the table into a key-value text format, and uses an LLM to reconstruct
a standardized table. Evaluated on a real-world dataset of 605 PK tables,
including captions and footnotes, AutoPK shows significant improvements in
precision and recall over direct LLM baselines. For instance, AutoPK with LLaMA
3.1-70B achieved an F1-score of 0.92 on half-life and 0.91 on clearance
parameters, outperforming direct use of LLaMA 3.1-70B by margins of 0.10 and
0.21, respectively. Smaller models such as Gemma 3-27B and Phi 3-12B with
AutoPK achieved 2-7 fold F1 gains over their direct use, with Gemma's
hallucination rates reduced from 60-95% down to 8-14%. Notably, AutoPK enabled
open-source models like Gemma 3-27B to outperform commercial systems such as
GPT-4o Mini on several PK parameters. AutoPK enables scalable and
high-confidence PK data extraction, making it well-suited for critical
applications in veterinary pharmacology, drug safety monitoring, and public
health decision-making, while addressing heterogeneous table structures and
terminology and demonstrating generalizability across key PK parameters. Code
and data: https://github.com/hosseinsholehrasa/AutoPK",https://github.com/hosseinsholehrasa/AutoPK,Accepted at the 2025 IEEE 37th ICTAI
Play by the Type Rules: Inferring Constraints for LLM Functions in Declarative Programs,"Parker Glenn, Alfy Samuel, Daben Liu",2025-09-24,2509.20208v1,http://arxiv.org/abs/2509.20208v1,http://arxiv.org/pdf/2509.20208v1,databases,cs.CL,"Integrating LLM powered operators in declarative query languages allows for
the combination of cheap and interpretable functions with powerful,
generalizable language model reasoning. However, in order to benefit from the
optimized execution of a database query language like SQL, generated outputs
must align with the rules enforced by both type checkers and database contents.
Current approaches address this challenge with orchestrations consisting of
many LLM-based post-processing calls to ensure alignment between generated
outputs and database values, introducing performance bottlenecks. We perform a
study on the ability of various sized open-source language models to both parse
and execute functions within a query language based on SQL, showing that small
language models can excel as function executors over hybrid data sources. Then,
we propose an efficient solution to enforce the well-typedness of LLM
functions, demonstrating 7% accuracy improvement on a multi-hop question
answering dataset with 53% improvement in latency over comparable solutions. We
make our implementation available at https://github.com/parkervg/blendsql",https://github.com/parkervg/blendsql,
ORQ: Complex Analytics on Private Data with Strong Security Guarantees,"Eli Baum, Sam Buxbaum, Nitin Mathai, Muhammad Faisal, Vasiliki Kalavri, Mayank Varia, John Liagouris",2025-09-13,2509.10793v1,http://arxiv.org/abs/2509.10793v1,http://arxiv.org/pdf/2509.10793v1,databases,cs.CR,"We present ORQ, a system that enables collaborative analysis of large private
datasets using cryptographically secure multi-party computation (MPC). ORQ
protects data against semi-honest or malicious parties and can efficiently
evaluate relational queries with multi-way joins and aggregations that have
been considered notoriously expensive under MPC. To do so, ORQ eliminates the
quadratic cost of secure joins by leveraging the fact that, in practice, the
structure of many real queries allows us to join records and apply the
aggregations ""on the fly"" while keeping the result size bounded. On the system
side, ORQ contributes generic oblivious operators, a data-parallel vectorized
query engine, a communication layer that amortizes MPC network costs, and a
dataflow API for expressing relational analytics -- all built from the ground
up.
  We evaluate ORQ in LAN and WAN deployments on a diverse set of workloads,
including complex queries with multiple joins and custom aggregations. When
compared to state-of-the-art solutions, ORQ significantly reduces MPC execution
times and can process one order of magnitude larger datasets. For our most
challenging workload, the full TPC-H benchmark, we report results entirely
under MPC with Scale Factor 10 -- a scale that had previously been achieved
only with information leakage or the use of trusted third parties.",https://github.com/CASP-Systems-BU/orq,"14 pages, plus Appendix. To appear at SOSP 2025. Code published at
  https://github.com/CASP-Systems-BU/orq"
